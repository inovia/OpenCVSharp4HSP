説明(英語),説明(英語)
引数説明(英語),引数説明(英語)
詳細説明(英語),詳細説明(英語)
,
Basic marker detection.,基本的なマーカー検出
image : input image,image : 入力画像
dictionary : indicates the type of markers that will be searched,dictionary : 検索対象となるマーカーの種類を示す
"corners : vector of detected marker corners. For each marker, its four corners are provided, (e.g std::vector<std::vector<cv::Point2f> > ). For N detected markers, the dimensions of this array is Nx4. The order of the corners is clockwise.",corners : 検出されたマーカーの角のベクトル．各マーカーには，その4つのコーナーが提供されます（例：std::vector<std::vector<cv::Point2f> > ）．N個のマーカーが検出された場合、この配列の次元はNx4となります。また，コーナーの順番は時計回りです．
"ids : vector of identifiers of the detected markers. The identifier is of type int (e.g. std::vector<int>). For N detected markers, the size of ids is also N. The identifiers have the same order than the markers in the imgPoints array.",ids : 検出されたマーカの識別子のベクトル．識別子はint型です（例：std::vector<int>）．検出されたマーカーがN個の場合，idsのサイズもNとなる．識別子は，imgPoints配列中のマーカーと同じ順番である．
parameters : marker detection parameters,parameters : マーカー検出時のパラメータ
rejectedImgPoints : contains the imgPoints of those squares whose inner code has not a correct codification. Useful for debugging purposes.,rejectedImgPoints : 内部コードが正しくない正方形のimgPointsを含む．デバッグの際に便利です．
cameraMatrix : optional input 3x3 floating-point camera matrix \(A = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}\),cameraMatrix : オプションである入力 3x3 浮動小数点カメラ行列 ˶(A = ˶ˆ꒳ˆ˵ )
"distCoeff : optional vector of distortion coefficients \((k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6],[s_1, s_2, s_3, s_4]])\) of 4, 5, 8 or 12 elements","distCoeff : オプションで，4，5，8，12個の要素を持つ歪み係数\((k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6],[s_1, s_2, s_3, s_4])を格納するベクトル"
"Performs marker detection in the input image. Only markers included in the specific dictionary are searched. For each detected marker, it returns the 2D position of its corner in the image and its corresponding identifier. Note that this function does not perform pose estimation.See alsoestimatePoseSingleMarkers, estimatePoseBoard","入力画像のマーカー検出を行う．特定の辞書に含まれるマーカのみが検索されます．検出された各マーカーに対して，画像中のコーナーの2次元位置と，それに対応する識別子が返されます．参照：estimatePoseSingleMarkers, estimatePoseBoard"
Draw detected markers in image.,検出されたマーカーを画像に描画します．
image : input/output image. It must have 1 or 3 channels. The number of channels is not altered.,image : 入力/出力画像．1チャンネルまたは3チャンネルの画像である必要があります．チャンネル数の変更はできません．
"corners : positions of marker corners on input image. (e.g std::vector<std::vector<cv::Point2f> > ). For N detected markers, the dimensions of this array should be Nx4. The order of the corners should be clockwise.",corners : 入力画像上のマーカーコーナーの位置。(例：std::vector<std::vector<cv::Point2f> > ).検出されたマーカがN個の場合，この配列の次元はNx4になります．また，コーナーの順番は時計回りとする．
"ids : vector of identifiers for markers in markersCorners . Optional, if not provided, ids are not painted.",ids : markersCorners内のマーカーの識別子のベクトル．オプションで，提供されない場合は，idsは描かれません．
borderColor : color of marker borders. Rest of colors (text color and first corner color) are calculated based on this one to improve visualization.,borderColor : マーカーの境界線の色．残りの色（テキストの色と最初のコーナーの色）は，可視性を高めるために，この色に基づいて計算されます．
"Given an array of detected marker corners and its corresponding ids, this functions draws the markers in the image. The marker borders are painted and the markers identifiers if provided. Useful for debugging purposes.",検出されたマーカーコーナーの配列と，それに対応する ID が与えられると，この関数は，画像中にマーカーを描画します．マーカーの境界が描画され，マーカーの識別子が与えられていればそれも描画されます．デバッグ用に便利です。
Draw a canonical marker image.,標準的なマーカー画像を描画します。
dictionary : dictionary of markers indicating the type of markers,dictionary : マーカーの種類を示すマーカーの辞書
id : identifier of the marker that will be returned. It has to be a valid id in the specified dictionary.,id : 返されるマーカーの識別子。指定された辞書の中で有効なidである必要があります。
sidePixels : size of the image in pixels,sidePixels : 画像の大きさ（ピクセル単位
img : output image with the marker,img : マーカーを表示した出力画像
borderBits : width of the marker border.,borderBits : マーカーの境界線の幅。
This function returns a marker image in its canonical form (i.e. ready to be printed),この関数は、マーカー画像を正規の形式で（つまり、すぐに印刷できる状態で）返します。
Pose estimation for single markers.,単一のマーカに対する姿勢推定
"corners : vector of already detected markers corners. For each marker, its four corners are provided, (e.g std::vector<std::vector<cv::Point2f> > ). For N detected markers, the dimensions of this array should be Nx4. The order of the corners should be clockwise.",corners : 既に検出されたマーカーの角のベクトル．各マーカーには，その4つのコーナーが提供されます（例：std::vector<std::vector<cv::Point2f> > ）．N個のマーカーが検出された場合、この配列の次元はNx4になります。角の順番は時計回りとする。
"markerLength : the length of the markers' side. The returning translation vectors will be in the same unit. Normally, unit is meters.",markerLength : マーカーの辺の長さ。戻ってくる並進ベクトルの単位は同じになります。通常，単位はメートルです．
cameraMatrix : input 3x3 floating-point camera matrix \(A = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}\),cameraMatrix : 入力された3×3浮動小数点カメラ行列 ˶ˆ꒳ˆ˵ )
"distCoeffs : vector of distortion coefficients \((k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6],[s_1, s_2, s_3, s_4]])\) of 4, 5, 8 or 12 elements","distCoeffs : 4, 5, 8, 12要素の歪曲係数のベクトル\((k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6],[s_1, s_2, s_3, s_4]))"
rvecs : array of output rotation vectors (,rvecs : 出力回転ベクトルの配列 (
tvecs : array of output translation vectors (e.g. std::vector<cv::Vec3d>). Each element in tvecs corresponds to the specific marker in imgPoints.,tvecs : 出力される並進ベクトルの配列（std::vector<cv::Vec3d>など）．tvecs の各要素は，imgPoints の特定のマーカーに対応しています．
_objPoints : array of object points of all the marker corners,_objPoints : 全てのマーカーのコーナーのオブジェクトポイントの配列．
"See alsodetectMarkersSee alsoRodrigues) (e.g. std::vector<cv::Vec3d>). Each element in rvecs corresponds to the specific marker in imgPoints.This function receives the detected markers and returns their pose estimation respect to the camera individually. So for each marker, one rotation and translation vector is returned. The returned transformation is the one that transforms points from each marker coordinate system to the camera coordinate system. The marker corrdinate system is centered on the middle of the marker, with the Z axis perpendicular to the marker plane. The coordinates of the four corners of the marker in its own coordinate system are: (-markerLength/2, markerLength/2, 0), (markerLength/2, markerLength/2, 0), (markerLength/2, -markerLength/2, 0), (-markerLength/2, -markerLength/2, 0)","See alsodetectMarkersSee alsoRodrigues）（例：std::vector<cv::Vec3d>）。この関数は，検出されたマーカを受け取り，カメラに対する姿勢の推定値を個別に返します．つまり，各マーカーに対して，1つの回転ベクトルと並進ベクトルが返されます．返される変換は，各マーカの座標系からカメラの座標系へと点を変換するものです．マーカー座標系は、マーカーの中央を中心とし、Z軸はマーカー平面に垂直です。マーカーの四隅の座標を独自の座標系で表すと(-markerLength/2, markerLength/2, 0)、(markerLength/2, markerLength/2, 0)、(markerLength/2, -markerLength/2, 0)、(-markerLength/2, -markerLength/2, 0)"
Draw coordinate system axis from pose estimation.,ポーズ推定から座標系の軸を描画します。
rvec : rotation vector of the coordinate system that will be drawn. (,rvec : 描画される座標系の回転ベクトル。(
tvec : translation vector of the coordinate system that will be drawn.,tvec : 描画される座標系の並進ベクトル。
length : length of the painted axis in the same unit than tvec (usually in meters),length : 描画される軸の長さをtvecと同じ単位（通常はメートル単位）で表す。
"See alsoRodrigues).Given the pose estimation of a marker or board, this function draws the axis of the world coordinate system, i.e. the system centered on the marker/board. Useful for debugging purposes.Deprecated:use cv::drawFrameAxesExamples: samples/cpp/tutorial_code/ml/introduction_to_pca/introduction_to_pca.cpp.",Rodrigues も参照のこと）.マーカーやボードの姿勢推定が与えられたときに、ワールド座標系、つまりマーカーやボードを中心とした座標系の軸を描画する関数である。Deprecated:use cv::drawFrameAxesExamples: samples/cpp/tutorial_code/ml/introduction_to_pca/introduction_to_pca.cpp.
Returns one of the predefined dictionaries defined in PREDEFINED_DICTIONARY_NAME.,PREDEFINED_DICTIONARY_NAME で定義された定義済みの辞書の一つを返します。
Detect ChArUco Diamond markers.,ChArUcoダイヤモンドマーカーを検出します。
image : input image necessary for corner subpixel.,image : corner subpixelに必要な入力画像．
markerCorners : list of detected marker corners from detectMarkers function.,markerCorners : detectMarkers 関数で検出されたマーカーコーナーのリスト．
markerIds : list of marker ids in markerCorners.,markerIds : markerCornersで検出されたマーカーIDのリスト
squareMarkerLengthRate : rate between square and marker length: squareMarkerLengthRate = squareLength/markerLength. The real units are not necessary.,squareMarkerLengthRate : 正方形の長さとマーカーの長さの比率： squareMarkerLengthRate = squareLength/markerLength．実数単位は必要ない。
"diamondCorners : output list of detected diamond corners (4 corners per diamond). The order is the same than in marker corners: top left, top right, bottom right and bottom left. Similar format than the corners returned by detectMarkers (e.g std::vector<std::vector<cv::Point2f> > ).",diamondCorners : 検出されたダイヤモンドコーナーのリストを出力します（1つのダイヤモンドにつき4つのコーナー）。順番はマーカーコーナーと同じで，左上，右上，右下，左下です．detectMarkersで返されるコーナーと同様のフォーマットです（例：std::vector<std::vector<cv::Point2f> > ）。
"diamondIds : ids of the diamonds in diamondCorners. The id of each diamond is in fact of type Vec4i, so each diamond has 4 ids, which are the ids of the aruco markers composing the diamond.",diamondIds : diamondCornersに含まれるダイヤモンドのidです．各ダイヤモンドのidは実際にはVec4i型なので、各ダイヤモンドは4つのidを持ち、それらはそのダイヤモンドを構成するarucoマーカーのidです。
cameraMatrix : Optional camera calibration matrix.,cameraMatrix :オプションのカメラキャリブレーション行列．
distCoeffs : Optional camera distortion coefficients.,distCoeffs :オプションであるカメラの歪み係数．
"This function detects Diamond markers from the previous detected ArUco markers. The diamonds are returned in the diamondCorners and diamondIds parameters. If camera calibration parameters are provided, the diamond search is based on reprojection. If not, diamond search is based on homography. Homography is faster than reprojection but can slightly reduce the detection rate.",この関数は，以前に検出されたアルコマーカからダイアモンドマーカを検出します．ダイアモンドは， diamondCorners と diamondIds パラメータで返されます．カメラのキャリブレーションパラメータが提供されている場合、ダイヤモンドの探索は再投影に基づいて行われます。提供されていない場合は、ホモグラフィーに基づいてダイヤモンドを探索します。ホモグラフィーはリプロジェクションよりも高速ですが、検出率が若干低下します。
Draw a set of detected ChArUco Diamond markers.,検出されたChArUcoダイヤモンドマーカーのセットを描画します。
"diamondCorners : positions of diamond corners in the same format returned by detectCharucoDiamond(). (e.g std::vector<std::vector<cv::Point2f> > ). For N detected markers, the dimensions of this array should be Nx4. The order of the corners should be clockwise.",diamondCorners : detectCharucoDiamond()で返されるのと同じフォーマットのダイヤモンドコーナーの位置。(例: std::vector<std::vector<cv::Point2f> > ).N個のマーカーが検出された場合、この配列の次元はNx4になるはずです。また，コーナーの順番は時計回りとします．
"diamondIds : vector of identifiers for diamonds in diamondCorners, in the same format returned by detectCharucoDiamond() (e.g. std::vector<Vec4i>). Optional, if not provided, ids are not painted.",diamondIds : diamondCornersに含まれるダイヤモンドの識別子のベクトルで， detectCharucoDiamond() が返すのと同じ形式です（例：std::vector<Vec4i>）．オプションで、提供されない場合は、IDは描かれません。
borderColor : color of marker borders. Rest of colors (text color and first corner color) are calculated based on this one.,borderColor : マーカーの境界線の色．残りの色（テキストの色と最初のコーナーの色）は、この色を基に計算されます。
"Given an array of detected diamonds, this functions draws them in the image. The marker borders are painted and the markers identifiers if provided. Useful for debugging purposes.",検出されたダイヤモンドの配列が与えられると，この関数はそれらを画像上に描画します．マーカーの境界線が描画され、マーカーの識別子が指定されていればそれも描画されます。デバッグ用に便利です。
Dictionary/Set of markers. It contains the inner codification. ,マーカーの辞書／セット。内部のコード化を含みます。
bytesList contains the marker codewords where,bytesListには、マーカーのコードワードが含まれています。
bytesList.rows is the dictionary size,bytesList.rowsは、辞書のサイズです。
each marker is encoded using nbytes = ceil(markerSize*markerSize/8.),各マーカーは nbytes = ceil(markerSize*markerSize/8.) でエンコードされます。
"each row contains all 4 rotations of the marker, so its length is 4*nbytes",各行には、マーカーの4つの回転がすべて含まれているので、その長さは4*nbytesです。
"bytesList.ptr(i)[k*nbytes + j] is then the j-th byte of i-th marker, in its k-th rotation. ",bytesList.ptr(i)[k*nbytes + j]は、i番目のマーカーのk番目の回転におけるj番目のバイトになります。
Creates mixture-of-gaussian background subtractor.,mixture of-gaussian background subtractorを作成する。
history : Length of the history.,history : ヒストリーの長さ．
nmixtures : Number of Gaussian mixtures.,nmixtures :ガウス混合分布の数．
backgroundRatio : Background ratio.,backgroundRatio :背景比．
noiseSigma : Noise strength (standard deviation of the brightness or each color channel). 0 means some automatic value.,noiseSigma : ノイズの強さ（輝度または各色チャンネルの標準偏差）。0は何らかの自動値を意味します．
Gaussian Mixture-based Background/Foreground Segmentation Algorithm. ,Gaussian Mixture-based Background/Foreground Segmentation Algorithm.
The class implements the algorithm described in [122] . ,このクラスは， [122] で述べられているアルゴリズムを実装しています．
Creates a GMG Background Subtractor.,GMG背景減算器を作成します．
initializationFrames : number of frames used to initialize the background models.,initializationFrames : 背景モデルの初期化に使われるフレームの数．
"decisionThreshold : Threshold value, above which it is marked foreground, else background.",decisionThreshold : 前景と判断されるしきい値，そうでなければ背景と判断されるしきい値．
Background Subtractor module based on the algorithm given in [92] . ,Background Subtractorモジュールは，[92]で示されたアルゴリズムに基づいています．
"Takes a series of images and returns a sequence of mask (8UC1) images of the same size, where 255 indicates Foreground and 0 represents Background. This class implements an algorithm described in ""Visual Tracking of Human Visitors under","一連の画像を受け取り，同じ大きさのマスク（8UC1）画像のシーケンスを返します（255は前景，0は背景を表します）．このクラスは，""Visual Tracking of Human Visitors under Variable-Lighting Conditions for a Responsive API ""に記載されているアルゴリズムを実装しています．"
"Variable-Lighting Conditions for a Responsive Audio Art Installation,"" A. Godbehere, A. Matsukawa, K. Goldberg, American Control Conference, Montreal, June 2012. ","Visual Tracking of Human Visitors under Variable-Lighting Conditions for a Responsive Audio Art Installation,"" A. Godbehere, A. Matsukawa, K. Goldberg, American Control Conference, Montreal, June 2012."
Returns total number of distinct colors to maintain in histogram.,ヒストグラムで保持する識別色の総数を返します。
Sets total number of distinct colors to maintain in histogram.,ヒストグラムで保持する異なる色の総数を設定します。
Returns the learning rate of the algorithm.,アルゴリズムの学習率を返します。
"It lies between 0.0 and 1.0. It determines how quickly features are ""forgotten"" from histograms.",0.0から1.0の間です。この値は、ヒストグラムから特徴がどれだけ早く「忘れられる」かを決定します。
Sets the learning rate of the algorithm.,アルゴリズムの学習率を設定します。
Returns the number of frames used to initialize background model.,背景モデルの初期化に使われたフレームの数を返します。
Sets the number of frames used to initialize background model.,背景モデルの初期化に使われるフレームの数を設定します。
Returns the parameter used for quantization of color-space.,Return the parameter used for quantization of color-space.
It is the number of discrete levels in each channel to be used in histograms.,ヒストグラムで使用する各チャンネルの離散的なレベルの数を指定します。
Sets the parameter used for quantization of color-space.,色空間の量子化に用いられるパラメータを設定します。
Returns the prior probability that each individual pixel is a background pixel.,ODA：各ピクセルが背景ピクセルである事前確率を返します。
Sets the prior probability that each individual pixel is a background pixel.,各々のピクセルが背景ピクセルであるという事前確率を設定します。
Returns the kernel radius used for morphological operations.,Morphological operationに使われるカーネル半径を返します。
Sets the kernel radius used for morphological operations.,モルフォロジー演算に使われるカーネル半径を設定します。
Returns the value of decision threshold.,決定閾値の値を返します。
Decision value is the value above which pixel is determined to be FG.,決定しきい値とは、ピクセルがFGであると判断される値のことです。
Sets the value of decision threshold.,決定しきい値の値を設定します。
Returns the status of background model update.,background model update のステータスを返します。
Sets the status of background model update.,背景モデルのアップデートの状態を設定します。
Returns the minimum value taken on by pixels in image sequence. Usually 0.,Returns the minimum value took on pixels in image sequence 画素の最小値を返します。通常は0です。
Sets the minimum value taken on by pixels in image sequence.,イメージシーケンス内のピクセルで取られた最小値を設定します。
Returns the maximum value taken on by pixels in image sequence. e.g. 1.0 or 255.,RETURN：イメージシーケンス内のピクセルに適用される最大値を返します。
Sets the maximum value taken on by pixels in image sequence.,イメージシーケンス内のピクセルにかかる最大値を設定します。
A class to upscale images via convolutional neural networks. The following four models are implemented: ,畳み込みニューラルネットワークを使って画像をアップスケールするクラスです。以下の4つのモデルが実装されています。
edsr,edsr
espcn,espcn
fsrcnn,fsrcnn
lapsrn ,lapsrn
Read the model from the given path.,与えられたパスからモデルを読み込みます。
path : Path to the model file.,path : モデルファイルへのパス。
weights : Path to the model weights file.,weights : モデルの重み付けファイルへのパス。
definition : Path to the model definition file.,definition : モデル定義ファイルへのパス。
Set desired model.,目的のモデルを設定する。
algo : String containing one of the desired models:,algo : 希望するモデルの一つを含む文字列。
lapsrn,laprn
scale : Integer specifying the upscale factor,scale : アップスケール係数を表す整数
Set computation backend.,計算バックエンドの設定
Set computation target.,計算対象を設定します。
Upsample via neural network.,ニューラルネットワークでアップサンプルします。
img : Image to upscale,img : アップスケールする画像
result : Destination upscaled image,result : アップスケール後の画像
Upsample via neural network of multiple outputs.,複数出力のニューラルネットワークによるアップサンプル。
imgs_new : Destination upscaled images,imgs_new : アップスケール後の出力画像
scale_factors : Scaling factors of the output nodes,scale_factors :出力ノードのスケーリングファクター
node_names : Names of the output nodes in the neural network,node_names : ニューラルネットワーク内の出力ノードの名前
Returns the scale factor of the model:,モデルのスケールファクターを返します。
The FLANN nearest neighbor index class. This class is templated with the type of elements for which the index is built.,FLANNの最近傍インデックスクラスです．このクラスは，インデックスが作成される要素の種類によってテンプレート化されています．
"the index will perform a linear, brute-force search.",このインデックスは、直線的なブルートフォースサーチを行います。
the index constructed will consist of a set of randomized kd-trees which will be searched in parallel.,構築されたインデックスは、並列に検索されるランダム化されたKDツリーのセットで構成されます。
"the index constructed will be a hierarchical k-means tree (one tree by default), dividing each set of points into n clusters whose barycenters are refined iteratively. Note that this algorithm has been extended to the support of binary vectors as an alternative to LSH when knn search speed is the criterium. It will also outperform LSH when processing directly (i.e. without the use of MCA/PCA) datasets whose points share mostly the same values for most of the dimensions. It is recommended to set more than one tree with binary data.",構築されたインデックスは、階層的なk-meansツリー（デフォルトでは1つのツリー）となり、各点の集合をn個のクラスタに分割し、そのバリセンターを反復的に改良します。なお，このアルゴリズムは，knnの検索速度を重視する場合にLSHの代替手段として，バイナリベクトルをサポートするように拡張されています．また、ほとんどの次元でポイントが同じ値を持つデータセットを直接（MCA/PCAを使用せずに）処理する場合にも、LSHを上回る性能を発揮します。バイナリデータでは、複数のツリーを設定することをお勧めします。
"the index created uses multi-probe LSH (by Multi-Probe LSH: Efficient Indexing for High-Dimensional Similarity Search by Qin Lv, William Josephson, Zhe Wang, Moses Charikar, Kai Li., Proceedings of the 33rd International Conference on Very Large Data Bases (VLDB). Vienna, Austria. September 2007). This algorithm is designed for binary vectors.","作成されたインデックスは，マルチプローブLSH（Multi-Probe LSH: Efficient Indexing for High-Dimensional Similarity Search by Qin Lv, William Josephson, Zhe Wang, Moses Charikar, Kai Li., Proceedings of the 33rd International Conference on Very Large Data Bases (VLDB).ウィーン、オーストリア。September 2007）に掲載されています。このアルゴリズムは，2値ベクトルを対象としています．"
the index created combines the randomized kd-trees and the hierarchical k-means tree.,作成されたインデックスは、ランダム化されたKDツリーと階層化されたk-meansツリーを組み合わせたものです。
"the index created is automatically tuned to offer the best performance, by choosing the optimal index type (randomized kd-trees, hierarchical kmeans, linear) and parameters for the dataset provided.",作成されたインデックスは，提供されたデータセットに対して最適なインデックスタイプ（ランダム化 kd-trees，階層的 kmeans，linear）とパラメータを選択することで，最高の性能を発揮するように自動的に調整されます．
loading a previously saved index from the disk.,以前に保存したインデックスをディスクから読み込む。
Creates a window.,ウィンドウを作成します．
winname : Name of the window in the window caption that may be used as a window identifier.,winname : ウィンドウのキャプションに表示されるウィンドウの名前で，ウィンドウの識別子として利用できます．
flags : Flags of the window. The supported flags are: (cv::WindowFlags),flags :ウィンドウのフラグ．サポートされるフラグは以下の通りです．(cv::WindowFlags)
"The function namedWindow creates a window that can be used as a placeholder for images and trackbars. Created windows are referred to by their names.If a window with the same name already exists, the function does nothing.You can call cv::destroyWindow or cv::destroyAllWindows to close the window and de-allocate any associated memory usage. For a simple program, you do not really have to call these functions because all the resources and windows of the application are closed automatically by the operating system upon exit.NoteQt backend supports additional flags:WINDOW_NORMAL or WINDOW_AUTOSIZE: WINDOW_NORMAL enables you to resize the window, whereas WINDOW_AUTOSIZE adjusts automatically the window size to fit the displayed image (see imshow ), and you cannot change the window size manually.",関数 namedWindow は，画像やトラックバーのプレースホルダとして利用可能なウィンドウを作成します．作成されたウィンドウは，その名前で参照されます．同じ名前のウィンドウが既に存在する場合，この関数は何もしません．ウィンドウを閉じて，関連するメモリ使用量の割り当てを解除するには， cv::destroyWindow または cv::destroyAllWindows を呼び出すことができます．NoteQt バックエンドは，次のような追加フラグをサポートしています： WINDOW_NORMAL または WINDOW_AUTOSIZE: WINDOW_NORMAL は，ウィンドウサイズを変更することができますが，WINDOW_AUTOSIZE は，表示されている画像に合わせてウィンドウサイズを自動的に調整するので（ imshow を参照してください），ウィンドウサイズを手動で変更することはできません．
"WINDOW_FREERATIO or WINDOW_KEEPRATIO: WINDOW_FREERATIO adjusts the image with no respect to its ratio, whereas WINDOW_KEEPRATIO keeps the image ratio.",WINDOW_FREERATIO または WINDOW_KEEPRATIO: WINDOW_FREERATIO は画像の比率を無視して画像を調整しますが、WINDOW_KEEPRATIO は画像の比率を維持します。
"WINDOW_GUI_NORMAL or WINDOW_GUI_EXPANDED: WINDOW_GUI_NORMAL is the old way to draw the window without statusbar and toolbar, whereas WINDOW_GUI_EXPANDED is a new enhanced GUI. By default, flags == WINDOW_AUTOSIZE | WINDOW_KEEPRATIO | WINDOW_GUI_EXPANDEDExamples: samples/cpp/camshiftdemo.cpp, samples/cpp/connected_components.cpp, samples/cpp/contours2.cpp, samples/cpp/create_mask.cpp, samples/cpp/demhist.cpp, samples/cpp/distrans.cpp, samples/cpp/edge.cpp, samples/cpp/falsecolor.cpp, samples/cpp/ffilldemo.cpp, samples/cpp/fitellipse.cpp, samples/cpp/grabcut.cpp, samples/cpp/image_alignment.cpp, samples/cpp/laplace.cpp, samples/cpp/lkdemo.cpp, samples/cpp/pca.cpp, samples/cpp/polar_transforms.cpp, samples/cpp/segment_objects.cpp, samples/cpp/train_HOG.cpp, samples/cpp/tutorial_code/HighGUI/AddingImagesTrackbar.cpp, samples/cpp/tutorial_code/Histograms_Matching/MatchTemplate_Demo.cpp, samples/cpp/tutorial_code/ImgProc/Morphology_1.cpp, samples/cpp/tutorial_code/ImgProc/Morphology_2.cpp, samples/cpp/tutorial_code/ImgProc/Smoothing/Smoothing.cpp, samples/cpp/tutorial_code/ImgTrans/copyMakeBorder_demo.cpp, samples/cpp/warpPerspective_demo.cpp, samples/cpp/watershed.cpp, samples/dnn/classification.cpp, samples/dnn/object_detection.cpp, samples/dnn/segmentation.cpp, and samples/tapi/squares.cpp.","WINDOW_GUI_NORMAL または WINDOW_GUI_EXPANDED: WINDOW_GUI_NORMAL はステータスバーやツールバーのない古いウィンドウの描き方で、WINDOW_GUI_EXPANDED は新しい拡張された GUI です。デフォルトでは、flags == WINDOW_AUTOSIZE | WINDOW_KEEPRATIO | WINDOW_GUI_EXPANDEDExamples: samples/cpp/camshiftdemo.cpp, samples/cpp/connected_components.cpp, samples/cpp/contours2.cpp, samples/cpp/create_mask.cpp, samples/cpp/demhist.cpp, samples/cpp/distrans.cpp, samples/cpp/edge.cpp, samples/cpp/falecolor.cpp, samples/cpp/ffilldemo.cpp, samples/cpp/fitellipse.cpp, samples/cpp/grabcut.cpp, samples/cpp/image_alignment.cpp, samples/cpp/laplace.cpp, samples/cpp/lkdemo.cpp, samples/cpp/pca.cpp, samples/cpp/polar_transforms.cpp, samples/cpp/segment_objects.cpp.cpp, samples/cpp/train_HOG.cpp, samples/cpp/tutorial_code/HighGUI/AddingImagesTrackbar.cpp, samples/cpp/tutorial_code/Histograms_Matching/MatchTemplate_Demo.cpp, samples/cpp/tutorial_code/ImgProc/Morphology_1.cpp, samples/cpp/tutorial_code/ImgProc/Morphology_2.cpp、samples/cpp/tutorial_code/ImgProc/Smoothing/Smoothing.cpp、samples/cpp/tutorial_code/ImgTrans/copyMakeBorder_demo.cpp、samples/cpp/warpPerspective_demo.cpp、samples/cpp/watershed.cpp、samples/dnn/classification.cpp、samples/dnn/object_detection.cpp、samples/dnn/segmentation.cpp、samples/tapi/squares.cppです。"
Destroys the specified window.,指定されたウィンドウを破棄します。
winname : Name of the window to be destroyed.,winname : 破壊されるウィンドウの名前。
"The function destroyWindow destroys the window with the given name.Examples: modules/shape/samples/shape_example.cpp, samples/cpp/camshiftdemo.cpp, samples/cpp/ffilldemo.cpp, and samples/cpp/grabcut.cpp.","例: modules/shape/samples/shape_example.cpp, samples/cpp/camshiftdemo.cpp, samples/cpp/ffilldemo.cpp, samples/cpp/grabcut.cpp."
Destroys all of the HighGUI windows.,HighGUIのすべてのウィンドウを破棄します。
The function destroyAllWindows destroys all of the opened HighGUI windows.,関数 destroyAllWindows は、開いているすべての HighGUI ウィンドウを破棄します。
Displays an image in the specified window.,指定されたウィンドウに画像を表示します。
winname : Name of the window.,winname : ウィンドウの名前です。
mat : Image to be shown.,mat :表示される画像。
"The function imshow displays an image in the specified window. If the window was created with the cv::WINDOW_AUTOSIZE flag, the image is shown with its original size, however it is still limited by the screen resolution. Otherwise, the image is scaled to fit the window. The function may scale the image, depending on its depth:If the image is 8-bit unsigned, it is displayed as is.",関数imshowは，指定されたウィンドウに画像を表示します．cv::WINDOW_AUTOSIZE フラグを指定してウィンドウを作成した場合，画像は元のサイズで表示されますが，スクリーンの解像度によって制限されます．そうでない場合，画像はウィンドウに合わせてスケーリングされます．この関数は，画像の深度に応じて画像をスケーリングします：画像が8ビット符号なしの場合は，そのまま表示されます．
"If the image is 16-bit unsigned or 32-bit integer, the pixels are divided by 256. That is, the value range [0,255*256] is mapped to [0,255].","画像が8ビット符号なしの場合は，そのまま表示されます。画像が16ビット符号なしまたは32ビット整数の場合は，ピクセルが256で分割されます。つまり、[0,255*256]の値域が[0,255]にマッピングされます。"
"If the image is 32-bit or 64-bit floating-point, the pixel values are multiplied by 255. That is, the value range [0,1] is mapped to [0,255].If window was created with OpenGL support, cv::imshow also support ogl::Buffer , ogl::Texture2D and cuda::GpuMat as input.If the window was not created before this function, it is assumed creating a window with cv::WINDOW_AUTOSIZE.If you need to show an image that is bigger than the screen resolution, you will need to call namedWindow("""", WINDOW_NORMAL) before the imshow.NoteThis function should be followed by a call to cv::waitKey or cv::pollKey to perform GUI housekeeping tasks that are necessary to actually show the given image and make the window respond to mouse and keyboard events. Otherwise, it won't display the image and the window might lock up. For example, waitKey(0) will display the window infinitely until any keypress (it is suitable for image display). waitKey(25) will display a frame and wait approximately 25 ms for a key press (suitable for displaying a video frame-by-frame). To remove the window, use cv::destroyWindow.[Windows Backend Only] Pressing Ctrl+C will copy the image to the clipboard.[Windows Backend Only] Pressing Ctrl+S will show a dialog to save the image.Examples: fld_lines.cpp, modules/shape/samples/shape_example.cpp, samples/cpp/camshiftdemo.cpp, samples/cpp/connected_components.cpp, samples/cpp/contours2.cpp, samples/cpp/convexhull.cpp, samples/cpp/create_mask.cpp, samples/cpp/demhist.cpp, samples/cpp/distrans.cpp, samples/cpp/edge.cpp, samples/cpp/facedetect.cpp, samples/cpp/falsecolor.cpp, samples/cpp/ffilldemo.cpp, samples/cpp/fitellipse.cpp, samples/cpp/grabcut.cpp, samples/cpp/image_alignment.cpp, samples/cpp/intersectExample.cpp, samples/cpp/kalman.cpp, samples/cpp/kmeans.cpp, samples/cpp/laplace.cpp, samples/cpp/lkdemo.cpp, samples/cpp/minarea.cpp, samples/cpp/pca.cpp, samples/cpp/peopledetect.cpp, samples/cpp/polar_transforms.cpp, samples/cpp/segment_objects.cpp, samples/cpp/squares.cpp, samples/cpp/train_HOG.cpp, samples/cpp/tutorial_code/features2D/Homography/homography_from_camera_displacement.cpp, samples/cpp/tutorial_code/features2D/Homography/pose_from_homography.cpp, samples/cpp/tutorial_code/HighGUI/AddingImagesTrackbar.cpp, samples/cpp/tutorial_code/Histograms_Matching/MatchTemplate_Demo.cpp, samples/cpp/tutorial_code/ImgProc/basic_drawing/Drawing_1.cpp, samples/cpp/tutorial_code/ImgProc/basic_drawing/Drawing_2.cpp, samples/cpp/tutorial_code/ImgProc/Morphology_1.cpp, samples/cpp/tutorial_code/ImgProc/Morphology_2.cpp, samples/cpp/tutorial_code/ImgProc/Pyramids/Pyramids.cpp, samples/cpp/tutorial_code/ImgProc/Smoothing/Smoothing.cpp, samples/cpp/tutorial_code/ImgTrans/copyMakeBorder_demo.cpp, samples/cpp/tutorial_code/ImgTrans/houghcircles.cpp, samples/cpp/tutorial_code/ImgTrans/houghlines.cpp, samples/cpp/tutorial_code/ImgTrans/Sobel_Demo.cpp, samples/cpp/tutorial_code/ml/introduction_to_pca/introduction_to_pca.cpp, samples/cpp/tutorial_code/photo/non_photorealistic_rendering/npr_demo.cpp, samples/cpp/tutorial_code/photo/seamless_cloning/cloning_demo.cpp, samples/cpp/videowriter_basic.cpp, samples/cpp/warpPerspective_demo.cpp, samples/cpp/watershed.cpp, samples/dnn/classification.cpp, samples/dnn/colorization.cpp, samples/dnn/object_detection.cpp, samples/dnn/openpose.cpp, samples/dnn/segmentation.cpp, samples/dnn/text_detection.cpp, samples/tapi/hog.cpp, and samples/tapi/squares.cpp.","画像が32ビットまたは64ビットの浮動小数点である場合、ピクセル値は255倍されます。つまり，値の範囲 [0,1] は [0,255] にマッピングされます．OpenGL をサポートするウィンドウが作成された場合，cv::imshow は入力として ogl::Buffer , ogl::Texture2D, cuda::GpuMat もサポートします．この関数より前にウィンドウが作成されていない場合は，cv::WINDOW_AUTOSIZE でウィンドウを作成したものとみなされます．スクリーンの解像度よりも大きな画像を表示する必要がある場合は，imshowの前に namedWindow("""", WINDOW_NORMAL) を呼び出す必要があります． 注意この関数は，与えられた画像を実際に表示したり，ウィンドウがマウスやキーボードのイベントに反応したりするのに必要なGUIハウスキーピングタスクを行うために，cv::waitKey または cv::pollKey の呼び出しの後に実行されるべきです．そうしないと，画像が表示されず，ウィンドウがロックしてしまうかもしれません．例えば，waitKey(0) は，任意のキーが押されるまで無限にウィンドウを表示します（画像表示に適しています）． waitKey(25) は，1 フレームを表示し，キーが押されるまで約 25 ms 待ちます（ビデオを 1 フレームごとに表示するのに適しています）．ウィンドウを削除するには，cv::destroyWindowを利用します．[Windows Backend Only] Ctrl+Cを押すと，画像をクリップボードにコピーします．[Windows Backend Only] Ctrl+Sを押すと，画像を保存するためのダイアログが表示されます．例: fld_lines.cpp, modules/shape/samples/shape_example.cpp, samples/cpp/camshiftdemo.cpp、samples/cpp/connected_components.cpp、samples/cpp/contours2.cpp、samples/cpp/convexhull.cpp、samples/cpp/create_mask.cpp、samples/cpp/demhist.cpp、samples/cpp/distrans.cpp、samples/cpp/edge.cpp、samples/cpp/facedetect.cpp、samples/cpp/falecolor.cpp、samples/cpp/ffilldemo.cpp、samples/cpp/fitellipse.cpp。cpp, samples/cpp/grabcut.cpp, samples/cpp/image_alignment.cpp, samples/cpp/intersectExample.cpp, samples/cpp/kalman.cpp, samples/cpp/kmeans.cpp, samples/cpp/laplace.cpp, samples/cpp/lkdemo.cpp, samples/cpp/minarea.cpp, samples/cpp/pca.cpp, samples/cpp/peopledetect.cpp, samples/cpp/polar_transforms.cpp, samples/cpp/segment_objects.cpp, samples/cpp/squares.cpp, samples/cpp/train_HOG.cpp, samples/cpp/tutorial_code/features2D/Homography/homography_from_camera_displacement.cpp, samples/cpp/tutorial_code/features2D/Homography/pose_from_homography.cpp, samples/cpp/tutorial_code/HighGUI/AddingImagesTrackbar.cpp, samples/cpp/tutorial_code/Histograms_Matching/MatchTemplate_Demo.cpp, samples/cpp/tutorial_code/ImgProc/basic_drawing/Drawing_1.cpp, samples/cpp/tutorial_code/ImgProc/basic_drawing/Drawing_2.cpp, samples/cpp/tutorial_code/ImgProc/Morphology_1.cpp, samples/cpp/tutorial_code/ImgProc/Morphology_2.cpp, samples/cpp/tutorial_code/ImgProc/Pyramids/Pyramids.cpp, samples/cpp/tutorial_code/ImgProc/Smoothing/Smoothing.cpp, samples/cpp/tutorial_code/ImgTrans/copyMakeBorder_demo.cpp, samples/cpp/tutorial_code/ImgTrans/houghcircles.cpp, samples/cpp/tutorial_code/ImgTrans/houghlines.cpp, samples/cpp/tutorial_code/ImgTrans/Sobel_Demo.cpp, samples/cpp/tutorial_code/ml/introduction_to_pca/introduction_to_pca.cpp, samples/cpp/tutorial_code/photo/non_photorealistic_rendering/npr_demo.cpp, samples/cpp/tutorial_code/photo/seamless_cloning/cloning_demo.cpp, samples/cpp/videowriter_basic.cpp、samples/cpp/warpPerspective_demo.cpp、samples/cpp/watershed.cpp、samples/dnn/classification.cpp、samples/dnn/colorization.cpp、samples/dnn/object_detection.cpp、samples/dnn/openpose.cpp、samples/dnn/segmentation.cpp、samples/dnn/text_detection.cpp、samples/tapi/hog.cpp、samples/tapi/squares.cppです。"
"Similar to waitKey, but returns full key code.",waitKeyと似ていますが、完全なキーコードを返します。
NoteKey code is implementation specific and depends on used backend: QT/GTK/Win32/etc,NoteKeyのコードは実装によって異なり、使用するバックエンドに依存します。QT/GTK/Win32/その他
Resizes the window to the specified size.,指定されたサイズにウィンドウをリサイズします。
winname : Window name.,winname : ウィンドウの名前です。
width : The new window width.,width : 新しいウィンドウの幅です。
height : The new window height.,height : 新しいウィンドウの高さを指定します。
NoteThe specified window size is for the image area. Toolbars are not counted.,注意：指定されたウィンドウサイズは画像領域のサイズです。ツールバーはカウントされません．
Only windows created without cv::WINDOW_AUTOSIZE flag can be resized.,cv::WINDOW_AUTOSIZE フラグを持たずに作成されたウィンドウだけが，リサイズ可能です．
Moves the window to the specified position.,ウィンドウを指定された位置に移動させます．
x : The new x-coordinate of the window.,x : ウィンドウの新しいx座標．
y : The new y-coordinate of the window.,y :ウィンドウの新しい y 座標．
"Examples: modules/shape/samples/shape_example.cpp, samples/cpp/image_alignment.cpp, samples/cpp/polar_transforms.cpp, samples/cpp/tutorial_code/ImgProc/basic_drawing/Drawing_1.cpp, samples/cpp/tutorial_code/ImgProc/Morphology_1.cpp, and samples/cpp/warpPerspective_demo.cpp.",例: modules/shape/samples/shape_example.cpp、samples/cpp/image_alignment.cpp、samples/cpp/polar_transforms.cpp、samples/cpp/tutorial_code/ImgProc/basic_drawing/Drawing_1.cpp、samples/cpp/tutorial_code/ImgProc/Morphology_1.cpp、samples/cpp/warpPerspective_demo.cpp.
Changes parameters of a window dynamically.,ウィンドウのパラメータを動的に変更します。
prop_id : Window property to edit. The supported operation flags are: (cv::WindowPropertyFlags),prop_id : 編集するウィンドウのプロパティです。サポートされる操作フラグは以下の通りです．(cv::WindowPropertyFlags)
prop_value : New value of the window property. The supported flags are: (cv::WindowFlags),prop_value : ウィンドウプロパティの新しい値．サポートされるフラグは以下の通りです．(cv::WindowFlags)
The function setWindowProperty enables changing properties of a window.,関数 setWindowProperty は，ウィンドウのプロパティを変更することができます．
Updates window title.,ウィンドウのタイトルを更新します．
title : New title.,title : 新しいタイトル．
Provides parameters of a window.,ウィンドウのパラメータを提供します．
prop_id : Window property to retrieve. The following operation flags are available: (cv::WindowPropertyFlags),prop_id : 取得するウィンドウプロパティ。以下の操作フラグがあります．(cv::WindowPropertyFlags)
The function getWindowProperty returns properties of a window.See alsosetWindowProperty,関数 getWindowProperty は，ウィンドウのプロパティを返します．
Provides rectangle of image in the window.,ウィンドウ内の画像の矩形領域を提供します．
"The function getWindowImageRect returns the client screen coordinates, width and height of the image rendering area.See alsoresizeWindow moveWindow",関数 getWindowImageRectは、イメージのレンダリング領域のクライアント スクリーン座標、幅、高さを返します。
Sets mouse handler for the specified window.,指定されたウィンドウのマウス ハンドラを設定します。
onMouse : Callback function for mouse events. See OpenCV samples on how to specify and use the callback.,onMouse : マウスイベント用のコールバック関数．コールバックの指定方法や使用方法については，OpenCVのサンプルを参照してください．
userdata : The optional parameter passed to the callback.,userdata :コールバックに渡されるオプションのパラメータ．
"Examples: samples/cpp/camshiftdemo.cpp, samples/cpp/create_mask.cpp, samples/cpp/ffilldemo.cpp, samples/cpp/grabcut.cpp, samples/cpp/lkdemo.cpp, samples/cpp/warpPerspective_demo.cpp, and samples/cpp/watershed.cpp.",例： samples/cpp/camshiftdemo.cpp，samples/cpp/create_mask.cpp，samples/cpp/ffilldemo.cpp，samples/cpp/grabcut.cpp，samples/cpp/lkdemo.cpp，samples/cpp/warpPerspective_demo.cpp，samples/cpp/watershed.cpp．
"Gets the mouse-wheel motion delta, when handling mouse-wheel events cv::EVENT_MOUSEWHEEL and cv::EVENT_MOUSEHWHEEL.",マウスホイールイベント cv::EVENT_MOUSEWHEEL と cv::EVENT_MOUSEHWHEEL を扱う際に，マウスホイールの動きのデルタ値を取得します．
flags : The mouse callback flags parameter.,flags :マウスコールバックフラグのパラメータ．
"For regular mice with a scroll-wheel, delta will be a multiple of 120. The value 120 corresponds to a one notch rotation of the wheel or the threshold for action to be taken and one such action should occur for each delta. Some high-precision mice with higher-resolution freely-rotating wheels may generate smaller values.For cv::EVENT_MOUSEWHEEL positive and negative values mean forward and backward scrolling, respectively. For cv::EVENT_MOUSEHWHEEL, where available, positive and negative values mean right and left scrolling, respectively.NoteMouse-wheel events are currently supported only on Windows.",スクロールホイールを備えた通常のマウスでは，delta は 120 の倍数になります．値120は，ホイールの1ノッチ回転に相当し，アクションを起こすための閾値となり，deltaごとに1つのアクションが発生します．cv::EVENT_MOUSEWHEEL において，正の値は前方へのスクロールを，負の値は後方へのスクロールを意味します．cv::EVENT_MOUSEHWHEEL では，正と負の値は，それぞれ前方と後方へのスクロールを意味し，正と負の値は，それぞれ右と左へのスクロールを意味する．
Allows users to select a ROI on the given image.,与えられた画像上の ROI を選択することができる．
windowName : name of the window where selection process will be shown.,windowName : 選択プロセスが表示されるウィンドウの名前。
img : image to select a ROI.,img : ROI を選択する画像を表示します．
showCrosshair : if true crosshair of selection rectangle will be shown.,showCrosshair : if true selection rectangleの十字線が表示されます。
fromCenter : if true center of selection will match initial mouse position. In opposite case a corner of selection rectangle will correspont to the initial mouse position.,fromCenter : trueの場合、選択範囲の中心がマウスの初期位置と一致します。fromCenter : trueの場合、選択範囲の中心がマウスの初期位置と一致します。逆の場合、選択範囲の角がマウスの初期位置と一致します。
"The function creates a window and allows users to select a ROI using the mouse. Controls: use space or enter to finish selection, use key c to cancel selection (function will return the zero cv::Rect).NoteThe function sets it's own mouse callback for specified window using cv::setMouseCallback(windowName, ...). After finish of work an empty callback will be set for the used window.","この関数は、ウィンドウを作成し、ユーザがマウスを使ってROIを選択できるようにします。コントロール：選択を終了するには，Space または Enter キーを利用し，選択をキャンセルするには，c キーを利用します（この関数は，ゼロの cv::Rect を返します）．注意この関数は，cv::setMouseCallback(windowName, ...)を用いて，指定されたウィンドウに対する独自のマウスコールバックを設定します．作業終了後は，使用されたウィンドウに対して空のコールバックがセットされます．"
Allows users to select multiple ROIs on the given image.,ユーザは，与えられた画像上の複数のROIを選択することができます．
boundingBoxes : selected ROIs.,boundingBoxes : 選択されたROI。
"The function creates a window and allows users to select multiple ROIs using the mouse. Controls: use space or enter to finish current selection and start a new one, use esc to terminate multiple ROI selection process.NoteThe function sets it's own mouse callback for specified window using cv::setMouseCallback(windowName, ...). After finish of work an empty callback will be set for the used window.","この関数は，ウィンドウを作成し，ユーザがマウスを使って複数のROIを選択できるようにします．注意この関数は，cv::setMouseCallback(windowName, ...)を用いて，指定されたウィンドウに対する独自のマウスコールバックを設定します．作業終了後は，使用されたウィンドウに対して空のコールバックがセットされます．"
Creates a trackbar and attaches it to the specified window.,トラックバーを作成して，指定されたウィンドウに取り付けます．
trackbarname : Name of the created trackbar.,trackbarname : 作成されたトラックバーの名前。
winname : Name of the window that will be used as a parent of the created trackbar.,winname : 作成されたトラックバーの親として使用されるウィンドウの名前です。
"value : Optional pointer to an integer variable whose value reflects the position of the slider. Upon creation, the slider position is defined by this variable.",value : スライダーの位置を反映する整数変数への任意のポインタ。作成時、スライダーの位置はこの変数で定義されます。
count : Maximal position of the slider. The minimal position is always 0.,count :スライダーの最大位置を指定します。最小位置は常に0です。
"onChange : Pointer to the function to be called every time the slider changes position. This function should be prototyped as void Foo(int,void*); , where the first parameter is the trackbar position and the second parameter is the user data (see the next parameter). If the callback is the NULL pointer, no callbacks are called, but only value is updated.","onChange : スライダーの位置が変わるたびに呼び出される関数へのポインタ。この関数は， void Foo(int,void*); といった形でプロトタイプを作成する必要があり，第1パラメータはトラックバーの位置，第2パラメータはユーザーデータ（次のパラメータを参照）となります．コールバックがNULLポインタの場合は，コールバックは呼ばれず，値だけが更新されます．"
userdata : User data that is passed as is to the callback. It can be used to handle trackbar events without using global variables.,userdata :コールバックにそのまま渡されるユーザーデータ。グローバル変数を使わずにトラックバーのイベントを処理するのに利用できます．
"The function createTrackbar creates a trackbar (a slider or range control) with the specified name and range, assigns a variable value to be a position synchronized with the trackbar and specifies the callback function onChange to be called on the trackbar position change. The created trackbar is displayed in the specified window winname.Note[Qt Backend Only] winname can be empty if the trackbar should be attached to the control panel.Clicking the label of each trackbar enables editing the trackbar values manually.Examples: samples/cpp/camshiftdemo.cpp, samples/cpp/connected_components.cpp, samples/cpp/contours2.cpp, samples/cpp/demhist.cpp, samples/cpp/distrans.cpp, samples/cpp/edge.cpp, samples/cpp/falsecolor.cpp, samples/cpp/ffilldemo.cpp, samples/cpp/fitellipse.cpp, samples/cpp/laplace.cpp, samples/cpp/pca.cpp, samples/cpp/tutorial_code/HighGUI/AddingImagesTrackbar.cpp, samples/cpp/tutorial_code/Histograms_Matching/MatchTemplate_Demo.cpp, samples/cpp/tutorial_code/ImgProc/Morphology_1.cpp, samples/cpp/tutorial_code/ImgProc/Morphology_2.cpp, and samples/dnn/object_detection.cpp.","関数 createTrackbar は、指定された名前と範囲のトラックバー（スライダやレンジコントロール）を作成し、トラックバーと同期した位置となる変数値を割り当て、トラックバーの位置変更時に呼び出されるコールバック関数 onChange を指定します。各トラックバーのラベルをクリックすると，トラックバーの値を手動で編集することができます．例：samples/cpp/camshiftdemo.cpp，samples/cpp/connected_components.cpp，samples/cpp/contours2.cpp，samples/cpp/demhist.cpp，samples/cpp/distrans.cpp，samples/cpp/edge.cpp, samples/cpp/falecolor.cpp, samples/cpp/ffilldemo.cpp, samples/cpp/fitellipse.cpp, samples/cpp/laplace.cpp, samples/cpp/pca.cpp, samples/cpp/tutorial_code/HighGUI/AddingImagesTrackbar.cpp、samples/cpp/tutorial_code/Histograms_Matching/MatchTemplate_Demo.cpp、samples/cpp/tutorial_code/ImgProc/Morphology_1.cpp、samples/cpp/tutorial_code/ImgProc/Morphology_2.cpp、samples/dnn/object_detection.cpp。"
Returns the trackbar position.,トラックバーの位置を返す。
trackbarname : Name of the trackbar.,trackbarname : トラックバーの名前。
winname : Name of the window that is the parent of the trackbar.,winname : トラックバーの親となるウィンドウの名前。
The function returns the current position of the specified trackbar.Note[Qt Backend Only] winname can be empty if the trackbar is attached to the control panel.,注意[Qt Backend Only] トラックバーがコントロールパネルに取り付けられている場合、winnameは空にすることができます。
Sets the trackbar position.,トラックバーの位置を設定します。
winname : Name of the window that is the parent of trackbar.,winname : トラックバーの親となるウィンドウの名前です。
pos : New position.,pos :新しい位置です。
The function sets the position of the specified trackbar in the specified window.Note[Qt Backend Only] winname can be empty if the trackbar is attached to the control panel.Examples: samples/cpp/falsecolor.cpp.,注意[Qt Backend Only] トラックバーがコントロールパネルに取り付けられている場合、winnameは空にすることができます。例: samples/cpp/falecolor.cpp.
Sets the trackbar maximum position.,トラックバーの最大位置を設定します。
maxval : New maximum position.,maxval : 新しい最大位置。
The function sets the maximum position of the specified trackbar in the specified window.Note[Qt Backend Only] winname can be empty if the trackbar is attached to the control panel.Examples: samples/cpp/falsecolor.cpp.,注意[Qt Backend Only] トラックバーがコントロールパネルに取り付けられている場合、winnameは空にすることができます.例: samples/cpp/falecolor.cpp.
Sets the trackbar minimum position.,トラックバーの最小位置を設定します。
minval : New minimum position.,minval : 新しい最小位置を指定します。
The function sets the minimum position of the specified trackbar in the specified window.Note[Qt Backend Only] winname can be empty if the trackbar is attached to the control panel.Examples: samples/cpp/falsecolor.cpp.,注意[Qt Backend Only] トラックバーがコントロールパネルに装着されている場合、winnameは空にすることができます.例: samples/cpp/falecolor.cpp.
Loads an image from a file.,ファイルから画像を読み込みます。
filename : Name of file to be loaded.,filename : 読み込まれるファイルの名前。
flags : Flag that can take values of cv::ImreadModes,flags :cv::ImreadModes の値を取ることができるフラグ．
"The function imread loads an image from the specified file and returns it. If the image cannot be read (because of missing file, improper permissions, unsupported or invalid format), the function returns an empty matrix ( Mat::data==NULL ).Currently, the following file formats are supported:Windows bitmaps - *.bmp, *.dib (always supported)",関数 imread は，指定されたファイルから画像を読み込み，それを返します．画像が読み込めない場合（ファイルが存在しない，不適切なパーミッション，サポートされていない，あるいは無効なフォーマットのため），この関数は空の行列（ Mat::data==NULL ）を返します．
"JPEG files - *.jpeg, *.jpg, *.jpe (see the Note section)","JPEGファイル - *.jpeg, *.jpg, *.jpe (Noteセクションを参照)"
JPEG 2000 files - *.jp2 (see the Note section),JPEG 2000ファイル - *.jp2 (注意事項を参照)
Portable Network Graphics - *.png (see the Note section),ポータブルネットワークグラフィックス - *.png (注釈欄参照)
WebP - *.webp (see the Note section),WebP - *.webp（注釈欄参照）
"Portable image format - *.pbm, *.pgm, *.ppm *.pxm, *.pnm (always supported)","ポータブルイメージフォーマット - *.pbm, *.pgm, *.ppm *.pxm, *.pnm (常時サポート)"
PFM files - *.pfm (see the Note section),PFMファイル - *.pfm (Noteセクションを参照)
"Sun rasters - *.sr, *.ras (always supported)","太陽ラスタ - *.sr, *.ras (常にサポートされています)"
"TIFF files - *.tiff, *.tif (see the Note section)","TIFFファイル - *.tiff, *.tif (Noteセクションを参照)"
OpenEXR Image files - *.exr (see the Note section),OpenEXR画像ファイル - *.exr (注釈欄参照)
"Radiance HDR - *.hdr, *.pic (always supported)","Radiance HDR - *.hdr, *.pic (常時サポート)"
Raster and Vector geospatial data supported by GDAL (see the Note section)Note,GDALでサポートされているラスターおよびベクターの地理空間データ（注釈セクションを参照）注釈
"The function determines the type of an image by the content, not by the file extension.",本機能では、拡張子ではなく、画像の内容で画像の種類を判断します。
"In the case of color images, the decoded images will have the channels stored in B G R order.",カラー画像の場合、デコードされた画像はB G Rの順にチャンネルが格納されます。
"When using IMREAD_GRAYSCALE, the codec's internal grayscale conversion will be used, if available. Results may differ to the output of cvtColor()",IMREAD_GRAYSCALE を使用した場合，コーデック内部のグレースケール変換が可能であれば，それが使用されます。結果は cvtColor() の出力と異なる場合があります。
"On Microsoft Windows* OS and MacOSX*, the codecs shipped with an OpenCV image (libjpeg, libpng, libtiff, and libjasper) are used by default. So, OpenCV can always read JPEGs, PNGs, and TIFFs. On MacOSX, there is also an option to use native MacOSX image readers. But beware that currently these native image loaders give images with different pixel values because of the color management embedded into MacOSX.","Microsoft Windows* OSやMacOSX*では，OpenCVの画像に同梱されているコーデック（libjpeg, libpng, libtiff, libjasper）がデフォルトで利用されます．そのため，OpenCV は常に JPEG, PNG, TIFF を読み込むことができます．MacOSXでは，MacOSXのネイティブな画像リーダを利用するオプションもあります．しかし，現在のところ，MacOSXに組み込まれたカラーマネージメントのために，これらのネイティブ画像ローダは，異なるピクセル値の画像を与えることに注意してください．"
"On Linux*, BSD flavors and other Unix-like open-source operating systems, OpenCV looks for codecs supplied with an OS image. Install the relevant packages (do not forget the development files, for example, ""libjpeg-dev"", in Debian* and Ubuntu*) to get the codec support or turn on the OPENCV_BUILD_3RDPARTY_LIBS flag in CMake.","Linux*，BSDフレーバー，その他のUnix系オープンソースOSでは，OpenCVはOSイメージに含まれるコーデックを探します．コーデックをサポートするために，関連するパッケージ（Debian* や Ubuntu* では，""libjpeg-dev"" のような開発ファイルも忘れないでください）をインストールするか，CMake で OPENCV_BUILD_3RDPARTY_LIBS フラグをオンにしてください．"
"In the case you set WITH_GDAL flag to true in CMake and IMREAD_LOAD_GDAL to load the image, then the GDAL driver will be used in order to decode the image, supporting the following formats: Raster, Vector.",CMakeでWITH_GDALフラグをtrueに設定し、IMREAD_LOAD_GDALで画像をロードした場合、画像をデコードするためにGDALドライバが使用され、以下のフォーマットをサポートします。ラスター、ベクター。
"If EXIF information is embedded in the image file, the EXIF orientation will be taken into account and thus the image will be rotated accordingly except if the flags IMREAD_IGNORE_ORIENTATION or IMREAD_UNCHANGED are passed.",画像ファイルにEXIF情報が埋め込まれている場合、EXIFの向きが考慮され、フラグIMREAD_IGNORE_ORIENTATIONまたはIMREAD_UNCHANGEDが渡される場合を除き、それに応じて画像が回転します。
Use the IMREAD_UNCHANGED flag to keep the floating point values from PFM image.,PFM画像から浮動小数点値を保持するためには、IMREAD_UNCHANGEDフラグを使用する。
"By default number of pixels must be less than 2^30. Limit can be set using system variable OPENCV_IO_MAX_IMAGE_PIXELSExamples: fld_lines.cpp, modules/shape/samples/shape_example.cpp, samples/cpp/connected_components.cpp, samples/cpp/create_mask.cpp, samples/cpp/demhist.cpp, samples/cpp/distrans.cpp, samples/cpp/edge.cpp, samples/cpp/facedetect.cpp, samples/cpp/falsecolor.cpp, samples/cpp/ffilldemo.cpp, samples/cpp/fitellipse.cpp, samples/cpp/grabcut.cpp, samples/cpp/image_alignment.cpp, samples/cpp/pca.cpp, samples/cpp/squares.cpp, samples/cpp/stitching.cpp, samples/cpp/stitching_detailed.cpp, samples/cpp/train_HOG.cpp, samples/cpp/tutorial_code/features2D/Homography/decompose_homography.cpp, samples/cpp/tutorial_code/features2D/Homography/homography_from_camera_displacement.cpp, samples/cpp/tutorial_code/features2D/Homography/pose_from_homography.cpp, samples/cpp/tutorial_code/HighGUI/AddingImagesTrackbar.cpp, samples/cpp/tutorial_code/Histograms_Matching/MatchTemplate_Demo.cpp, samples/cpp/tutorial_code/ImgProc/Morphology_1.cpp, samples/cpp/tutorial_code/ImgProc/Morphology_2.cpp, samples/cpp/tutorial_code/ImgProc/Pyramids/Pyramids.cpp, samples/cpp/tutorial_code/ImgProc/Smoothing/Smoothing.cpp, samples/cpp/tutorial_code/ImgTrans/copyMakeBorder_demo.cpp, samples/cpp/tutorial_code/ImgTrans/houghcircles.cpp, samples/cpp/tutorial_code/ImgTrans/houghlines.cpp, samples/cpp/tutorial_code/ImgTrans/Sobel_Demo.cpp, samples/cpp/tutorial_code/ml/introduction_to_pca/introduction_to_pca.cpp, samples/cpp/tutorial_code/photo/non_photorealistic_rendering/npr_demo.cpp, samples/cpp/tutorial_code/photo/seamless_cloning/cloning_demo.cpp, samples/cpp/warpPerspective_demo.cpp, samples/cpp/watershed.cpp, samples/dnn/colorization.cpp, samples/dnn/openpose.cpp, samples/tapi/hog.cpp, and samples/tapi/squares.cpp.","デフォルトでは、ピクセル数は2^30以下でなければなりません。システム変数OPENCV_IO_MAX_IMAGE_PIXELSEで設定できます。サンプル：fld_lines.cpp, modules/shape/samples/shape_example.cpp, samples/cpp/connected_components.cpp, samples/cpp/create_mask.cpp, samples/cpp/demhist.cpp, samples/cpp/distrans.cpp, samples/cpp/edge.cpp, samples/cpp/facedetect.cpp, samples/cpp/falecolor.cpp, samples/cpp/ffilldemo.cpp, samples/cpp/fitellipse.cpp, samples/cpp/grabcut.cpp, samples/cpp/image_alignment.cpp, samples/cpp/pca.cpp, samples/cpp/squares.cpp, samples/cpp/stitching.cpp, samples/cpp/stitching_detailed.cpp, samples/cpp/train_HOG.cpp, samples/cpp/tutorial_code/features2D/Homography/decompose_homography.cpp, samples/cpp/tutorial_code/features2D/Homography/decompose_homography.cpp, samples/cpp/tutorial_code/features2D/Homography/homography_from_camera_displacement.cpp, samples/cpp/tutorial_code/features2D/Homography/pose_from_homography.cpp, samples/cpp/tutorial_code/HighGUI/AddingImagesTrackbar.cpp, samples/cpp/tutorial_code/Histograms_Matching/MatchTemplate_Demo.cpp, samples/cpp/tutorial_code/ImgProc/Morphology_1.cpp, samples/cpp/tutorial_code/ImgProc/Morphology_2.cpp, samples/cpp/tutorial_code/ImgProc/Pyramids/Pyramids.cpp, samples/cpp/tutorial_code/ImgProc/Smoothing/Smoothing.cpp, samples/cpp/tutorial_code/ImgTrans/copyMakeBorder_demo.cpp, samples/cpp/tutorial_code/ImgTrans/houghcircles.cpp, samples/cpp/tutorial_code/ImgTrans/houghlines.cpp, samples/cpp/tutorial_code/ImgTrans/Sobel_Demo.cpp, samples/cpp/tutorial_code/ml/introduction_to_pca/introduction_to_pca.cpp, samples/cpp/tutorial_code/photo/non_photorealistic_rendering/npr_demo.cpp、samples/cpp/tutorial_code/photo/seamless_cloning/cloning_demo.cpp、samples/cpp/warpPerspective_demo.cpp、samples/cpp/watershed.cpp、samples/dnn/colorization.cpp、samples/dnn/openpose.cpp、samples/tapi/hog.cpp、samples/tapi/squares.cppです。"
Loads a multi-page image from a file.,複数ページの画像をファイルから読み込みます。
"flags : Flag that can take values of cv::ImreadModes, default with cv::IMREAD_ANYCOLOR.",flags :cv::ImreadModes の値を取ることができるフラグ，デフォルトは cv::IMREAD_ANYCOLOR です．
"mats : A vector of Mat objects holding each page, if more than one.",mats : 各ページを保持する Mat オブジェクトのベクトル（複数の場合）．
The function imreadmulti loads a multi-page image from the specified file into a vector of Mat objects.See alsocv::imread,関数 imreadmulti は，複数ページにわたる画像を指定されたファイルから Mat オブジェクトのベクトルに読み込みます．
Saves an image to a specified file.,画像を指定されたファイルに保存します．
filename : Name of the file.,filename : ファイルの名前．
img : (Mat or vector of Mat) Image or Images to be saved.,img : (Mat または Mat のベクトル) 保存される画像またはイメージ．
"params : Format-specific parameters encoded as pairs (paramId_1, paramValue_1, paramId_2, paramValue_2, ... .) see cv::ImwriteFlags","params :フォーマット固有のパラメータを，ペア（paramId_1, paramValue_1, paramId_2, paramValue_2, ... ...）でエンコードしたもの．"
"The function imwrite saves the image to the specified file. The image format is chosen based on the filename extension (see cv::imread for the list of extensions). In general, only 8-bit single-channel or 3-channel (with 'BGR' channel order) images can be saved using this function, with these exceptions:16-bit unsigned (CV_16U) images can be saved in the case of PNG, JPEG 2000, and TIFF formats","関数 imwrite は，画像を指定されたファイルに保存します．画像フォーマットは，ファイル名の拡張子に基づいて選択されます（拡張子のリストについては cv::imread を参照してください）．一般に，この関数を用いて保存できるのは，8ビットのシングルチャンネルまたは3チャンネル（チャンネル順序は 'BGR' ）の画像のみですが，以下のような例外があります： 16ビット符号なし（CV_16U）の画像は，PNG, JPEG 2000, TIFF フォーマットの場合に保存できます．"
"32-bit float (CV_32F) images can be saved in PFM, TIFF, OpenEXR, and Radiance HDR formats; 3-channel (CV_32FC3) TIFF images will be saved using the LogLuv high dynamic range encoding (4 bytes per pixel)","32ビット浮動小数点（CV_32F）画像は，PFM, TIFF, OpenEXR, Radiance HDR 形式で保存できます．3チャンネル（CV_32FC3）TIFF 画像は，LogLuv ハイダイナミックレンジエンコーディング（1ピクセルあたり4バイト）で保存されます．"
"PNG images with an alpha channel can be saved using this function. To do this, create 8-bit (or 16-bit) 4-channel image BGRA, where the alpha channel goes last. Fully transparent pixels should have alpha set to 0, fully opaque pixels should have alpha set to 255/65535 (see the code sample below).",アルファチャンネル付きのPNG画像もこの機能を使って保存できます。これを行うには，8ビット（あるいは16ビット）の4チャンネル画像BGRAを作成し，アルファチャンネルを最後に置きます。完全に透明なピクセルは，アルファを 0 に，完全に不透明なピクセルは，アルファを 255/65535 に設定する必要があります（以下のコードサンプルを参照してください）．
"Multiple images (vector of Mat) can be saved in TIFF format (see the code sample below).If the image format is not supported, the image will be converted to 8-bit unsigned (CV_8U) and saved that way.If the format, depth or channel order is different, use Mat::convertTo and cv::cvtColor to convert it before saving. Or, use the universal FileStorage I/O functions to save the image to XML or YAML format.The sample below shows how to create a BGRA image, how to set custom compression parameters and save it to a PNG file. It also demonstrates how to save multiple images in a TIFF file:#include <opencv2/imgcodecs.hpp>using namespace cv;using namespace std;static void paintAlphaMat(Mat &mat){    CV_Assert(mat.channels() == 4);    for (int i = 0; i < mat.rows; ++i)    {        for (int j = 0; j < mat.cols; ++j)        {            Vec4b& bgra = mat.at<Vec4b>(i, j);            bgra[0] = UCHAR_MAX; // Blue            bgra[1] = saturate_cast<uchar>((float (mat.cols - j)) / ((float)mat.cols) * UCHAR_MAX); // Green            bgra[2] = saturate_cast<uchar>((float (mat.rows - i)) / ((float)mat.rows) * UCHAR_MAX); // Red            bgra[3] = saturate_cast<uchar>(0.5 * (bgra[1] + bgra[2])); // Alpha        }    }}int main(){    Mat mat(480, 640, CV_8UC4); // Create a matrix with alpha channel    paintAlphaMat(mat);    vector<int> compression_params;    compression_params.push_back(IMWRITE_PNG_COMPRESSION);    compression_params.push_back(9);    bool result = false;    try    {        result = imwrite(""alpha.png"", mat, compression_params);    }    catch (const cv::Exception& ex)    {        fprintf(stderr, ""Exception converting image to PNG format: %s\n"", ex.what());    }    if (result)        printf(""Saved PNG file with alpha data.\n"");    else        printf(""ERROR: Can't save PNG file.\n"");    vector<Mat> imgs;    imgs.push_back(mat);    imgs.push_back(~mat);    imgs.push_back(mat(Rect(0, 0, mat.cols / 2, mat.rows / 2)));    imwrite(""test.tiff"", imgs);    printf(""Multiple files saved in test.tiff\n"");    return result ? 0 : 1;}fragmentExamples: samples/cpp/image_alignment.cpp, samples/cpp/stitching.cpp, samples/cpp/stitching_detailed.cpp, samples/cpp/tutorial_code/photo/seamless_cloning/cloning_demo.cpp, samples/tapi/hog.cpp, and samples/tapi/squares.cpp.","複数の画像（Mat のベクトル）は，TIFF 形式で保存することができます（以下のコードサンプルを参照してください）．画像フォーマットがサポートされていない場合，画像は 8 ビット符号なし（CV_8U）に変換され，そのように保存されます．フォーマット，ビット深度，チャンネル順序が異なる場合は， Mat::convertTo や cv::cvtColor を用いて，保存前に変換してください．あるいは，汎用の FileStorage I/O 関数を利用して，画像を XML や YAML 形式で保存します．以下のサンプルでは，BGRA 画像の作成方法，カスタム圧縮パラメータの設定方法，PNG ファイルへの保存方法を示しています．以下のサンプルは，BGRA 画像を作成し，カスタム圧縮パラメータを設定して PNG ファイルに保存する方法と，複数の画像を 1 つの TIFF ファイルに保存する方法を示しています： #include <opencv2/imgcodecs.hpp>using namespace cv;using namespace std;static void paintAlphaMat(Mat &mat){ CV_Assert(mat.channels() == 4); for (int i = 0; i < mat.rows; ++i) { for (int j = 0; j < mat.cols; ++j) { Vec4b& bgra = mat.at<Vec4b>(i, j); bgra[0] = UCHAR_MAX; // 青 bgra[1] = saturate_cast<uchar>((float (mat.cols - j)) / ((float)mat.cols./ ((float)mat.cols) * UCHAR_MAX); // 緑 bgra[2] = saturate_cast<uchar>((float (mat.rows - i))/ ((float)mat.rows) * UCHAR_MAX); // 赤 bgra[3] = saturate_cast<uchar>(0.5 * (bgra[1] + bgra[2])); // α }。    }}int main(){ Mat mat(480, 640, CV_8UC4); // アルファチャンネルを持つ行列を作成 paintAlphaMat(mat); vector<int> compression_params; compression_params.push_back(IMWRITE_PNG_COMPRESSION); compression_params.push_back(9); bool result = false; try { result = imwrite(""alpha.png"", mat, compression_params); } catch (const cv::Exception& ex) { fprintf(stderr, ""Exception converting image to PNG format: %s\n"", ex.what()); } if (result) printf(""Saved PNG file with alpha data.\\""); else printf(""ERROR: Can't save PNG file.\\""); vector<Mat> imgs; imgs.push_back(mat); imgs.push_back(~mat); imgs.push_back(mat(Rect(0, 0, mat.cols / 2, mat.rows / 2))); imwrite(""test.tiff"", imgs); printf(""Multiple files saved in test.tiff\n""); return result ?0 : 1;}fragmentExamples: samples/cpp/image_alignment.cpp, samples/cpp/stitching.cpp, samples/cpp/stitching_detailed.cpp, samples/cpp/tutorial_code/photo/seamless_cloning/cloning_demo.cpp, samples/tapi/hog.cpp, samples/tapi/squares.cpp."
Reads an image from a buffer in memory.,メモリ上のバッファから画像を読み込みます。
buf : Input array or vector of bytes.,buf : バイトの入力配列またはベクトル。
"flags : The same flags as in cv::imread, see cv::ImreadModes.",flags :cv::imread の場合と同じフラグ， cv::ImreadModes を参照してください．
"The function imdecode reads an image from the specified buffer in the memory. If the buffer is too short or contains invalid data, the function returns an empty matrix ( Mat::data==NULL ).See cv::imread for the list of supported formats and flags description.NoteIn the case of color images, the decoded images will have the channels stored in B G R order.",関数 imdecode は，メモリ上の指定されたバッファから画像を読み込みます．サポートされるフォーマットのリストとフラグの説明は， cv::imread を参照してください．注意カラー画像の場合，デコードされた画像は，B G R の順にチャンネルが格納されます．
Encodes an image into a memory buffer.,画像をメモリバッファにエンコードします．
ext : File extension that defines the output format.,ext : 出力フォーマットを定義するファイルの拡張子です。
img : Image to be written.,img : 書き込まれる画像。
buf : Output buffer resized to fit the compressed image.,buf :buf : 圧縮された画像に合わせてリサイズされた出力バッファ。
params : Format-specific parameters. See cv::imwrite and cv::ImwriteFlags.,params :フォーマット固有のパラメータ．cv::imwrite と cv::ImwriteFlags を参照してください．
The function imencode compresses the image and stores it in the memory buffer that is resized to fit the result. See cv::imwrite for the list of supported formats and flags description.,関数 imencode は，画像を圧縮し，その結果に合わせてリサイズされたメモリバッファに格納します．サポートされるフォーマットのリストとフラグの説明については， cv::imwrite を参照してください．
Returns true if an image with the specified filename can be encoded by OpenCV.,指定された filename の画像が，OpenCV でエンコードできる場合に true を返します．
filename : File name of the image,filename : 画像のファイル名．
Computes hash of the input image.,入力画像のハッシュを計算します．
inputArr : input image want to compute hash value,inputArr : ハッシュ値を計算したい入力画像．
outputArr : hash of the image,outputArr : 入力画像のハッシュ値
Compare the hash value between inOne and inTwo.,inOneとinTwoのハッシュ値を比較します．
hashOne : Hash value one,hashOne : ハッシュ値1
hashTwo : Hash value two,hashTwo : ハッシュ値2
Computes average hash value of the input image. ,入力画像の平均的なハッシュ値を計算します。
"This is a fast image hashing algorithm, but only work on simple case. For more details, please refer to [130] ",これは高速な画像ハッシュアルゴリズムですが、単純なケースでしか動作しません。詳しくは、[130]を参照してください。
Image hash based on block mean. ,ブロック平均に基づく画像ハッシュ。
See [289] for details. ,詳細は[289]を参照してください。
Create BlockMeanHash object.,BlockMeanHashオブジェクトを作成します。
mode : the mode,mode : モード
Image hash based on color moments. ,カラーモーメントに基づく画像ハッシュ。
See [238] for details. ,詳しくは[238]を参照してください．
alpha : int scale factor for marr wavelet (default=2).,alpha : marrウェーブレットのスケールファクター（デフォルトは2）．
scale : int level of scale factor (default = 1),scale : スケールファクタのレベルを表す整数 (デフォルト=1)
"Marr-Hildreth Operator Based Hash, slowest but more discriminative. ","Marr-Hildreth Operator Based Hash, 最も遅いですが，より識別性が高いです．"
Set Mh kernel parameters.,Mhカーネルのパラメータを設定します。
self explain,自己説明
Image hash based on Radon transform. ,ラドン変換に基づく画像ハッシュです．
Detect lines inside an image.,画像内の線を検出します．
keypoints : vector that will store extracted lines for one or more images,keypoints : 1つまたは複数の画像に対して抽出された線を格納するベクトル．
scale : scale factor used in pyramids generation,scale : ピラミッドを生成する際に利用されるスケールファクター．
numOctaves : number of octaves inside pyramid,numOctaves : ピラミッド内部のオクターブ数．
mask : mask matrix to detect only KeyLines of interest,mask : 注目するキーラインのみを検出するためのマスク行列
Updates the motion history image by a moving silhouette.,シルエットが動くことで動体視力を更新します。
silhouette : Silhouette mask that has non-zero pixels where the motion occurs.,silhouette : 動きのある部分の画素数が0ではないシルエットマスク。
"mhi : Motion history image that is updated by the function (single-channel, 32-bit floating-point).",mhi : この関数によって更新されるモーションヒストリー画像（シングルチャンネル，32ビット浮動小数点）．
timestamp : Current time in milliseconds or other units.,timestamp : 現在の時刻をミリ秒などの単位で表したもの．
duration : Maximal duration of the motion track in the same units as timestamp .,duration : モーショントラックの最大継続時間（timestamp と同じ単位）．
"The function updates the motion history image as follows:\[\texttt{mhi} (x,y)= \forkthree{\texttt{timestamp}}{if \(\texttt{silhouette}(x,y) \ne 0\)}{0}{if \(\texttt{silhouette}(x,y) = 0\) and \(\texttt{mhi} < (\texttt{timestamp} - \texttt{duration})\)}{\texttt{mhi}(x,y)}{otherwise}\]That is, MHI pixels where the motion occurs are set to the current timestamp , while the pixels where the motion happened last time a long time ago are cleared.The function, together with calcMotionGradient and calcGlobalOrientation , implements a motion templates technique described in [53] and [32] .","この関数は，次のように動作履歴画像を更新します．(x,y)= \\{timestamp}}{if ❶(\\{silhouette}(x,y) = 0\)}{0}{if ❷(\{silhouette}(x,y) = 0\)and ❷(\{mhi}) < (୨୧-͈ᴗ-͈)< (˶‾᷄ -̫ ‾᷅˵) }{\\}(x,y)}{otherwise}]つまり、動きが発生したMHIピクセルは、現在のタイムスタンプに設定され、前回ずっと前に動きが発生したピクセルはクリアされます。この関数は、calcMotionGradient、calcGlobalOrientationとともに、[53]や[32]で述べられているモーションテンプレート技術を実装しています。"
Calculates a gradient orientation of a motion history image.,モーションヒストリー画像のグラデーションオリエンテーションを計算します．
mhi : Motion history single-channel floating-point image.,mhi : モーションヒストリーのシングルチャンネル浮動小数点型画像．
mask : Output mask image that has the type CV_8UC1 and the same size as mhi . Its non-zero elements mark pixels where the motion gradient data is correct.,mask : CV_8UC1 型で，mhi と同じサイズの出力マスク画像．非0の要素は，動きのグラデーションデータが正しいピクセルを示します．
"orientation : Output motion gradient orientation image that has the same type and the same size as mhi . Each pixel of the image is a motion orientation, from 0 to 360 degrees.",orientation :mhi と同じ型，同じサイズの，動き勾配を持つ方位画像を出力します．この画像の各ピクセルは、0度から360度までの動きの方向を表しています。
delta1 : Minimal (or maximal) allowed difference between mhi values within a pixel neighborhood.,delta1 : ピクセルの近傍における mhi 値の差の最小値（または最大値）．
"delta2 : Maximal (or minimal) allowed difference between mhi values within a pixel neighborhood. That is, the function finds the minimum ( \(m(x,y)\) ) and maximum ( \(M(x,y)\) ) mhi values over \(3 \times 3\) neighborhood of each pixel and marks the motion orientation at \((x, y)\) as valid only if ","delta2 : ピクセルの近傍における，mhi の値の差の最大値（あるいは最小値）．つまり、この関数は、各ピクセルの近傍領域におけるmhi値の最小値（\(m(x,y)\)）と最大値（\(M(x,y)\)）を求め、次の条件を満たす場合にのみ、その動きの向きを有効とマークします。"
"\[\min ( \texttt{delta1} , \texttt{delta2} ) \le M(x,y)-m(x,y) \le \max ( \texttt{delta1} , \texttt{delta2} ).\]",\MAX ( ˶ˆ꒳ˆ˵ )
apertureSize : Aperture size of the Sobel operator.,apertureSize : Sobel演算子のアパーチャサイズ．
"The function calculates a gradient orientation at each pixel \((x, y)\) as:\[\texttt{orientation} (x,y)= \arctan{\frac{d\texttt{mhi}/dy}{d\texttt{mhi}/dx}}\]In fact, fastAtan2 and phase are used so that the computed angle is measured in degrees and covers the full range 0..360. Also, the mask is filled to indicate pixels where the computed angle is valid.Note","この関数は，各ピクセル ˶((x, y)˶)を用いて，グラデーションの方向性を計算します：˶((x, y)˶)˶ˆ꒳ˆ˵ )(x,y)= \frac{d\texttt{mhi}/dy}{d\texttt{mhi}/dx}}\]実際には，fastAtan2 と phase が利用され，計算される角度は度単位で，0 ～ 360 の範囲をカバーします．また，計算された角度が有効なピクセルを示すために，マスクが塗りつぶされています．"
(Python) An example on how to perform a motion template technique can be found at opencv_source_code/samples/python2/motempl.py,(Python) モーションテンプレート法の実行例は、opencv_source_code/samples/python2/motempl.py にあります。
Calculates a global motion orientation in a selected region.,選択された領域のグローバルモーションオリエンテーションを計算します。
orientation : Motion gradient orientation image calculated by the function calcMotionGradient,orientation : 関数 calcMotionGradient で算出されたモーショングラデーションの方位画像
"mask : Mask image. It may be a conjunction of a valid gradient mask, also calculated by calcMotionGradient , and the mask of a region whose direction needs to be calculated.",mask : マスク画像。同じく calcMotionGradient によって算出された有効なグラデーションマスクと、方向を算出する必要がある領域のマスクを組み合わせたものになります。
mhi : Motion history image calculated by updateMotionHistory .,mhi : updateMotionHistory で算出されたモーションヒストリー画像。
timestamp : Timestamp passed to updateMotionHistory .,timestamp : updateMotionHistory に渡されるタイムスタンプ。
"duration : Maximum duration of a motion track in milliseconds, passed to updateMotionHistory",duration : updateMotionHistory に渡される、ミリ秒単位のモーショントラックの最大継続時間
"The function calculates an average motion direction in the selected region and returns the angle between 0 degrees and 360 degrees. The average direction is computed from the weighted orientation histogram, where a recent motion has a larger weight and the motion occurred in the past has a smaller weight, as recorded in mhi .",この関数は、選択された領域における平均的な動きの方向を計算し、0 度から 360 度の間の角度を返します。この平均方向は，mhi に記録されている，最近の動きの重みが大きく，過去に発生した動きの重みが小さい，重み付けされた方向ヒストグラムから計算されます．
"Splits a motion history image into a few parts corresponding to separate independent motions (for example, left hand, right hand).",モーションヒストリー画像を、独立した別々の動作（例えば、左手、右手）に対応するいくつかの部分に分割します。
mhi : Motion history image.,mhi : モーションヒストリー画像。
"segmask : Image where the found mask should be stored, single-channel, 32-bit floating-point.",segmask : 検出されたマスクが格納される画像，シングルチャンネル，32ビット浮動小数点．
boundingRects : Vector containing ROIs of motion connected components.,boundingRects :モーション連結成分のROIを含むベクトル．
"segThresh : Segmentation threshold that is recommended to be equal to the interval between motion history ""steps"" or greater.","segThresh : モーションヒストリーの ""ステップ ""の間隔以上にすることが推奨されるセグメンテーションの閾値．"
"The function finds all of the motion segments and marks them in segmask with individual values (1,2,...). It also computes a vector with ROIs of motion connected components. After that the motion direction for every component can be calculated with calcGlobalOrientation using the extracted mask of the particular component.","この関数は，すべてのモーションセグメントを検出し，segmask に個別の値 (1,2,...) でマークします．また，モーションに接続されたコンポーネントのROIを含むベクトルを計算します．その後，calcGlobalOrientation を用いて，特定のコンポーネントの抽出されたマスクを用いて，各コンポーネントの動きの方向を計算します．"
"Calculate an optical flow using ""SimpleFlow"" algorithm.","SimpleFlow ""アルゴリズムを用いて、オプティカルフローを計算します。"
from : First 8-bit 3-channel image.,から。最初の8ビット3チャンネル画像．
to : Second 8-bit 3-channel image of the same size as prev,to :prevと同じサイズの2枚目の8ビット3チャンネル画像
flow : computed flow image that has the same size as prev and type CV_32FC2,flow : 「prev」と同じサイズで，タイプが「CV_32FC2」である，計算されたフロー画像．
layers : Number of layers,layers : レイヤーの数．
averaging_block_size : Size of block through which we sum up when calculate cost function for pixel,averaging_block_size : ピクセルに対するコスト関数を計算する際に，積算するブロックのサイズ．
max_flow : maximal flow that we search at each level,max_flow : 各レベルで探索する最大のフロー．
sigma_dist : vector smooth spatial sigma parameter,sigma_dist : ベクトル・スムージングによる空間シグマ・パラメータ
sigma_color : vector smooth color sigma parameter,sigma_color : ベクトル・スムース・カラー・シグマ・パラメータ
postprocess_window : window size for postprocess cross bilateral filter,postprocess_window : 後処理のクロスバイラテラルフィルターのウィンドウサイズ
sigma_dist_fix : spatial sigma for postprocess cross bilateralf filter,sigma_dist_fix : 後処理のクロスバイラテラルフィルターの空間シグマ
sigma_color_fix : color sigma for postprocess cross bilateral filter,sigma_color_fix : 後処理のクロスバイラテラルフィルターのカラーシグマ
occ_thr : threshold for detecting occlusions,occ_thr : オクルージョン検出用閾値
upscale_averaging_radius : window size for bilateral upscale operation,upscale_averaging_radius : バイラテラルアップスケール処理のウィンドウサイズ
upscale_sigma_dist : spatial sigma for bilateral upscale operation,upscale_sigma_dist : バイラテラルアップスケール処理の空間シグマ
upscale_sigma_color : color sigma for bilateral upscale operation,upscale_sigma_color : バイラテラル・アップスケール処理のためのカラー・シグマ
speed_up_thr : threshold to detect point with irregular flow - where flow should be recalculated after upscale,speed_up_thr : 不規則な流れを持つ点を検出するための閾値（アップスケール後に流れを再計算する必要がある場合）．
See [239] . And site of project - http://graphics.berkeley.edu/papers/Tao-SAN-2012-05/.Note,239]を参照してください。そして、プロジェクトのサイト - http://graphics.berkeley.edu/papers/Tao-SAN-2012-05/.Note
An example using the simpleFlow algorithm can be found at samples/simpleflow_demo.cpp,simpleFlowアルゴリズムを用いた例は、samples/simpleflow_demo.cppにあります。
Fast dense optical flow based on PyrLK sparse matches interpolation.,PyrLK スパースマッチ補間に基づく高速高密度オプティカルフロー．
from : first 8-bit 3-channel or 1-channel image.,from : 1枚目の8ビット3チャンネルまたは1チャンネル画像．
to : second 8-bit 3-channel or 1-channel image of the same size as from,to : from と同じサイズの2番目の8ビット3チャンネルまたは1チャンネルの画像．
flow : computed flow image that has the same size as from and CV_32FC2 type,flow : from と同じサイズで，CV_32FC2 型の計算されたフロー画像．
grid_step : stride used in sparse match computation. Lower values usually result in higher quality but slow down the algorithm.,grid_step : 疎なマッチング計算で利用されるストライド．通常，値を小さくすると画質は向上しますが，アルゴリズムが遅くなります．
"k : number of nearest-neighbor matches considered, when fitting a locally affine model. Lower values can make the algorithm noticeably faster at the cost of some quality degradation.",k : 局所的にアフィンなモデルをフィッティングする際に考慮される，最近傍マッチの数．低い値にすると，多少の品質低下を犠牲にして，アルゴリズムが著しく高速になります．
"sigma : parameter defining how fast the weights decrease in the locally-weighted affine fitting. Higher values can help preserve fine details, lower values can help to get rid of the noise in the output flow.",sigma : 局所的に重み付けされたアフィンフィッティングにおいて、重みが減少する速度を定義するパラメータです。値を高くすると細かいディテールを保持するのに役立ち，値を低くすると出力フローのノイズを除去するのに役立ちます．
use_post_proc : defines whether the ximgproc::fastGlobalSmootherFilter() is used for post-processing after interpolation,use_post_proc : ximgproc::fastGlobalSmootherFilter()を補間後の後処理に使うかどうかを定義します。
fgs_lambda : see the respective parameter of the ximgproc::fastGlobalSmootherFilter(),fgs_lambda : ximgproc::fastGlobalSmootherFilter()の各パラメータを参照。
fgs_sigma : see the respective parameter of the ximgproc::fastGlobalSmootherFilter(),fgs_sigma : ximgproc::fastGlobalSmootherFilter()の各パラメータをご参照ください。
"Returns output quality map that was generated during computation, if supported by the algorithm.",アルゴリズムがサポートしていれば、計算中に生成された出力品質マップを返します。
Implements Algorithm::clear(),Algorithm::clear() を実装しています．
"Reimplemented from cv::Algorithm.Reimplemented in cv::quality::QualityPSNR, cv::quality::QualityGMSD, cv::quality::QualitySSIM, and cv::quality::QualityMSE.","cv::Algorithm.Reimplemented from cv::quality::QualityPSNR, cv::quality::QualityGMSD, cv::quality::QualitySSIM, および cv::quality::QualityMSE."
Implements Algorithm::empty(),Algorithm::empty() を実装しています．
Create an object which calculates quality.,品質を計算するオブジェクトを作成します．
ref : input image to use as the source for comparison,ref : 比較の対象となる入力画像．
maxPixelValue : maximum per-channel value for any individual pixel; eg 255 for uint8 image,maxPixelValue : 個々のピクセルに対するチャンネル毎の最大値，例えば uint8 画像ならば 255．
Full reference peak signal to noise ratio (PSNR) algorithm https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio. ,full reference peak signal to noise ratio (PSNR) algorithm https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio.
return the maximum pixel value used for PSNR computation,PSNRの計算に用いる最大ピクセル値を返す
sets the maximum pixel value used for PSNR computation,PSNRの計算に用いる最大ピクセル値を設定します．
val : Maximum pixel value,val : 最大ピクセル値
ref : input image to use as the reference image for comparison,ref : 比較のための参照画像として利用する入力画像
Full reference structural similarity algorithm https://en.wikipedia.org/wiki/Structural_similarity. ,完全参照構造類似性アルゴリズム https://en.wikipedia.org/wiki/Structural_similarity.
Create an object which calculates image quality.,画質を計算するオブジェクトを作成します．
ref : reference image,ref : 基準となる画像
Full reference GMSD algorithm http://www4.comp.polyu.edu.hk/~cslzhang/IQA/GMSD/GMSD.htm. ,完全参照GMSDアルゴリズム http://www4.comp.polyu.edu.hk/~cslzhang/IQA/GMSD/GMSD.htm.
ref : input image to use as the reference for comparison,ref : 比較のための基準となる入力画像
Full reference mean square error algorithm https://en.wikipedia.org/wiki/Mean_squared_error. ,Full reference mean square error algorithm https://en.wikipedia.org/wiki/Mean_squared_error.
"model_file_path : cv::String which contains a path to the BRISQUE model data, eg. /path/to/brisque_model_live.yml",model_file_path : BRISQUEモデルデータへのパスを含む cv::String，例： /path/to/brisque_model_live.yml．
"range_file_path : cv::String which contains a path to the BRISQUE range data, eg. /path/to/brisque_range_live.yml",range_file_path : BRISQUEの範囲データへのパスを含む cv::String．
BRISQUE (Blind/Referenceless Image Spatial Quality Evaluator) is a No Reference Image Quality Assessment (NR-IQA) algorithm. ,BRISQUE (Blind/Referenceless Image Spatial Quality Evaluator) は，NR-IQA (No Reference Image Quality Assessment) アルゴリズムの一つです．
BRISQUE computes a score based on extracting Natural Scene Statistics (https://en.wikipedia.org/wiki/Scene_statistics) and calculating feature vectors. See Mittal et al. [174] for original paper and original implementation [173] .,BRISQUEは、Natural Scene Statistics (https://en.wikipedia.org/wiki/Scene_statistics)を抽出し、特徴ベクトルを計算することでスコアを算出します。オリジナルの論文はMittal et al.[174]、オリジナルの実装は[173]を参照してください。
"A trained model is provided in the /samples/ directory and is trained on the LIVE-R2 database [114] as in the original implementation. When evaluated against the TID2008 database [180] , the SROCC is -0.8424 versus the SROCC of -0.8354 in the original implementation. C++ code for the BRISQUE LIVE-R2 trainer and TID2008 evaluator are also provided in the /samples/ directory. ",学習済みのモデルは，/samples/ ディレクトリに用意されており，オリジナルの実装と同様に LIVE-R2 データベース [114] で学習されています．TID2008データベース[180]に対して評価したところ，オリジナルの実装ではSROCCが-0.8354であったのに対し，SROCCは-0.8424となりました．BRISQUE LIVE-R2トレーナーとTID2008評価器のC++コードも/samples/ディレクトリにあります。
static method for computing image features used by the BRISQUE algorithm,BRISQUEアルゴリズムで使用する画像特徴量の静的な計算方法
img : image (BGR(A) or grayscale) for which to compute features,img : 特徴量を計算するための画像（BGR(A)またはグレースケール）．
features : output row vector of features to cv::Mat or cv::UMat,features : 特徴量の行ベクトルを cv::Mat または cv::UMat に出力します．
"Class for video capturing from video files, image sequences or cameras. ",ビデオファイル，画像シーケンス，カメラからビデオをキャプチャするためのクラス．
The class provides C++ API for capturing video from cameras or for reading video files and image sequences.,このクラスは，カメラからビデオをキャプチャしたり，ビデオファイルや画像シーケンスを読み込んだりするための C++ API を提供します．
"Here is how the class can be used: #include <opencv2/core.hpp>#include <opencv2/videoio.hpp>#include <opencv2/highgui.hpp>#include <iostream>#include <stdio.h>using namespace cv;using namespace std;int main(int, char**){    Mat frame;    //--- INITIALIZE VIDEOCAPTURE    VideoCapture cap;    // open the default camera using default API    // cap.open(0);    // OR advance usage: select any API backend    int deviceID = 0;             // 0 = open default camera    int apiID = cv::CAP_ANY;      // 0 = autodetect default API    // open selected camera using selected API    cap.open(deviceID, apiID);    // check if we succeeded    if (!cap.isOpened()) {        cerr << ""ERROR! Unable to open camera\n"";        return -1;    }    //--- GRAB AND WRITE LOOP    cout << ""Start grabbing"" << endl        << ""Press any key to terminate"" << endl;    for (;;)    {        // wait for a new frame from camera and store it into 'frame'        cap.read(frame);        // check if we succeeded        if (frame.empty()) {            cerr << ""ERROR! blank frame grabbed\n"";            break;        }        // show live and wait for a key with timeout long enough to show images        imshow(""Live"", frame);        if (waitKey(5) >= 0)            break;    }    // the camera will be deinitialized automatically in VideoCapture destructor    return 0;}NoteIn C API the black-box structure CvCapture is used instead of VideoCapture. ","このクラスは，以下のように利用されます．#include <opencv2/core.hpp>#include <opencv2/videoio.hpp>#include <opencv2/highgui.hpp>#include <iostream>#include <stdio.h>using namespace cv;using namespace std;int main(int, char**){ Mat frame; //--- INITIALIZE VIDEOCAPTURE VideoCapture cap; // デフォルトの API を用いて，デフォルトのカメラをオープンします // cap.open(0); // OR 事前使用: 任意の API バックエンドを選択する int deviceID = 0; // 0 = デフォルトカメラを開く int apiID = cv::CAP_ANY; // 0 = デフォルト API を自動検出する // 選択されたカメラを，選択された API を用いて開く cap.open(deviceID, apiID); // 成功したかどうかをチェックする if (!cap.isOpened()) { cerr << ""ERROR!Unable to open camera\n""; return -1; }。    //--- GRAB AND WRITE LOOP cout << ""Start grabbing"" << endl << ""Press any key to terminate"" << endl; for (;;) { // カメラからの新しいフレームを待ち、'frame'に格納 cap.read(frame); // 成功したかどうかをチェック if (frame.empty()) { cerr << ""ERROR! blank frame grabbed\n""; break; }。        imshow(""Live"", frame); if (waitKey(5) >= 0) break; } // ライブを表示し、画像を表示するのに十分なタイムアウト時間でキーを待ちます。    // カメラは，VideoCaptureのデストラクタで自動的に初期化されます return 0;}注：C言語のAPIでは，VideoCaptureの代わりにブラックボックス構造のCvCaptureが利用されます．"
(C++) A basic sample on using the VideoCapture interface can be found at OPENCV_SOURCE_CODE/samples/cpp/videocapture_starter.cpp,(C++）VideoCapture インターフェースを利用した基本的なサンプルは，OPENCV_SOURCE_CODE/samples/cpp/videocapture_starter.cpp にあります．
(Python) A basic sample on using the VideoCapture interface can be found at OPENCV_SOURCE_CODE/samples/python/video.py,(Python) VideoCapture インターフェースを使用した基本的なサンプルは，OPENCV_SOURCE_CODE/samples/python/video.py にあります．
(Python) A multi threaded video processing sample can be found at OPENCV_SOURCE_CODE/samples/python/video_threaded.py,(Python) マルチスレッドビデオ処理のサンプルは、OPENCV_SOURCE_CODE/samples/python/video_threaded.py にあります。
(Python) VideoCapture sample showcasing some features of the Video4Linux2 backend OPENCV_SOURCE_CODE/samples/python/video_v4l2.py ,(Python) Video4Linux2 バックエンドの一部の機能を紹介する VideoCapture サンプル OPENCV_SOURCE_CODE/samples/python/video_v4l2.py
"Examples: samples/cpp/camshiftdemo.cpp, samples/cpp/facedetect.cpp, samples/cpp/laplace.cpp, samples/cpp/lkdemo.cpp, samples/cpp/peopledetect.cpp, samples/cpp/polar_transforms.cpp, samples/cpp/segment_objects.cpp, samples/cpp/train_HOG.cpp, samples/cpp/tutorial_code/videoio/video-write/video-write.cpp, samples/cpp/videowriter_basic.cpp, samples/dnn/classification.cpp, samples/dnn/object_detection.cpp, samples/dnn/segmentation.cpp, samples/dnn/text_detection.cpp, and samples/tapi/hog.cpp.","サンプル： samples/cpp/camshiftdemo.cpp, samples/cpp/facedetect.cpp, samples/cpp/laplace.cpp, samples/cpp/lkdemo.cpp, samples/cpp/peopledetect.cpp, samples/cpp/polar_transforms.cpp, samples/cpp/segment_objects.cpp, samples/cpp/train_HOG.cpp、samples/cpp/tutorial_code/videoio/video-write/video-write.cpp、samples/cpp/videowriter_basic.cpp、samples/dnn/classification.cpp、samples/dnn/object_detection.cpp、samples/dnn/segmentation.cpp、samples/dnn/text_detection.cpp、samples/tapi/hog.cppです。"
Opens a video file or a capturing device or an IP video stream for video capturing.,ビデオファイルやキャプチャーデバイス、ビデオキャプチャー用のIPビデオストリームを開く。
"This is an overloaded member function, provided for convenience. It differs from the above function only in what argument(s) it accepts.Parameters are same as the constructor VideoCapture(const String& filename, int apiPreference = CAP_ANY)The method first calls VideoCapture::release to close the already opened file or camera.Examples: samples/cpp/camshiftdemo.cpp, samples/cpp/facedetect.cpp, samples/cpp/laplace.cpp, samples/cpp/lkdemo.cpp, samples/cpp/peopledetect.cpp, samples/cpp/polar_transforms.cpp, samples/cpp/segment_objects.cpp, samples/cpp/train_HOG.cpp, samples/dnn/classification.cpp, samples/dnn/object_detection.cpp, samples/dnn/segmentation.cpp, samples/dnn/text_detection.cpp, and samples/tapi/hog.cpp.","これはオーバーロードされたメンバー関数で、利便性のために提供されています。パラメータはコンストラクタと同じです。 VideoCapture(const String& filename, int apiPreference = CAP_ANY)このメソッドは、最初にVideoCapture::releaseを呼び出して、すでに開いているファイルやカメラを閉じます。cpp、samples/cpp/laplace.cpp、samples/cpp/lkdemo.cpp、samples/cpp/peopledetect.cpp、samples/cpp/polar_transforms.cpp、samples/cpp/segment_objects.cpp、samples/cpp/train_HOG.cpp、samples/dnn/classification.cpp、samples/dnn/object_detection.cpp、samples/dnn/segmentation.cpp、samples/dnn/text_detection.cpp、およびsamples/tapi/hog.cpp。"
Returns true if video capturing has been initialized already.,ビデオキャプチャーがすでに初期化されている場合は、trueを返します。
"If the previous call to VideoCapture constructor or VideoCapture::open() succeeded, the method returns true.Examples: samples/cpp/camshiftdemo.cpp, samples/cpp/facedetect.cpp, samples/cpp/laplace.cpp, samples/cpp/lkdemo.cpp, samples/cpp/peopledetect.cpp, samples/cpp/polar_transforms.cpp, samples/cpp/segment_objects.cpp, samples/cpp/train_HOG.cpp, samples/cpp/tutorial_code/videoio/video-write/video-write.cpp, samples/cpp/videowriter_basic.cpp, and samples/tapi/hog.cpp.",例: samples/cpp/camshiftdemo.cpp、samples/cpp/facedetect.cpp、samples/cpp/laplace.cpp、samples/cpp/lkdemo.cpp、samples/cpp/peopledetect.cpp、samples/cpp/polar_transforms.cpp、samples/cpp/segment_objects.cpp、samples/cpp/train_HOG.cpp、samples/cpp/tutorial_code/videoio/video-writ/video-writ.cpp、samples/cpp/videowriter_basic.cpp、samples/tapi/hog.cpp。
Closes video file or capturing device.,ビデオファイルまたはキャプチャーデバイスを閉じます。
The method is automatically called by subsequent VideoCapture::open and by VideoCapture destructor.The C function also deallocates memory and clears *capture pointer.,このメソッドは、後続の VideoCapture::open や VideoCapture destructor から自動的に呼び出されます。また、この C 関数は、メモリを解放し、*capture ポインタをクリアします。
Grabs the next frame from video file or capturing device.,ビデオファイルまたはキャプチャーデバイスから次のフレームを取得します。
"The method/function grabs the next frame from video file or camera and returns true (non-zero) in the case of success.The primary use of the function is in multi-camera environments, especially when the cameras do not have hardware synchronization. That is, you call VideoCapture::grab() for each camera and after that call the slower method VideoCapture::retrieve() to decode and get frame from each camera. This way the overhead on demosaicing or motion jpeg decompression etc. is eliminated and the retrieved frames from different cameras will be closer in time.Also, when a connected camera is multi-head (for example, a stereo camera or a Kinect device), the correct way of retrieving data from it is to call VideoCapture::grab() first and then call VideoCapture::retrieve() one or more times with different values of the channel parameter.Using Kinect and other OpenNI compatible depth sensors",このメソッド／関数は，ビデオファイルまたはカメラから次のフレームを取得し，成功した場合は true（0 以外）を返します。この関数の主な用途は，マルチカメラ環境で，特にカメラがハードウェア同期を行っていない場合です。つまり，各カメラに対して VideoCapture::grab() を呼び出し，その後，より低速なメソッドである VideoCapture::retrieve() を呼び出して，各カメラからフレームをデコードして取得するのです．また、接続されているカメラがマルチヘッドの場合（ステレオカメラやKinectデバイスなど）は、最初にVideoCapture::grab()を呼び出し、その後VideoCapture::retrieve()をチャンネルパラメータの値を変えて複数回呼び出すのが正しいデータ取得方法です。
Decodes and returns the grabbed video frame.,掴んだビデオフレームをデコードして返します。
image : [out],image : [out] です。
flag : it could be a frame index or a driver specific flag,flag : フレームインデックスやドライバ固有のフラグなど
"The method decodes and returns the just grabbed frame. If no frames has been grabbed (camera has been disconnected, or there are no more frames in video file), the method returns false and the function returns an empty image (with cv::Mat, test it with Mat::empty()).See alsoread()NoteIn C API, functions cvRetrieveFrame() and cv.RetrieveFrame() return image stored inside the video capturing structure. It is not allowed to modify or release the image! You can copy the frame using cvCloneImage and then do whatever you want with the copy.",このメソッドはグラブされたばかりのフレームをデコードして返します。フレームが取得されなかった場合（カメラが切断された，あるいは，ビデオファイルにフレームが存在しない），このメソッドは false を返し，この関数は空の画像を返します（ cv::Mat の場合は， Mat::empty() でテストしてください）．この画像を変更したり，解放したりすることはできません．cvCloneImage を使ってフレームをコピーし，そのコピーに対して好きなことを行うことができます．
"Grabs, decodes and returns the next video frame.",次のビデオフレームを取得，デコードして返します．
"The method/function combines VideoCapture::grab() and VideoCapture::retrieve() in one call. This is the most convenient method for reading video files or capturing data from decode and returns the just grabbed frame. If no frames has been grabbed (camera has been disconnected, or there are no more frames in video file), the method returns false and the function returns empty image (with cv::Mat, test it with Mat::empty()).NoteIn C API, functions cvRetrieveFrame() and cv.RetrieveFrame() return image stored inside the video capturing structure. It is not allowed to modify or release the image! You can copy the frame using cvCloneImage and then do whatever you want with the copy.Examples: samples/cpp/videowriter_basic.cpp.",このメソッド/関数は， VideoCapture::grab() と VideoCapture::retrieve() を1つの呼び出しにまとめたものです．これは、ビデオファイルを読み込んだり、デコードからデータをキャプチャしたりする際に最も便利な方法であり、グラブされたばかりのフレームを返します。フレームが取得されなかった場合（カメラが切断された，あるいは，ビデオファイルにフレームが存在しない），このメソッドは false を返し，この関数は空の画像を返します（ cv::Mat の場合は， Mat::empty() でテストしてください）．この画像を変更したり，解放したりすることはできません．cvCloneImage を使ってフレームをコピーし，そのコピーに対して好きなことを行うことができます．例： samples/cpp/videowriter_basic.cpp.
Sets a property in the VideoCapture.,VideoCapture のプロパティを設定します．
"propId : Property identifier from cv::VideoCaptureProperties (eg. cv::CAP_PROP_POS_MSEC, cv::CAP_PROP_POS_FRAMES, ...) or one from Additional flags for video I/O API backends","propId : cv::VideoCaptureProperties のプロパティ識別子（例： cv::CAP_PROP_POS_MSEC, cv::CAP_PROP_POS_FRAMES, ...），あるいは，ビデオ入出力 API バックエンドのための追加フラグのうちの1つ．"
value : Value of the property.,value : プロパティの値．
NoteEven if it returns true this doesn't ensure that the property value has been accepted by the capture device. See note in VideoCapture::get()Examples: samples/cpp/laplace.cpp.,注意： true を返したとしても，そのプロパティの値がキャプチャデバイスに受け入れられたかどうかは保証されません．VideoCapture::get()の注記を参照してください。 サンプル: samples/cpp/laplace.cpp.
Returns the specified VideoCapture property.,指定した VideoCapture プロパティを返します。
"NoteReading / writing properties involves many layers. Some unexpected result might happens along this chain. VideoCapture -> API Backend -> Operating System -> Device Driver -> Device Hardware The returned value might be different from what really used by the device or it could be encoded using device dependent rules (eg. steps or percentage). Effective behaviour depends from device driver and API BackendExamples: samples/cpp/laplace.cpp, and samples/cpp/tutorial_code/videoio/video-write/video-write.cpp.",注意：プロパティの読み書きには多くのレイヤーが関わっています。予期せぬ結果が発生する可能性がありますので、ご注意ください。VideoCapture -> API Backend -> Operating System -> Device Driver -> Device Hardware 返された値は、デバイスで実際に使用されているものとは異なる可能性があり、また、デバイス依存のルール（例：ステップまたはパーセンテージ）を使用してエンコードされている可能性があります。有効な動作は、デバイスドライバーとAPIバックエンドに依存します。サンプル：samples/cpp/laplace.cpp、samples/cpp/tutorial_code/videoio/video-write/video-writ.cpp。
Returns used backend API name.,使用したバックエンドのAPI名を返します。
NoteStream should be opened.,NoteStreamをオープンします。
Switches exceptions mode,例外モードを切り替えます。
methods raise exceptions if not successful instead of returning an error code,メソッドは、成功しなかった場合、エラーコードを返す代わりに例外を発生させます。
query if exception mode is active,例外モードがアクティブかどうかを問い合わせる
Wait for ready frames from VideoCapture.,VideoCaptureからの準備完了フレームを待ちます。
streams : input video streams,streams : 入力ビデオストリーム
readyIndex : stream indexes with grabbed frames (ready to use .retrieve() to fetch actual frame),readyIndex : 取得したフレームを持つストリームのインデックス（.retrieve()を使用して実際のフレームを取得する準備ができている）。
timeoutNs : number of nanoseconds (0 - infinite),timeoutNs : ナノ秒数（0～無限大）
Exceptions,例外処理
  ,  
"    ExceptionException on stream errors (check .isOpened() to filter out malformed streams) or VideoCapture type is not supportedThe primary use of the function is in multi-camera environments. The method fills the ready state vector, grabs video frame, if camera is ready.After this call use VideoCapture::retrieve() to decode and fetch frame data.",    ストリームエラー（.isOpened()をチェックして、不正なストリームを除外する）またはVideoCaptureタイプがサポートされていない場合は、Exceptionが発生しますこの関数の主な用途は、マルチカメラ環境です。このメソッドは、カメラの準備ができていれば、レディステートベクトルを満たし、ビデオフレームを取得します。この呼び出しの後、VideoCapture::retrieve()を使用して、フレームデータをデコードして取得します。
Video writer class. ,ビデオライタークラス。
The class provides C++ API for writing video files or image sequences. ,このクラスは，ビデオファイルやイメージシーケンスを書き込むための C++ API を提供します。
"Examples: samples/cpp/tutorial_code/videoio/video-write/video-write.cpp, samples/cpp/videowriter_basic.cpp, and samples/tapi/hog.cpp.",sample/cpp/tutorial_code/videoio/video-write/video-write.cpp、samples/cpp/videowriter_basic.cpp、samples/tapi/hog.cppがその例です。
Initializes or reinitializes video writer.,ビデオライターを初期化または再初期化します。
"The method opens video writer. Parameters are the same as in the constructor VideoWriter::VideoWriter.The method first calls VideoWriter::release to close the already opened file.Examples: samples/cpp/tutorial_code/videoio/video-write/video-write.cpp, samples/cpp/videowriter_basic.cpp, and samples/tapi/hog.cpp.","このメソッドは、ビデオライターを開きます。パラメータは、コンストラクタ VideoWriter::VideoWriter と同じです。このメソッドは、最初に VideoWriter::release を呼び出して、すでに開かれているファイルを閉じます。サンプル: samples/cpp/tutorial_code/videoio/video-writ/video-writ.cpp, samples/cpp/videowriter_basic.cpp, samples/tapi/hog.cpp."
Returns true if video writer has been successfully initialized.,ビデオライターの初期化に成功した場合は、trueを返します。
"Examples: samples/cpp/videowriter_basic.cpp, and samples/tapi/hog.cpp.",例：samples/cpp/videowriter_basic.cpp、samples/tapi/hog.cpp。
Closes the video writer.,ビデオライターを閉じます。
The method is automatically called by subsequent VideoWriter::open and by the VideoWriter destructor.,このメソッドは、後続の VideoWriter::open や VideoWriter のデストラクタから自動的に呼び出されます。
Writes the next video frame.,次のビデオフレームを書き込みます。
"image : The written frame. In general, color images are expected in BGR format.",image : 書き込んだフレームです。一般的には BGR 形式のカラー画像を想定しています。
The function/method writes the specified image to video file. It must have the same size as has been specified when opening the video writer.Examples: samples/cpp/videowriter_basic.cpp.,関数/メソッドは指定された画像をビデオファイルに書き込みます。ビデオファイルのサイズは、ビデオライターを開くときに指定したサイズと同じである必要があります。
Sets a property in the VideoWriter.,VideoWriter のプロパティを設定します。
propId : Property identifier from cv::VideoWriterProperties (eg. cv::VIDEOWRITER_PROP_QUALITY) or one of Additional flags for video I/O API backends,propId : cv::VideoWriterProperties のプロパティ識別子（例： cv::VIDEOWRITER_PROP_QUALITY），またはビデオ入出力 API バックエンドの追加フラグの一つ．
Returns the specified VideoWriter property.,指定された VideoWriter のプロパティを返します．
Concatenates 4 chars to a fourcc code.,4文字をfourccコードに連結します。
This static method constructs the fourcc code of the codec to be used in the constructor VideoWriter::VideoWriter or VideoWriter::open.Examples: samples/cpp/videowriter_basic.cpp.,VideoWriter::VideoWriter または VideoWriter::open のコンストラクタで使用するコーデックの fourcc コードを構築します。例: samples/cpp/videowriter_basic.cpp.
Class for computing BRIEF descriptors described in [40] . ,40]に記載されている BRIEF 記述子を計算するクラスです。
Parameters,パラメータ
"    byteslegth of the descriptor in bytes, valid values are: 16, 32 (default) or 64 . ",    bytes ディスクリプタの長さをバイト単位で指定します，有効な値は有効な値は，16，32（デフォルト），64です．
"    use_orientationsample patterns using keypoints orientation, disabled by default. ",    use_orientations sample pattern using keypoints orientation，デフォルトでは無効です．
orientationNormalized : Enable orientation normalization.,orientationNormalized : 向きの正規化を有効にします．
scaleNormalized : Enable scale normalization.,scaleNormalized : スケールの正規化を有効にします。
patternScale : Scaling of the description pattern.,patternScale : 記述パターンのスケーリング。
nOctaves : Number of octaves covered by the detected keypoints.,nOctaves : 検出されたキーポイントでカバーされるオクターブの数
"selectedPairs : (Optional) user defined selected pairs indexes,",selectedPairs : (Optional) ユーザ定義の選択ペアインデックス．
"Class implementing the FREAK (Fast Retina Keypoint) keypoint descriptor, described in [8] . ",FREAK (Fast Retina Keypoint) キーポイントディスクリプタを実装したクラスで，[8]で述べられています．
"The algorithm propose a novel keypoint descriptor inspired by the human visual system and more precisely the retina, coined Fast Retina Key- point (FREAK). A cascade of binary strings is computed by efficiently comparing image intensities over a retinal sampling pattern. FREAKs are in general faster to compute with lower memory load and also more robust than SIFT, SURF or BRISK. They are competitive alternatives to existing keypoints in particular for embedded applications.",このアルゴリズムは，人間の視覚システム，より正確には網膜にインスパイアされた新しいキーポイント記述子を提案しており，Fast Retina Key-point (FREAK)と呼ばれています．FREAKは、網膜のサンプリングパターン上で画像の強度を効率的に比較することで、2値文字列のカスケードを計算します。FREAKは、一般的に、SIFT、SURF、BRISKよりも、メモリ負荷が低く、計算が速く、また、ロバスト性に優れています。また、SIFTやSURF、BRISKに比べてロバスト性が高く、既存のキーポイントに代わる競争力のあるキーポイントとして、特に組込み用途に有効です。
Note,備考
An example on how to use the FREAK descriptor can be found at opencv_source_code/samples/cpp/freak_demo.cpp ,FREAK 記述子の使用方法の例は、opencv_source_code/samples/cpp/freak_demo.cpp にあります。
the full constructor,フルコンストラクタ
"The class implements the keypoint detector introduced by [2], synonym of StarDetector. : ",このクラスは，[2]で紹介されたキーポイント検出器（StarDetectorの同義語）を実装しています．
"lucid_kernel : kernel for descriptor construction, where 1=3x3, 2=5x5, 3=7x7 and so forth","lucid_kernel : ディスクリプタ構築用カーネル（1=3x3, 2=5x5, 3=7x7 など"
"blur_kernel : kernel for blurring image prior to descriptor construction, where 1=3x3, 2=5x5, 3=7x7 and so forth","blur_kernel : 記述子構築の前に画像をぼかすためのカーネルで，1=3x3, 2=5x5, 3=7x7 など"
"Class implementing the locally uniform comparison image descriptor, described in [295]. ",295]で述べられている，局所的に一様な比較画像記述子を実装したクラス．
"An image descriptor that can be computed very fast, while being about as robust as, for example, SURF or BRIEF.",SURFやBRIEFなどと同程度のロバスト性を持ちながら，非常に高速に計算できる画像記述子です．
NoteIt requires a color image as input. ,注：入力としてカラー画像が必要です。
"latch Class for computing the LATCH descriptor. If you find this code useful, please add a reference to the following paper in your work: Gil Levi and Tal Hassner, ""LATCH: Learned Arrangements of Three Patch Codes"", arXiv preprint arXiv:1501.03719, 15 Jan. 2015","latch LATCH ディスクリプタを計算するクラス．このコードが役に立ったら，あなたの作品に次の論文への参照を加えてください。Gil Levi and Tal Hassner, ""LATCH:Learned Arrangements of Three Patch Codes"", arXiv preprint arXiv:1501.03719, 15 Jan.2015"
LATCH is a binary descriptor based on learned comparisons of triplets of image patches.,LATCHは、画像パッチのトリプレットの学習された比較に基づく、バイナリ記述子です。
"bytes is the size of the descriptor - can be 64, 32, 16, 8, 4, 2 or 1 rotationInvariance - whether or not the descriptor should compansate for orientation changes. half_ssd_size - the size of half of the mini-patches size. For example, if we would like to compare triplets of patches of size 7x7x then the half_ssd_size should be (7-1)/2 = 3. sigma - sigma value for GaussianBlur smoothing of the source image. Source image will be used without smoothing in case sigma value is 0.",bytesは記述子のサイズで，64，32，16，8，4，2，1のいずれかです． rotationInvariance - 記述子が方向の変化に対応するかどうかを指定します． half_ssd_size - ミニパッチのサイズの半分のサイズです．例えば，サイズが7x7xのパッチのトリプレットを比較したい場合，half_ssd_sizeは(7-1)/2 = 3となります． sigma - ソース画像をガウスブラーで平滑化するためのシグマ値です．sigma値が0の場合，ソース画像はスムージングされずに利用されます．
Note: the descriptor can be coupled with any keypoint extractor. The only demand is that if you use set rotationInvariance = True then you will have to use an extractor which estimates the patch orientation (in degrees). Examples for such extractors are ORB and SIFT.,注意：このディスクリプタは，任意のキーポイント抽出器と組み合わせることができます．唯一の要求は，set rotationInvariance = True を使った場合には，パッチの向き（度数）を推定する抽出器を使わなければならないということです．このような抽出器の例としては、ORBやSIFTがあります。
Note: a complete example can be found under /samples/cpp/tutorial_code/xfeatures2D/latch_match.cpp ,注：完全な例は、/samples/cpp/tutorial_code/xfeatures2D/latch_match.cppにあります。
hessianThreshold : Threshold for hessian keypoint detector used in SURF.,hessianThreshold : SURFで使用される hessian keypoint 検出器の閾値。
nOctaves : Number of pyramid octaves the keypoint detector will use.,nOctaves : キーポイント検出器が使用するピラミッドオクターブの数。
nOctaveLayers : Number of octave layers within each octave.,nOctaveLayers : 各オクターブ内のオクターブレイヤーの数。
extended : Extended descriptor flag (true - use extended 128-element descriptors; false - use 64-element descriptors).,extended : 拡張ディスクリプタフラグ（true - 128要素の拡張ディスクリプタを利用，false - 64要素のディスクリプタを利用）．
upright : Up-right or rotated features flag (true - do not compute orientation of features; false - compute orientation).,upright : 右上がり，または回転した特徴量フラグ（true - 特徴量の向きを計算しない，false - 向きを計算する）．
Class for extracting Speeded Up Robust Features from an image [16] . ,画像からSpeeded Up Robust Featuresを抽出するクラス [16] ．
The algorithm parameters:,アルゴリズムのパラメータです．
member int extended,メンバー int extended
0 means that the basic descriptors (64 elements each) shall be computed,0 は，基本記述子（各64要素）を計算することを意味します．
1 means that the extended descriptors (128 elements each) shall be computed,1 は，拡張ディスクリプタ（各128要素）を計算することを意味します．
member int upright,メンバー int upright
0 means that detector computes orientation of each feature.,0 は，検出器が各特徴の向きを計算することを意味します．
"1 means that the orientation is not computed (which is much, much faster). For example, if you match images from a stereo pair, or do image stitching, the matched features likely have very similar angles, and you can speed up feature extraction by setting upright=1.",1 は，向きの計算を行わないことを意味します（この方が，はるかに高速です）．例えば，ステレオペアの画像をマッチングさせたり，画像のスティッチングを行ったりする場合，マッチングした特徴の角度が非常に似ている可能性があり，upright=1を設定することで，特徴の抽出を高速化することができます．
"member double hessianThreshold Threshold for the keypoint detector. Only features, whose hessian is larger than hessianThreshold are retained by the detector. Therefore, the larger the value, the less keypoints you will get. A good default value could be from 300 to 500, depending from the image contrast.",member double hessianThreshold キーポイント検出器のしきい値。hessianThresholdよりも大きいヘシアンを持つ特徴のみが、検出器によって保持されます。したがって，この値が大きくなるほど，得られるキーポイントは少なくなります．初期値としては，画像のコントラストにもよりますが，300から500程度が良いでしょう。
"member int nOctaves The number of a gaussian pyramid octaves that the detector uses. It is set to 4 by default. If you want to get very large features, use the larger value. If you want just small features, decrease it.",member int nOctaves 検出器が利用するガウシアンピラミッドのオクターブ数を指定します。デフォルトでは，4に設定されています．非常に大きな特徴を得たい場合は，より大きな値を使用してください．小さな特徴を得るためには、この値を小さくしてください。
member int nOctaveLayers The number of images within each octave of a gaussian pyramid. It is set to 2 by default. Note,member int nOctaveLayers ガウシアンピラミッドの各オクターブ内の画像の数。デフォルトでは、2に設定されています。参考
An example using the SURF feature detector can be found at opencv_source_code/samples/cpp/generic_descriptor_match.cpp,SURF 特徴検出器を用いた例は， opencv_source_code/samples/cpp/generic_descriptor_match.cpp にあります．
"Another example using the SURF feature detector, extractor and matcher can be found at opencv_source_code/samples/cpp/matcher_simple.cpp ",SURF 特徴検出器，抽出器，マッチャーを用いた別の例は， opencv_source_code/samples/cpp/matcher_simple.cpp にあります．
Performs image denoising using the Block-Matching and 3D-filtering algorithm http://www.cs.tut.fi/~foi/GCF-BM3D/BM3D_TIP_2007.pdf with several computational optimizations. Noise expected to be a gaussian white noise.,Block-Matching and 3D-filtering algorithm http://www.cs.tut.fi/~foi/GCF-BM3D/BM3D_TIP_2007.pdf を用いて，いくつかの計算上の最適化を行いながら，画像のノイズ除去を行います．ノイズは，ガウスホワイトノイズであることが期待されます．
src : Input 8-bit or 16-bit 1-channel image.,src : 8ビットまたは16ビット，1チャンネルの入力画像．
dstStep1 : Output image of the first step of BM3D with the same size and type as src.,dstStep1 : src と同じサイズ・タイプの，BM3D のファーストステップの出力画像．
dstStep2 : Output image of the second step of BM3D with the same size and type as src.,dstStep2 : BM3D の第2段階で出力される画像．
"h : Parameter regulating filter strength. Big h value perfectly removes noise but also removes image details, smaller h value preserves details but also preserves some noise.",h : フィルタの強さを調整するパラメータ．h が大きいと，ノイズは完全に除去されますが，画像の細部も除去されます．h が小さいと，細部は保持されますが，ノイズも若干保持されます．
templateWindowSize : Size in pixels of the template patch that is used for block-matching. Should be power of 2.,templateWindowSize : ブロックマッチングに利用されるテンプレートパッチのサイズ（ピクセル）．2の累乗が必要。
searchWindowSize : Size in pixels of the window that is used to perform block-matching. Affect performance linearly: greater searchWindowsSize - greater denoising time. Must be larger than templateWindowSize.,searchWindowSize : ブロックマッチングを行う際に使用されるウィンドウのピクセル単位のサイズ．searchWindowSize が大きいほど，ノイズ除去にかかる時間が長くなります．templateWindowSizeよりも大きくなければならない。
"blockMatchingStep1 : Block matching threshold for the first step of BM3D (hard thresholding), i.e. maximum distance for which two blocks are considered similar. Value expressed in euclidean distance.",blockMatchingStep1 : BM3Dの最初のステップ（ハードスレッショルド）におけるブロックマッチングのしきい値，つまり，2つのブロックが類似しているとみなされる最大距離．ユークリッド距離で表される。
"blockMatchingStep2 : Block matching threshold for the second step of BM3D (Wiener filtering), i.e. maximum distance for which two blocks are considered similar. Value expressed in euclidean distance.",blockMatchingStep2 : BM3Dの第2ステップ（ウィーナー・フィルタリング）のためのブロック・マッチング閾値、つまり2つのブロックが類似しているとみなされる最大の距離。ユークリッド距離で表されます。
groupSize : Maximum size of the 3D group for collaborative filtering.,groupSize : 協調フィルタリングのための3Dグループの最大サイズ。
slidingStep : Sliding step to process every next reference block.,slidingStep : 次の参照ブロックを処理するためのスライディングステップ。
"beta : Kaiser window parameter that affects the sidelobe attenuation of the transform of the window. Kaiser window is used in order to reduce border effects. To prevent usage of the window, set beta to zero.",beta :カイザー窓のパラメータで，窓の変換によるサイドローブの減衰に影響する．カイザー窓は，ボーダー効果を軽減するために使用されます。カイザー窓を使わないようにするには、βを0に設定します。
normType : Norm used to calculate distance between blocks. L2 is slower than L1 but yields more accurate results.,normType :ブロック間の距離を計算するのに使われるノルム。L2はL1より遅いですが、より正確な結果が得られます。
"step : Step of BM3D to be executed. Possible variants are: step 1, step 2, both steps.",step : 実行されるBM3Dのステップ。step1、step2、both stepのいずれかを指定します。
transformType : Type of the orthogonal transform used in collaborative filtering step. Currently only Haar transform is supported.,transformType :協調フィルタリングステップで使われる直交変換のタイプ．現在は，Haar変換のみがサポートされています．
This function expected to be applied to grayscale images. Advanced usage of this function can be manual denoising of colored image in different colorspaces.See alsofastNlMeansDenoising,この関数は，グレースケール画像に適用されることを想定しています．この関数の高度な利用法として，異なる色空間を持つカラー画像を手動でノイズ除去することが挙げられます．
dst : Output image with the same size and type as src.,dst : src と同じサイズ，同じタイプの出力画像．
step : Step of BM3D to be executed. Allowed are only BM3D_STEP1 and BM3D_STEPALL. BM3D_STEP2 is not allowed as it requires basic estimate to be present.,step : 実行されるBM3Dのステップ。BM3D_STEP1 と BM3D_STEPALL のみ許可されます。BM3D_STEP2は、基本的な推定値が存在する必要があるため、許可されません。
The function implements simple dct-based denoising.,この関数は，シンプルな dct ベースのノイズ除去を行います．
src : source image,src : ソース画像
dst : destination image,dst : 出力画像
sigma : expected noise standard deviation,sigma : 期待されるノイズの標準偏差
psize : size of block side where dct is computed,psize : dct が計算されるブロックサイドのサイズ
http://www.ipol.im/pub/art/2011/ys-dct/.See alsofastNlMeansDenoising,http://www.ipol.im/pub/art/2011/ys-dct/.See また，fastNlMeansDenoising
The function implements different single-image inpainting algorithms.,この関数は，様々な単一画像のインペインティングアルゴリズムを実装しています．
"INPAINT_SHIFTMAP: it could be of any type and any number of channels from 1 to 4. In case of 3- and 4-channels images the function expect them in CIELab colorspace or similar one, where first color component shows intensity, while second and third shows colors. Nonetheless you can try any colorspaces.",INPAINT_SHIFTMAP: これは，1から4までの任意の種類，任意のチャンネル数のものです．3チャンネルや4チャンネルの画像の場合，この関数はCIELab色空間，あるいはそれに似た色空間を想定しており，1番目の色成分は強度を，2番目と3番目は色を表します．しかし，どんな色空間でも試してみることができます．
INPAINT_FSR_BEST or INPAINT_FSR_FAST: 1-channel grayscale or 3-channel BGR image.,INPAINT_FSR_BEST や INPAINT_FSR_FAST:1チャンネルのグレースケールまたは3チャンネルのBGR画像．
"mask : mask (CV_8UC1), where non-zero pixels indicate valid image area, while zero pixels indicate area to be inpainted",mask : マスク（CV_8UC1）．0以外のピクセルは有効な画像領域を示し，0のピクセルはペイントされる領域を示します．
algorithmType : see xphoto::InpaintTypes,algorithmType : xphoto::InpaintTypes を参照してください．
See the original papers [105] (Shiftmap) or [90] and [214] (FSR) for details.,"詳細は原著論文[105](Shiftmap)または[90],[214](FSR)を参照してください。"
oilPainting See the book [39] for details.,oilPainting 詳細は原著論文[39]を参照．
src : Input three-channel or one channel image (either CV_8UC3 or CV_8UC1),src : 3チャンネルまたは1チャンネルの画像を入力（CV_8UC3 または CV_8UC1 のどちらか）．
dst : Output image of the same size and type as src.,dst : src と同じサイズ，同じ種類の画像を出力．
size : neighbouring size is 2-size+1,size : 隣接するサイズは，2-size+1となります．
dynRatio : image is divided by dynRatio before histogram processing,dynRatio : ヒストグラム処理の前に，画像を dynRatio で分割します．
code : color space conversion code(see ColorConversionCodes). Histogram will used only first plane,code : 色空間変換コード（ColorConversionCodesを参照）．ヒストグラムは第1面のみ使用されます。
Creates TonemapDurand object.,TonemapDurandオブジェクトを作成します。
gamma : gamma value for gamma correction. See createTonemap,gamma : ガンマ補正のためのガンマ値。createTonemap参照。
"contrast : resulting contrast on logarithmic scale, i. e. log(max / min), where max and min are maximum and minimum luminance values of the resulting image.",ここでmaxとminは、結果として得られる画像の輝度の最大値と最小値です。
saturation : saturation enhancement value. See createTonemapDrago,saturation : 彩度拡張値。createTonemapDrago参照
sigma_color : bilateral filter sigma in color space,sigma_color : 色空間におけるバイラテラルフィルタシグマ
sigma_space : bilateral filter sigma in coordinate space,sigma_space : 座標空間でのバイラテラルフィルターシグマ
You need to set the OPENCV_ENABLE_NONFREE option in cmake to use those. Use them at your own risk.,これらを使うには、cmakeでOPENCV_ENABLE_NONFREEオプションを設定する必要があります。これらを使用する場合は，自己責任でお願いします．
This algorithm decomposes image into two layers: base layer and detail layer using bilateral filter and compresses contrast of the base layer thus preserving all the details. ,このアルゴリズムは，バイラテラルフィルタを用いて画像をベース層と詳細層の2つの層に分解し，ベース層のコントラストを圧縮することで，すべての詳細部分を保存します．
This implementation uses regular bilateral filter from OpenCV.,この実装では，OpenCV の通常のバイラテラルフィルタを利用します．
Saturation enhancement is possible as in cv::TonemapDrago.,cv::TonemapDrago のように，彩度を強調することができます．
For more information see [63] . ,詳細は [63] を参照してください．
"Implements an efficient fixed-point approximation for applying channel gains, which is the last step of multiple white balance algorithms.",複数のホワイトバランスアルゴリズムの最後のステップである，チャンネルゲインを適用するための効率的な固定小数点近似法を実装しています．
src : Input three-channel image in the BGR color space (either CV_8UC3 or CV_16UC3),src : BGR 色空間における3チャンネルの入力画像（CV_8UC3 または CV_16UC3 のいずれか）．
gainB : gain for the B channel,gainB : Bチャンネル用のゲイン
gainG : gain for the G channel,gainG : Gチャンネルのゲイン
gainR : gain for the R channel,gainR : Rチャンネルのゲイン
Creates an instance of GrayworldWB.,GrayworldWB のインスタンスを生成します．
Creates an instance of LearningBasedWB.,LearningBasedWBのインスタンスを作成します．
"path_to_model : Path to a .yml file with the model. If not specified, the default model is used",path_to_model : モデルを記述した.ymlファイルへのパス。指定しない場合は、デフォルトのモデルが使用されます
More sophisticated learning-based automatic white balance algorithm. ,より洗練された学習ベースの自動ホワイトバランスアルゴリズムです。
"As GrayworldWB, this algorithm works by applying different gains to the input image channels, but their computation is a bit more involved compared to the simple gray-world assumption. More details about the algorithm can be found in [46] .",GrayworldWBと同様に、このアルゴリズムは、入力画像のチャンネルに異なるゲインを適用することで動作しますが、その計算は、単純なグレイワールドの仮定に比べて、少し複雑です。このアルゴリズムの詳細については，[46]を参照してください。
To mask out saturated pixels this function uses only pixels that satisfy the following condition:,この関数は，飽和したピクセルをマスクアウトするために，以下の条件を満たすピクセルのみを利用します．
"\[ \frac{\textrm{max}(R,G,B)}{\texttt{range_max_val}} < \texttt{saturation_thresh} \]","\[ \frac{\textrm{max}(R,G,B)}{\texttt{range_max_val}}< ˶ˆ꒳ˆ˵ )\]"
Currently supports images of type CV_8UC3 and CV_16UC3. ,現在，CV_8UC3 と CV_16UC3 のタイプの画像をサポートしています．
Implements the feature extraction part of the algorithm.,アルゴリズムの特徴抽出部分を実装します．
src : Input three-channel image (BGR color space is assumed).,src : 入力3チャンネル画像（BGR 色空間を仮定します）．
"dst : An array of four (r,g) chromaticity tuples corresponding to the features listed above.","dst : 上述の特徴量に対応する4つの(r,g) 色度タプルの配列．"
"In accordance with [46] , computes the following features for the input image:Chromaticity of an average (R,G,B) tuple","46] に従って，入力画像に対して以下の特徴量を計算します：平均的な(R,G,B)タプルの色度"
"Chromaticity of the brightest (R,G,B) tuple (while ignoring saturated pixels)","最も明るい(R,G,B)タプルの色度（飽和した画素は無視する）"
"Chromaticity of the dominant (R,G,B) tuple (the one that has the highest value in the RGB histogram)","支配的な(R,G,B)タプルの色度（RGBヒストグラムで最も高い値を持つタプルの色度）"
"Mode of the chromaticity palette, that is constructed by taking 300 most common colors according to the RGB histogram and projecting them on the chromaticity plane. Mode is the most high-density point of the palette, which is computed by a straightforward fixed-bandwidth kernel density estimator with a Epanechnikov kernel function.",色度パレットのモード（RGBヒストグラムで最も一般的な300色を色度平面に投影して作成されます。モードは，パレットの中で最も密度の高い点であり，これは，エパニチオフカーネル関数を用いた単純な固定帯域幅のカーネル密度推定法によって求められます．
Defines the size of one dimension of a three-dimensional RGB histogram that is used internally by the algorithm. It often makes sense to increase the number of bins for images with higher bit depth (e.g. 256 bins for a 12 bit image).,アルゴリズムが内部的に使用する3次元RGBヒストグラムの1次元のサイズを定義します。ビット深度の高い画像では，ビンの数を増やすことがしばしば意味を持ちます（例えば，12ビットの画像では256ビン）．
See alsogetHistBinNum,関連項目： logetHistBinNum
"Maximum possible value of the input image (e.g. 255 for 8 bit images, 4095 for 12 bit images)",入力画像の最大可能値（例：8ビット画像なら255、12ビット画像なら4095）。
See alsogetRangeMaxVal,参照：「alsogetRangeMaxVal
"Threshold that is used to determine saturated pixels, i.e. pixels where at least one of the channels exceeds \(\texttt{saturation_threshold}\times\texttt{range_max_val}\) are ignored.",飽和した画素を判定するための閾値です。つまり、少なくとも1つのチャンネルが\\を超える画素は無視されます。
See alsogetSaturationThreshold,See alsogetSaturationThreshold
See alsosetHistBinNum,See alsosetHistBinNum
See alsosetRangeMaxVal,See alsosetRangeMaxVal
See alsosetSaturationThreshold,関連項目： alsosetSaturationThreshold
Creates an instance of SimpleWB.,SimpleWBのインスタンスを作成します。
A simple white balance algorithm that works by independently stretching each of the input image channels to the specified range. For increased robustness it ignores the top and bottom \(p\%\) of pixel values. ,シンプルなホワイトバランスアルゴリズムで、入力画像の各チャンネルを指定された範囲に独立して引き伸ばすことで動作します。堅牢性を高めるために、ピクセル値の上下を無視します。
Input image range maximum value.,入力画像範囲の最大値です。
See alsosetInputMax,See alsosetInputMax
See alsogetInputMax,See alsogetInputMax
Input image range minimum value.,入力映像範囲の最小値
See alsosetInputMin,参照：ArsosetInputMin
See alsogetInputMin,alsogetInputMin参照
Output image range maximum value.,出力映像範囲の最大値
See alsosetOutputMax,参照：アルソセットアウトプットマックス
See alsogetOutputMax,alsogetOutputMax参照
Output image range minimum value.,出力映像範囲の最小値
See alsosetOutputMin,参照：アルソセットアウトプットミン
See alsogetOutputMin,alsogetOutputMin参照
Percent of top/bottom values to ignore.,無視するトップ／ボトム値の割合。
See alsosetP,参照：アルソセットP
See alsogetP,alsogetP参照
Converts a rotation matrix to a rotation vector or vice versa.,回転行列を回転ベクトルに変換したり、逆に回転行列を回転ベクトルに変換したりします。
src : Input rotation vector (3x1 or 1x3) or rotation matrix (3x3).,src : 入力回転ベクトル（3x1 または 1x3），または回転行列（3x3）．
"dst : Output rotation matrix (3x3) or rotation vector (3x1 or 1x3), respectively.",dst : 回転行列（3x3）または回転ベクトル（3x1または1x3）をそれぞれ出力．
"jacobian : Optional output Jacobian matrix, 3x9 or 9x3, which is a matrix of partial derivatives of the output array components with respect to the input array components.",jacobian : オプションである出力ジャコビアン行列（3x9 または 9x3）．これは，入力配列成分に対する出力配列成分の偏微分の行列です．
"\[\begin{array}{l} \theta \leftarrow norm(r) \\ r \leftarrow r/ \theta \\ R = \cos(\theta) I + (1- \cos{\theta} ) r r^T + \sin(\theta) \vecthreethree{0}{-r_z}{r_y}{r_z}{0}{-r_x}{-r_y}{r_x}{0} \end{array}\]Inverse transformation can be also done easily, since\[\sin ( \theta ) \vecthreethree{0}{-r_z}{r_y}{r_z}{0}{-r_x}{-r_y}{r_x}{0} = \frac{R - R^T}{2}\]A rotation vector is a convenient and most compact representation of a rotation matrix (since any rotation matrix has just 3 degrees of freedom). The representation is used in the global 3D geometry optimization procedures like calibrateCamera, stereoCalibrate, or solvePnP .NoteMore information about the computation of the derivative of a 3D rotation matrix with respect to its exponential coordinate can be found in:",\♪♪～\R = I + (1- ˶ˆ꒳ˆ˵ ) r^T + ˶ˆ꒳ˆ˵ ) R = I + (1- ˶ˆ꒳ˆ˵ ) r^T + ˶ˆ꒳ˆ˵ ) R = I + (1- ˶ˆ꒳ˆ˵ ) R = I + (1- ˶ˆ꒳ˆ˵ ) R = I + (1- ˶ˆ꒳ˆ˵ ) R = I + (1- ˶ˆ꒳ˆ˵ ) R = I + (1- ˶ˆ꒳ˆ˵ ) R = I + (1- ˶ˆ꒳ˆ˵ ) R = I + (1- ˶ˆ꒳ˆ˵ ) R = I + (1- ˶ˆ꒳ˆ˵)\回転ベクトルは、回転行列の最もコンパクトで便利な表現です。この表現は、calibrateCamera、stereoCalibrate、solvePnPなどのグローバルな3Dジオメトリ最適化手順で使用されます。
"A Compact Formula for the Derivative of a 3-D Rotation in Exponential Coordinates, Guillermo Gallego, Anthony J. Yezzi [83]","A Compact Formula for the Derivative of a 3-D Rotation in Exponential Coordinates, Guillermo Gallego, Anthony J. Yezzi [83] に記載されています。"
Useful information on SE(3) and Lie Groups can be found in:,SE(3)とLie群に関する有用な情報は、以下にあります。
"A tutorial on SE(3) transformation parameterizations and on-manifold optimization, Jose-Luis Blanco [24]","A tutorial on SE(3) transformation parameterizations and on-manifold optimization, Jose-Luis Blanco [24]."
"Lie Groups for 2D and 3D Transformation, Ethan Eade [65]","Lie Groups for 2D and 3D Transformation, Ethan Eade [65]."
"A micro Lie theory for state estimation in robotics, Joan Solà, Jérémie Deray, Dinesh Atchuthan [226]Examples: samples/cpp/tutorial_code/features2D/Homography/decompose_homography.cpp, samples/cpp/tutorial_code/features2D/Homography/homography_from_camera_displacement.cpp, and samples/cpp/tutorial_code/features2D/Homography/pose_from_homography.cpp.","A micro Lie theory for state estimation in robotics, Joan Solà, Jérémie Deray, Dinesh Atchuthan [226]例： samples/cpp/tutorial_code/features2D/Homography/decompose_homography.cpp, samples/cpp/tutorial_code/features2D/Homography/homography_from_camera_displacement.cpp, samples/cpp/tutorial_code/features2D/Homography/pose_from_homography.cpp."
Finds a perspective transformation between two planes.,2つの平面間の透視変換を求めます。
"srcPoints : Coordinates of the points in the original plane, a matrix of the type CV_32FC2 or vector<Point2f> .",srcPoints :元の平面上の点の座標．CV_32FC2 型の行列，または vector<Point2f> 型の行列．
"dstPoints : Coordinates of the points in the target plane, a matrix of the type CV_32FC2 or a vector<Point2f> .",dstPoints :CV_32FC2 型の行列，または vector<Point2f> 型の行列．
method : Method used to compute a homography matrix. The following methods are possible:,method : ホモグラフィ行列の計算に用いられる手法．以下のような方法が考えられます．
"0 - a regular method using all the points, i.e., the least squares method",0 - すべての点を利用する通常の手法，つまり，最小二乗法．
RANSAC - RANSAC-based robust method,RANSAC - RANSACベースのロバスト法．
LMEDS - Least-Median robust method,LMEDS - Least-Median ロバスト手法
RHO - PROSAC-based robust method,RHO - PROSACベースのロバスト手法
"ransacReprojThreshold : Maximum allowed reprojection error to treat a point pair as an inlier (used in the RANSAC and RHO methods only). That is, if ",ransacReprojThreshold : 点のペアをインライアとして扱うための最大許容再投影誤差（RANSAC法とRHO法のみで使用）。つまり，次のような場合
\[\| \texttt{dstPoints} _i - \texttt{convertPointsHomogeneous} ( \texttt{H} * \texttt{srcPoints} _i) \|_2 > \texttt{ransacReprojThreshold}\],\であれば_i - ˶ˆ꒳ˆ˵ )( ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ > ˶ ransacReprojThreshold˶
" then the point \(i\) is considered as an outlier. If srcPoints and dstPoints are measured in pixels, it usually makes sense to set this parameter somewhere in the range of 1 to 10.", とすると，点\(i\)は外れ値とみなされます．srcPoints と dstPoints がピクセル単位で計測される場合，通常，このパラメータを 1 から 10 の範囲で設定するのが良いでしょう．
mask : Optional output mask set by a robust method ( RANSAC or LMeDS ). Note that the input mask values are ignored.,mask : ロバスト手法（RANSAC や LMeDS ）で設定されるオプションの出力マスク．なお，入力マスクの値は無視されます．
maxIters : The maximum number of RANSAC iterations.,maxIters : RANSACの反復回数の最大値です．
"confidence : Confidence level, between 0 and 1.",confidence : 信頼度，0から1の間で指定します．
"The function finds and returns the perspective transformation \(H\) between the source and the destination planes:\[s_i \vecthree{x'_i}{y'_i}{1} \sim H \vecthree{x_i}{y_i}{1}\]so that the back-projection error\[\sum _i \left ( x'_i- \frac{h_{11} x_i + h_{12} y_i + h_{13}}{h_{31} x_i + h_{32} y_i + h_{33}} \right )^2+ \left ( y'_i- \frac{h_{21} x_i + h_{22} y_i + h_{23}}{h_{31} x_i + h_{32} y_i + h_{33}} \right )^2\]is minimized. If the parameter method is set to the default value 0, the function uses all the point pairs to compute an initial homography estimate with a simple least-squares scheme.However, if not all of the point pairs ( \(srcPoints_i\), \(dstPoints_i\) ) fit the rigid perspective transformation (that is, there are some outliers), this initial estimate will be poor. In this case, you can use one of the three robust methods. The methods RANSAC, LMeDS and RHO try many different random subsets of the corresponding point pairs (of four pairs each, collinear pairs are discarded), estimate the homography matrix using this subset and a simple least-squares algorithm, and then compute the quality/goodness of the computed homography (which is the number of inliers for RANSAC or the least median re-projection error for LMeDS). The best subset is then used to produce the initial estimate of the homography matrix and the mask of inliers/outliers.Regardless of the method, robust or not, the computed homography matrix is refined further (using inliers only in case of a robust method) with the Levenberg-Marquardt method to reduce the re-projection error even more.The methods RANSAC and RHO can handle practically any ratio of outliers but need a threshold to distinguish inliers from outliers. The method LMeDS does not need any threshold but it works correctly only when there are more than 50% of inliers. Finally, if there are no outliers and the noise is rather small, use the default method (method=0).The function is used to find initial intrinsic and extrinsic matrices. Homography matrix is determined up to a scale. Thus, it is normalized so that \(h_{33}=1\). Note that whenever an \(H\) matrix cannot be estimated, an empty one will be returned.See alsogetAffineTransform, estimateAffine2D, estimateAffinePartial2D, getPerspectiveTransform, warpPerspective, perspectiveTransformExamples: samples/cpp/tutorial_code/features2D/Homography/decompose_homography.cpp, samples/cpp/tutorial_code/features2D/Homography/homography_from_camera_displacement.cpp, and samples/cpp/tutorial_code/features2D/Homography/pose_from_homography.cpp.","この関数は，ソース平面とデスティネーション平面の間の透視変換\(H\)を求めて返します．\逆投影誤差\[sum _i ˶left ( x'_i- ˶frac{h_{11} x_i + h_{12} y_i + h_{13}}{h_{31} x_i + h_{32} y_i + h_{33}} ˶right )^2+ ୨୧-͈୧-͈୧-͈୧-͈!\h_{21} x_i + h_{22} y_i + h_{23}}{h_{31} x_i + h_{32} y_i + h_{33}} eldest ) ^2+ eldest ( y'_i- ˶ˆ꒳ˆ˵ )\)^2\]を最小化します。パラメータメソッドがデフォルト値の 0 に設定されている場合，この関数は，すべての点のペアを使って，単純な最小二乗法でホモグラフィの初期推定値を計算します．しかし，すべての点のペア（ ˶˙º̬˙˶,˶˙º̬˙˶）が剛体透視変換に適合しない場合（つまり，外れ値がある場合），この初期推定値は悪くなります．このような場合には，3つのロバストな手法のうちの1つを用いることができます．RANSAC，LMeDS，RHOの各手法は，対応する点のペアの多くの異なるランダムなサブセット（各4ペアで，共線的なペアは破棄されます）を試し，このサブセットと単純な最小二乗アルゴリズムを使ってホモグラフィ行列を推定し，計算されたホモグラフィの品質/良さ（RANSACの場合はインライアの数，LMeDSの場合は再投影誤差の最小中央値）を計算します．ロバストかどうかに関わらず，計算されたホモグラフィ行列は，Levenberg-Marquardt法を用いて（ロバスト法の場合はインライアのみを用いて）さらに精密化され，再投影誤差がさらに減少します。RANSAC法とRHO法は，アウトライアの比率を実質的に問わずに扱うことができますが，インライアとアウトライアを区別するための閾値が必要です。RANSACとRHOはアウトライアの比率に関係なく扱えますが、インライアとアウトライアを区別するための閾値が必要です。最後に，外れ値がなく，ノイズがかなり小さい場合は，デフォルトの手法（method=0）を利用します．この関数は，初期の内在行列と外在行列を求めるために利用されます．ホモグラフィー行列は，ある尺度まで決定されます．そのため，\(h_{33}=1\)となるように正規化されています．また，getAffineTransform, estimateAffine2D, estimateAffinePartial2D, getPerspectiveTransform, warpPerspective, perspectiveTransformExamples: samples/cpp/tutorial_code/features2D/Homography/decompose_homography.cpp, samples/cpp/tutorial_code/features2D/Homography/homography_from_camera_displacement.cpp, and samples/cpp/tutorial_code/features2D/Homography/pose_from_homography.cpp."
Computes an RQ decomposition of 3x3 matrices.,3x3 行列の RQ 分解を計算します．
src : 3x3 input matrix.,src : 3x3 の入力行列．
mtxR : Output 3x3 upper-triangular matrix.,mtxR : 3x3 の上三角行列を出力．
mtxQ : Output 3x3 orthogonal matrix.,mtxQ : 3x3 の直交行列を出力．
Qx : Optional output 3x3 rotation matrix around x-axis.,Qx : オプションで，3x3 の x 軸周りの回転行列を出力します．
Qy : Optional output 3x3 rotation matrix around y-axis.,Qy :Qy : オプションで出力される 3x3 の y-軸周りの回転行列．
Qz : Optional output 3x3 rotation matrix around z-axis.,Qz : オプションで，3x3 の z-軸周りの回転行列を出力します．
"The function computes a RQ decomposition using the given rotations. This function is used in decomposeProjectionMatrix to decompose the left 3x3 submatrix of a projection matrix into a camera and a rotation matrix.It optionally returns three rotation matrices, one for each axis, and the three Euler angles in degrees (as the return value) that could be used in OpenGL. Note, there is always more than one sequence of rotations about the three principal axes that results in the same orientation of an object, e.g. see [225] . Returned tree rotation matrices and corresponding three Euler angles are only one of the possible solutions.",この関数は，与えられた回転を用いてRQ分解を行います．この関数は， decomposeProjectionMatrix の中で，射影行列の左3x3部分行列をカメラ行列と回転行列に分解するために利用されます．また，オプションとして，各軸に1つずつ配置された3つの回転行列と，OpenGLで利用可能な3つのオイラー角を（戻り値として）返します．なお，オブジェクトの向きが同じになるような3つの主軸を中心とした回転のシーケンスは，常に1つ以上存在します（例えば，[225]参照）．返された木の回転行列とそれに対応する3つのオイラー角は，可能な解決策の1つに過ぎません．
Decomposes a projection matrix into a rotation matrix and a camera intrinsic matrix.,投影行列を，回転行列とカメラ固有の行列に分解します．
projMatrix : 3x4 input projection matrix P.,projMatrix : 3x4 の入力射影行列 P.
cameraMatrix : Output 3x3 camera intrinsic matrix \(\cameramatrix{A}\).,cameraMatrix :3x3 の出力カメラ固有行列 ˶ˆ꒳ˆ˵ )
rotMatrix : Output 3x3 external rotation matrix R.,rotMatrix :3x3 の外部回転行列 R を出力します．
transVect : Output 4x1 translation vector T.,transVect : 4x1 並進ベクトル T を出力．
rotMatrixX : Optional 3x3 rotation matrix around x-axis.,rotMatrixX : オプションである 3x3 の x 軸周りの回転行列．
rotMatrixY : Optional 3x3 rotation matrix around y-axis.,rotMatrixY : オプションである 3x3 の Y 軸周りの回転行列．
rotMatrixZ : Optional 3x3 rotation matrix around z-axis.,rotMatrixZ : オプションである 3x3 の z-軸周りの回転行列．
eulerAngles : Optional three-element vector containing three Euler angles of rotation in degrees.,eulerAngles :オプションで，3つのオイラー回転角（度）を含む3要素のベクトル．
"The function computes a decomposition of a projection matrix into a calibration and a rotation matrix and the position of a camera.It optionally returns three rotation matrices, one for each axis, and three Euler angles that could be used in OpenGL. Note, there is always more than one sequence of rotations about the three principal axes that results in the same orientation of an object, e.g. see [225] . Returned tree rotation matrices and corresponding three Euler angles are only one of the possible solutions.The function is based on RQDecomp3x3 .",この関数は，射影行列をキャリブレーション行列と回転行列に分解し，カメラの位置を計算します．また，オプションとして，各軸に1つずつ配置された3つの回転行列と，OpenGLで利用可能な3つのオイラー角を返します．なお，オブジェクトの向きが同じになるような3つの主軸を中心とした回転のシーケンスは，常に1つ以上存在します（例えば，[225]を参照してください）．返された木の回転行列と，それに対応する3つのオイラー角は，可能な解の1つに過ぎません．
Computes partial derivatives of the matrix product for each multiplied matrix.,乗算された各行列に対する行列積の偏導関数を求めます．
A : First multiplied matrix.,A : 1 番目に乗算された行列。
B : Second multiplied matrix.,B : 2 番目に乗算された行列。
dABdA : First output derivative matrix d(A*B)/dA of size \(\texttt{A.rows*B.cols} \times {A.rows*A.cols}\) .,dABdA : 1 番目に出力される微分行列 d(A*B)/dA (size ˶‾᷄ -̫ ‾᷅˵)です。
dABdB : Second output derivative matrix d(A*B)/dB of size \(\texttt{A.rows*B.cols} \times {B.rows*B.cols}\) .,dABdB : Second output derivative matrix d(A*B)/ddB of size ˶‾᷄ -̫ ‾᷅˵ ˶‾᷅˵ .
The function computes partial derivatives of the elements of the matrix product \(A*B\) with regard to the elements of each of the two input matrices. The function is used to compute the Jacobian matrices in stereoCalibrate but can also be used in any other similar optimization function.,この関数は，入力された2つの行列のそれぞれの要素に対する，行列積 ˶A*B˶の要素の偏微分を求めます．この関数は， stereoCalibrate のジャコビアン行列を計算するために利用されますが，他の類似した最適化関数でも利用できます．
Combines two rotation-and-shift transformations.,2つの回転-シフト変換を組み合わせます。
rvec1 : First rotation vector.,rvec1 ：1つ目の回転ベクトル。
tvec1 : First translation vector.,tvec1 ：1つ目の並進ベクトル
rvec2 : Second rotation vector.,rvec2 ：2つ目の回転ベクトル。
tvec2 : Second translation vector.,tvec2 : 2つ目の並進ベクトル。
rvec3 : Output rotation vector of the superposition.,rvec3 : 重ね合わせの出力回転ベクトル。
tvec3 : Output translation vector of the superposition.,tvec3 : 重ね合わせの並進ベクトルを出力する。
dr3dr1 : Optional output derivative of rvec3 with regard to rvec1,dr3dr1 : rvec1に対するrvec3の出力微分（オプション
dr3dt1 : Optional output derivative of rvec3 with regard to tvec1,dr3dt1 : tvec1を基準にしたrvec3の任意出力微分
dr3dr2 : Optional output derivative of rvec3 with regard to rvec2,dr3dr2 : rvec2に対するrvec3の出力導関数（オプション
dr3dt2 : Optional output derivative of rvec3 with regard to tvec2,dr3dt2 ：tvec2に対するrvec3の出力微分です。
dt3dr1 : Optional output derivative of tvec3 with regard to rvec1,dt3dr1：tvec3の出力微分（オプション）で、rvec1を基準とする。
dt3dt1 : Optional output derivative of tvec3 with regard to tvec1,dt3dt1：tvec1に対するtvec3の出力微分です。
dt3dr2 : Optional output derivative of tvec3 with regard to rvec2,dt3dr2：tvec3の出力微分（オプション）：rvec2に関して
dt3dt2 : Optional output derivative of tvec3 with regard to tvec2,dt3dt2 ： tvec3 の tvec2 に対する出力微分（オプション
"The functions compute:\[\begin{array}{l} \texttt{rvec3} = \mathrm{rodrigues} ^{-1} \left ( \mathrm{rodrigues} ( \texttt{rvec2} ) \cdot \mathrm{rodrigues} ( \texttt{rvec1} ) \right ) \\ \texttt{tvec3} = \mathrm{rodrigues} ( \texttt{rvec2} ) \cdot \texttt{tvec1} + \texttt{tvec2} \end{array} ,\]where \(\mathrm{rodrigues}\) denotes a rotation vector to a rotation matrix transformation, and \(\mathrm{rodrigues}^{-1}\) denotes the inverse transformation. See Rodrigues for details.Also, the functions can compute the derivatives of the output vectors with regards to the input vectors (see matMulDeriv ). The functions are used inside stereoCalibrate but can also be used in your own code where Levenberg-Marquardt or another gradient-based solver is used to optimize a function that contains a matrix multiplication.",この関数は、次のように計算します。\¶texttt{rvec3} = ¶mathrm{rodrigues}.^{-1} ˶ˆ꒳ˆ˵ ( ˶ˆ꒳ˆ˵ )( ˶ˆ꒳ˆ˵ )( ˶ˆ꒳ˆ˵ )\\ ♪♪～( ˶ˆ꒳ˆ˵ )+ ˶ˆ꒳ˆ˵ )\ここで、\(mathrm{rodrigues}\)は回転ベクトルから回転行列への変換を表し、\(mathrm{rodrigues}^{-1}\)は逆変換を表します。また，この関数は，入力ベクトルに対する出力ベクトルの導関数を計算することができます（matMulDeriv 参照）．また，この関数は，入力ベクトルに対する出力ベクトルの導関数を計算することができます（matMulDeriv を参照）．これらの関数は stereoCalibrate 内で使用されますが，Levenberg-Marquardt などの勾配ベースのソルバーを使って，行列の乗算を含む関数を最適化するような独自のコードでも使用できます．
Projects 3D points to an image plane.,3D ポイントを画像平面に投影します。
"objectPoints : Array of object points expressed wrt. the world coordinate frame. A 3xN/Nx3 1-channel or 1xN/Nx1 3-channel (or vector<Point3f> ), where N is the number of points in the view.",objectPoints :ワールド座標系を基準にして表現されたオブジェクトポイントの配列。3xN/Nx3の1チャンネルまたは1xN/Nx1の3チャンネル（またはvector<Point3f>）で，Nはビュー内の点の数です．
"rvec : The rotation vector (Rodrigues) that, together with tvec, performs a change of basis from world to camera coordinate system, see calibrateCamera for details.",rvec :tvecと一緒に、ワールド座標系からカメラ座標系への基底変更を行う回転ベクトル（Rodrigues）。
"tvec : The translation vector, see parameter description above.",tvec :並進ベクトル，上記パラメータの説明を参照．
cameraMatrix : Camera intrinsic matrix \(\cameramatrix{A}\) .,cameraMatrix :カメラ固有の行列．
"distCoeffs : Input vector of distortion coefficients \(\distcoeffs\) . If the vector is empty, the zero distortion coefficients are assumed.",distCoeffs : ディストーション係数の入力ベクトル．このベクトルが空の場合は，歪み係数が0であるとみなされます．
"imagePoints : Output array of image points, 1xN/Nx1 2-channel, or vector<Point2f> .",imagePoints :1xN/Nx1 2-channel または vector<Point2f> で表される画像点の出力配列．
"jacobian : Optional output 2Nx(10+<numDistCoeffs>) jacobian matrix of derivatives of image points with respect to components of the rotation vector, translation vector, focal lengths, coordinates of the principal point and the distortion coefficients. In the old interface different components of the jacobian are returned via different output parameters.",jacobian : オプションで，回転ベクトル，並進ベクトル，焦点距離，主点の座標，歪み係数の各成分に対する画像点の微分を表す，2Nx(10+<numDistCoeffs>) のヤコビ行列を出力します．旧インターフェースでは，ヤコビアンの異なる成分は，異なる出力パラメータを介して返されます．
"aspectRatio : Optional ""fixed aspect ratio"" parameter. If the parameter is not 0, the function assumes that the aspect ratio ( \(f_x / f_y\)) is fixed and correspondingly adjusts the jacobian matrix.",aspectRatio :オプションの「固定アスペクト比」パラメータ．このパラメータが0ではない場合，この関数はアスペクト比（\(f_x / f_y\)）が固定であると仮定し，それに応じてヤコビアン行列を調整します．
"The function computes the 2D projections of 3D points to the image plane, given intrinsic and extrinsic camera parameters. Optionally, the function computes Jacobians -matrices of partial derivatives of image points coordinates (as functions of all the input parameters) with respect to the particular parameters, intrinsic and/or extrinsic. The Jacobians are used during the global optimization in calibrateCamera, solvePnP, and stereoCalibrate. The function itself can also be used to compute a re-projection error, given the current intrinsic and extrinsic parameters.NoteBy setting rvec = tvec = \([0, 0, 0]\), or by setting cameraMatrix to a 3x3 identity matrix, or by passing zero distortion coefficients, one can get various useful partial cases of the function. This means, one can compute the distorted coordinates for a sparse set of points or apply a perspective transformation (and also compute the derivatives) in the ideal zero-distortion setup.","この関数は，カメラの内部および外部パラメータが与えられた場合に，3次元点の画像平面への2次元投影を求めます．オプションとして，この関数は，（すべての入力パラメータの関数としての）画像点の座標の偏微分の行列であるヤコビアンを，特定のパラメータ（内部および外部）に関して求めます．このヤコビアンは， calibrateCamera, solvePnP, stereoCalibrate における全体最適化の際に利用されます．Notervec = tvec = ˶([0, 0, 0]˶)を設定したり，cameraMatrix を 3x3 の単位行列にしたり，歪み係数を 0 にしたりすると，この関数の様々な便利な部分例を得ることができます．つまり，理想的なゼロディストーションの設定において，疎な点群に対する歪んだ座標を計算したり，透視変換を適用したり（そして，その微分を計算したり）することができるのです．"
"Finds an object pose from 3D-2D point correspondences. This function returns the rotation and the translation vectors that transform a 3D point expressed in the object coordinate frame to the camera coordinate frame, using different methods:",3次元と2次元の点の対応関係から，物体の姿勢を見つけます．この関数は，オブジェクト座標フレームで表現された3次元点を，カメラ座標フレームに変換するための回転ベクトルと並進ベクトルを，それぞれ異なる方法で返します．
"objectPoints : Array of object points in the object coordinate space, Nx3 1-channel or 1xN/Nx1 3-channel, where N is the number of points. vector<Point3d> can be also passed here.",objectPoints :Nx3 1-channel または 1xN/Nx1 3-channel（N は点の数）のオブジェクト座標空間におけるオブジェクト点の配列．
"imagePoints : Array of corresponding image points, Nx2 1-channel or 1xN/Nx1 2-channel, where N is the number of points. vector<Point2d> can be also passed here.",imagePoints :Nx2 1-channel または 1xN/Nx1 2-channel で表される，対応する画像上の点の配列（N は点の数）．
cameraMatrix : Input camera intrinsic matrix \(\cameramatrix{A}\) .,cameraMatrix :入力されるカメラ固有の行列 ˶ˆ꒳ˆ˵ )
"distCoeffs : Input vector of distortion coefficients \(\distcoeffs\). If the vector is NULL/empty, the zero distortion coefficients are assumed.",distCoeffs : 入力される歪み係数のベクトルです．このベクトルがNULL/空の場合は，歪み係数が0であると仮定される．
"rvec : Output rotation vector (see Rodrigues ) that, together with tvec, brings points from the model coordinate system to the camera coordinate system.",rvec :tvecと一緒に，モデル座標系からカメラ座標系へと点を移動させる回転ベクトル（Rodrigues参照）を出力．
tvec : Output translation vector.,tvec :並進ベクトルの出力
"useExtrinsicGuess : Parameter used for SOLVEPNP_ITERATIVE. If true (1), the function uses the provided rvec and tvec values as initial approximations of the rotation and translation vectors, respectively, and further optimizes them.",useExtrinsicGuess : SOLVEPNP_ITERATIVEで使用されるパラメータ．true (1) の場合，この関数は，与えられた rvec と tvec の値を，それぞれ回転ベクトルと並進ベクトルの初期近似値として使用し，さらにそれらを最適化する．
flags : Method for solving a PnP problem:,フラグ．PnP問題の解法。
"SOLVEPNP_ITERATIVE Iterative method is based on a Levenberg-Marquardt optimization. In this case the function finds such a pose that minimizes reprojection error, that is the sum of squared distances between the observed projections imagePoints and the projected (using projectPoints ) objectPoints .",SOLVEPNP_ITERATIVE 反復法は，Levenberg-Marquardt 最適化に基づいています．この場合，この関数は，再投影誤差（観測された投影像 imagePoints と，（projectPoints を用いて）投影された objectPoints との間の二乗距離の総和）を最小化するようなポーズを見つけます．
"SOLVEPNP_P3P Method is based on the paper of X.S. Gao, X.-R. Hou, J. Tang, H.-F. Chang ""Complete Solution Classification for the Perspective-Three-Point Problem"" ([84]). In this case the function requires exactly four object and image points.","SOLVEPNP_P3P 法は、X.S. Gao, X.-R. Hou, J. Tang, H.S. Gao, J. Tang, H.S. Gao の論文に基づいています。Hou, J. Tang, H.-F.Chang ""Complete Solution Classification for the Perspective-Three-Point Problem"" ([84])に基づいている。この場合、関数はちょうど4つの物体と画像のポイントを必要とする。"
"SOLVEPNP_AP3P Method is based on the paper of T. Ke, S. Roumeliotis ""An Efficient Algebraic Solution to the Perspective-Three-Point Problem"" ([126]). In this case the function requires exactly four object and image points.","SOLVEPNP_AP3P 法は、T. Ke, S. Roumeliotis ""An Efficient Algebraic Solution to the Perspective-Three-Point Problem"" ([126])の論文に基づいています。この場合、関数はちょうど4つのオブジェクトポイントと画像ポイントを必要とします。"
"SOLVEPNP_EPNP Method has been introduced by F. Moreno-Noguer, V. Lepetit and P. Fua in the paper ""EPnP: Efficient Perspective-n-Point Camera Pose Estimation"" ([138]).","SOLVEPNP_EPNP F. Moreno-Noguer, V. Lepetit and P. Fua による論文 ""EPnP: Efficient Perspective-n-Point Camera Pose Estimation"" ([138])で紹介された手法。"
SOLVEPNP_DLS Broken implementation. Using this flag will fallback to EPnP. ,SOLVEPNP_DLS 壊れた実装。このフラグを使用すると、EPnPにフォールバックする。
"Method is based on the paper of J. Hesch and S. Roumeliotis. ""A Direct Least-Squares (DLS) Method for PnP"" ([110]).","この方法は、J. Hesch and S. Roumeliotis の論文に基づいている。""A Direct Least-Squares (DLS) Method for PnP"" ([110])という論文に基づいている。"
SOLVEPNP_UPNP Broken implementation. Using this flag will fallback to EPnP. ,SOLVEPNP_UPNP 壊れた実装。このフラグを使用すると、EPnPにフォールバックする。
"Method is based on the paper of A. Penate-Sanchez, J. Andrade-Cetto, F. Moreno-Noguer. ""Exhaustive Linearization for Robust Camera Pose and Focal Length","この方法は、A. Penate-Sanchez, J. Andrade-Cetto, F. Moreno-Noguer の論文に基づいている。""Exhaustive Linearization for Robust Camera Pose and Focal Length Estimation"" ([191])"
"Estimation"" ([191]). In this case the function also estimates the parameters \(f_x\) and \(f_y\) assuming that both have the same value. Then the cameraMatrix is updated with the estimated focal length.","Estimation"" ([191])という論文があります．この場合，この関数は，パラメータ ˶(f_x˶)と˶(f_y˶)が同じ値であると仮定して，パラメータの推定も行います．そして，推定された焦点距離で cameraMatrix が更新されます．"
"SOLVEPNP_IPPE Method is based on the paper of T. Collins and A. Bartoli. ""Infinitesimal Plane-Based Pose Estimation"" ([48]). This method requires coplanar object points.","SOLVEPNP_IPPE メソッドは、T. Collins と A. Bartoli の論文に基づいています。""Infinitesimal Plane-Based Pose Estimation"" ([48])に基づいています．この方法は、コプラナーなオブジェクトポイントを必要とします。"
"SOLVEPNP_IPPE_SQUARE Method is based on the paper of Toby Collins and Adrien Bartoli. ""Infinitesimal Plane-Based Pose Estimation"" ([48]). This method is suitable for marker pose estimation. It requires 4 coplanar object points defined in the following order:","SOLVEPNP_IPPE_SQUARE 方法は、Toby CollinsとAdrien Bartoliの論文に基づいています。""Infinitesimal Plane-Based Pose Estimation"" ([48])を参考にしている。この方法は、マーカーのポーズ推定に適しています。以下の順序で定義された4つのコプラナーオブジェクトポイントを必要とします。"
"point 0: [-squareLength / 2, squareLength / 2, 0]","点0： [-squareLength / 2, squareLength / 2, 0] 。"
"point 1: [ squareLength / 2, squareLength / 2, 0]","point 1: [ squareLength / 2, squareLength / 2, 0]の順に定義される。"
"point 2: [ squareLength / 2, -squareLength / 2, 0]","ポイント2：[ squareLength / 2, -squareLength / 2, 0]の順になります。"
"point 3: [-squareLength / 2, -squareLength / 2, 0]","点3：［-squareLength / 2, -squareLength / 2, 0］。"
"SOLVEPNP_SQPNP Method is based on the paper ""A Consistently Fast and Globally Optimal Solution to the",SOLVEPNP_SQPNP法は、論文「A Consistently Fast and Globally Optimal Solution to the Perspective-n-Point Problem」に基づいている。
"Perspective-n-Point Problem"" by G. Terzakis and M.Lourakis ([243]). It requires 3 or more points.","Perspective-n-Point Problem"" by G. Terzakis and M.Lourakis ([243])という論文に基づいている。3つ以上のポイントが必要です。"
"P3P methods (SOLVEPNP_P3P, SOLVEPNP_AP3P): need 4 input points to return a unique solution.",P3Pメソッド（SOLVEPNP_P3P、SOLVEPNP_AP3P）：唯一の解を返すために4つの入力点を必要とする。
SOLVEPNP_IPPE Input points must be >= 4 and object points must be coplanar.,SOLVEPNP_IPPE 入力点は >= 4 でなければならず、オブジェクト点は同一平面上になければならない。
SOLVEPNP_IPPE_SQUARE Special case suitable for marker pose estimation. Number of input points must be 4. Object points must be defined in the following order:,SOLVEPNP_IPPE_SQUARE マーカーのポーズ推定に適した特殊なケース。入力点の数は4でなければなりません。オブジェクトポイントは以下の順序で定義する必要があります。
"for all the other flags, number of input points must be >= 4 and object points can be in any configuration.The function estimates the object pose given a set of object points, their corresponding image projections, as well as the camera intrinsic matrix and the distortion coefficients, see the figure below (more precisely, the X-axis of the camera frame is pointing to the right, the Y-axis downward and the Z-axis forward).Points expressed in the world frame \( \bf{X}_w \) are projected into the image plane \( \left[ u, v \right] \) using the perspective projection model \( \Pi \) and the camera intrinsic parameters matrix \( \bf{A} \):\[ \begin{align*} \begin{bmatrix} u \\ v \\ 1 \end{bmatrix} &= \bf{A} \hspace{0.1em} \Pi \hspace{0.2em} ^{c}\bf{T}_w \begin{bmatrix} X_{w} \\ Y_{w} \\ Z_{w} \\ 1 \end{bmatrix} \\ \begin{bmatrix} u \\ v \\ 1 \end{bmatrix} &= \begin{bmatrix} f_x & 0 & c_x \\ 0 & f_y & c_y \\ 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \end{bmatrix} \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \\ 0 & 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} X_{w} \\ Y_{w} \\ Z_{w} \\ 1 \end{bmatrix} \end{align*} \]The estimated pose is thus the rotation (rvec) and the translation (tvec) vectors that allow transforming a 3D point expressed in the world frame into the camera frame:\[ \begin{align*} \begin{bmatrix} X_c \\ Y_c \\ Z_c \\ 1 \end{bmatrix} &= \hspace{0.2em} ^{c}\bf{T}_w \begin{bmatrix} X_{w} \\ Y_{w} \\ Z_{w} \\ 1 \end{bmatrix} \\ \begin{bmatrix} X_c \\ Y_c \\ Z_c \\ 1 \end{bmatrix} &= \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \\ 0 & 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} X_{w} \\ Y_{w} \\ Z_{w} \\ 1 \end{bmatrix} \end{align*} \]Note","この関数は，オブジェクトポイントの集合と，それに対応する画像投影，カメラの固有行列と歪み係数が与えられた場合に，オブジェクトの姿勢を推定します（より正確には，カメラフレームのX軸は右向き，Y軸は下向き，Z軸は前向き）．ワールドフレームで表現された点は、透視投影モデルとカメラ固有のパラメータマトリクスを用いて、画像平面に投影されます。\♪ u ♪ v ♪ 1 ♪ end{bmatrix} &= ˶‾᷄ -̫ ‾᷅˵˵\0.1em\\\\^{c}\\{T}_wX_{w}\\ Y_{w}\\ ♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪\\ ♪♪♪♪♪～\\ ♪♪♪♪♪♪♪♪～\♪♪～♪ 1 & 0 & 0 & 0 ♪ 0 & 0 & 1 & 0 ♪ END\r_{11} & r_{12} & r_{13} & t_x ¶ r_{21} & r_{22} & r_{23} & t_y ¶ r_{31} & r_{32} & r_{33} & t_z ¶ 0 & 0 & 1 ¶ end{bmatrix}.\♪♪♪♪♪♪♪～X_{w}\\ X_{w}/Y_{w}\\ ♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪\\\\\♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪\推定されたポーズは、ワールドフレームで表現された3Dポイントをカメラフレームに変換するための回転（rvec）と平行移動（tvec）のベクトルとなります。\\\\X_c ¶ Y_c ¶ Z_c ¶ 1 ¶end{bmatrix} &= ¶hspace{0.2em}.^{c}\\{T}_wX_{w}\\ Y_{w}\\ ♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪\\ ♪♪♪♪♪～\\\\♪ X, Y, Z\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\♪♪♪♪♪♪♪～X_{w}\\ X_{w}/Y_{w}\\ ♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪\\\\\♪♪♪♪end{align*}\Note"
An example of how to use solvePnP for planar augmented reality can be found at opencv_source_code/samples/python/plane_ar.py,平面型拡張現実感に対する solvePnP の使用例は opencv_source_code/samples/python/plane_ar.py にあります。
If you are using Python:,Pythonをお使いの方へ。
Numpy array slices won't work as input because solvePnP requires contiguous arrays (enforced by the assertion using cv::Mat::checkVector() around line 55 of modules/calib3d/src/solvepnp.cpp version 2.4.9),solvePnP は連続した配列を必要とするので，Numpy 配列スライスは入力として動作しません（modules/calib3d/src/solvepnp.cpp version 2.4.9 の 55 行目あたりで cv::Mat::checkVector() を用いたアサーションにより強制されています）．
"The P3P algorithm requires image points to be in an array of shape (N,1,2) due to its calling of undistortPoints (around line 75 of modules/calib3d/src/solvepnp.cpp version 2.4.9) which requires 2-channel information.","P3P アルゴリズムでは，2 チャンネルの情報を必要とする undistortPoints の呼び出し（modules/calib3d/src/solvepnp.cpp version 2.4.9 の 75 行目付近）のために，画像ポイントが (N,1,2) の形をした配列になっている必要があります．"
"Thus, given some data D = np.array(...) where D.shape = (N,M), in order to use a subset of it as, e.g., imagePoints, one must effectively copy it into a new array: imagePoints = np.ascontiguousarray(D[:,:2]).reshape((N,1,2))","したがって，あるデータ D = np.array(...) (D.shape = (N,M)) が与えられた場合，そのサブセットを例えば imagePoints として使用するためには，それを新しい配列に効果的にコピーしなければなりません： imagePoints = np.ascontiguousarray(D[:,:2]).reshape((N,1,2))"
"The methods SOLVEPNP_DLS and SOLVEPNP_UPNP cannot be used as the current implementations are unstable and sometimes give completely wrong results. If you pass one of these two flags, SOLVEPNP_EPNP method will be used instead.",SOLVEPNP_DLSおよびSOLVEPNP_UPNPメソッドは、現在の実装が不安定で、完全に間違った結果を与えることがあるため、使用できません。これら2つのフラグのいずれかを渡すと、代わりにSOLVEPNP_EPNPメソッドが使用されます。
"The minimum number of points is 4 in the general case. In the case of SOLVEPNP_P3P and SOLVEPNP_AP3P methods, it is required to use exactly 4 points (the first 3 points are used to estimate all the solutions of the P3P problem, the last one is used to retain the best solution that minimizes the reprojection error).",一般的なケースでは、最小ポイント数は4である。SOLVEPNP_P3PおよびSOLVEPNP_AP3P法の場合、正確に4点を使用することが必要である（最初の3点はP3P問題のすべての解を推定するために使用され、最後の1点は再投影誤差を最小化する最良の解を保持するために使用される）。
"With SOLVEPNP_ITERATIVE method and useExtrinsicGuess=true, the minimum number of points is 3 (3 points are sufficient to compute a pose but there are up to 4 solutions). The initial solution should be close to the global solution to converge.",SOLVEPNP_ITERATIVEメソッドとuseExtrinsicGuess=trueを使用した場合、最小のポイント数は3です（ポーズを計算するには3ポイントで十分ですが、最大で4つの解があります）。収束させるためには、初期解をグローバル解に近づける必要があります。
With SOLVEPNP_IPPE input points must be >= 4 and object points must be coplanar.,SOLVEPNP_IPPEの場合、入力ポイントは≧4でなければならず、オブジェクトポイントはコプラナーでなければなりません。
With SOLVEPNP_IPPE_SQUARE this is a special case suitable for marker pose estimation. Number of input points must be 4. Object points must be defined in the following order:,SOLVEPNP_IPPE_SQUARE の場合、これはマーカーのポーズ推定に適した特殊なケースです。入力点の数は 4 でなければなりません。オブジェクトポイントは以下の順序で定義しなければならない。
"point 3: [-squareLength / 2, -squareLength / 2, 0]With SOLVEPNP_SQPNP input points must be >= 3Examples: samples/cpp/tutorial_code/features2D/Homography/decompose_homography.cpp, and samples/cpp/tutorial_code/features2D/Homography/homography_from_camera_displacement.cpp.","point 3: [-squareLength / 2, -squareLength / 2, 0]SOLVEPNP_SQPNPの場合、入力ポイントは≧3でなければなりませんサンプル：samples/cpp/tutorial_code/features2D/Homography/decompose_homography.cpp、samples/cpp/tutorial_code/features2D/Homography/homography_from_camera_displacement.cpp。"
Finds an object pose from 3D-2D point correspondences using the RANSAC scheme.,RANSACスキームを用いて，3D-2Dの点の対応関係から物体のポーズを見つけます．
iterationsCount : Number of iterations.,iterationsCount :イテレーションの回数．
reprojectionError : Inlier threshold value used by the RANSAC procedure. The parameter value is the maximum allowed distance between the observed and computed point projections to consider it an inlier.,reprojectionError : RANSAC手順で用いられるインライアの閾値．このパラメータ値は，観測された点の投影と計算された点の投影の間で，それをインライアとみなすために許容される最大の距離です．
confidence : The probability that the algorithm produces a useful result.,confidence : アルゴリズムが有用な結果を生成する確率．
inliers : Output vector that contains indices of inliers in objectPoints and imagePoints .,inliers : objectPoints と imagePoints に含まれるインライアのインデックスを含む出力ベクトル．
flags : Method for solving a PnP problem (see solvePnP ).,flags :PnP 問題の解法（ solvePnP を参照）．
"The function estimates an object pose given a set of object points, their corresponding image projections, as well as the camera intrinsic matrix and the distortion coefficients. This function finds such a pose that minimizes reprojection error, that is, the sum of squared distances between the observed projections imagePoints and the projected (using projectPoints ) objectPoints. The use of RANSAC makes the function resistant to outliers.Note",この関数は，オブジェクトポイントの集合と，それに対応する画像投影，さらにカメラの固有行列と歪み係数が与えられた場合に，オブジェクトのポーズを推定します．この関数は，再投影誤差，つまり，観測された投影画像ポイントと，（projectPoints を用いて）投影されたオブジェクトポイントとの間の二乗距離の総和を最小にするようなポーズを求めます．RANSAC を用いることで，外れ値に強い関数となっています．
An example of how to use solvePNPRansac for object detection can be found at opencv_source_code/samples/cpp/tutorial_code/calib3d/real_time_pose_estimation/,オブジェクト検出のための solvePNPRansac の使用例は， opencv_source_code/samples/cpp/tutorial_code/calib3d/real_time_pose_estimation/ にあります．
The default method used to estimate the camera pose for the Minimal Sample Sets step is SOLVEPNP_EPNP. Exceptions are:,Minimal Sample Setsステップのカメラポーズの推定に使用されるデフォルトの方法はSOLVEPNP_EPNPです。例外があります。
"if you choose SOLVEPNP_P3P or SOLVEPNP_AP3P, these methods will be used.",SOLVEPNP_P3PまたはSOLVEPNP_AP3Pを選択した場合、これらの方法が使用されます。
"if the number of input points is equal to 4, SOLVEPNP_P3P is used.",入力ポイント数が4に等しい場合、SOLVEPNP_P3Pが使用されます。
"The method used to estimate the camera pose using all the inliers is defined by the flags parameters unless it is equal to SOLVEPNP_P3P or SOLVEPNP_AP3P. In this case, the method SOLVEPNP_EPNP will be used instead.",すべてのインライアを使用してカメラポーズを推定するために使用される手法は、SOLVEPNP_P3PまたはSOLVEPNP_AP3Pに等しくない限り、フラグパラメータによって定義される。この場合、代わりにSOLVEPNP_EPNP法が使用される。
Finds an initial camera intrinsic matrix from 3D-2D point correspondences.,3D-2D の点の対応関係から，初期のカメラ固有マトリックスを求めます．
objectPoints : Vector of vectors of the calibration pattern points in the calibration pattern coordinate space. In the old interface all the per-view vectors are concatenated. See calibrateCamera for details.,objectPoints :キャリブレーションパターン座標空間におけるキャリブレーションパターンの点のベクトルのベクトル．古いインターフェースでは，すべてのビューごとのベクトルが連結されます．詳細は， calibrateCamera を参照してください．
imagePoints : Vector of vectors of the projections of the calibration pattern points. In the old interface all the per-view vectors are concatenated.,imagePoints :キャリブレーションパターンの点の投影像のベクトル．旧インターフェースでは，すべてのビューごとのベクトルが連結されています．
imageSize : Image size in pixels used to initialize the principal point.,imageSize : 主点の初期化に用いられるピクセル単位の画像サイズ．
"aspectRatio : If it is zero or negative, both \(f_x\) and \(f_y\) are estimated independently. Otherwise, \(f_x = f_y * \texttt{aspectRatio}\) .",aspectRatio : 0または負の値であれば，\(f_x\)と\(f_y\)の両方が独立して推定される．それ以外の場合は，\(f_x = f_y * ˶ˆ꒳ˆ˵ ) 。
"The function estimates and returns an initial camera intrinsic matrix for the camera calibration process. Currently, the function only supports planar calibration patterns, which are patterns where each object point has z-coordinate =0.",この関数は，カメラキャリブレーション処理のための，初期のカメラ固有マトリクスを推定して返します．現在のところ，この関数は平面的なキャリブレーションパターン（各オブジェクトポイントのz座標が0であるパターン）のみをサポートしています．
Finds the positions of internal corners of the chessboard.,チェスボードの内側の角の位置を求めます．
image : Source chessboard view. It must be an 8-bit grayscale or color image.,image : ソースとなるチェスボードのビュー．8ビットのグレースケールまたはカラー画像である必要があります。
"patternSize : Number of inner corners per a chessboard row and column ( patternSize = cv::Size(points_per_row,points_per_colum) = cv::Size(columns,rows) ).","patternSize : チェスボードの行と列毎に存在する内側コーナーの数 ( patternSize = cv::Size(points_per_row,points_per_colum) = cv::Size(columns,rows) ) ．"
corners : Output array of detected corners.,corners : 検出されたコーナーの出力配列．
flags : Various operation flags that can be zero or a combination of the following values:,flags :0または以下の値の組み合わせになる様々な操作フラグ．
"CALIB_CB_ADAPTIVE_THRESH Use adaptive thresholding to convert the image to black and white, rather than a fixed threshold level (computed from the average image brightness).",CALIB_CB_ADAPTIVE_THRESH （画像の平均輝度から計算される）固定の閾値ではなく，適応型閾値を用いて画像を白黒に変換します．
CALIB_CB_NORMALIZE_IMAGE Normalize the image gamma with equalizeHist before applying fixed or adaptive thresholding.,CALIB_CB_NORMALIZE_IMAGE 固定または適応型しきい値を適用する前に equalizeHist で画像のガンマを正規化します。
"CALIB_CB_FILTER_QUADS Use additional criteria (like contour area, perimeter, square-like shape) to filter out false quads extracted at the contour retrieval stage.",CALIB_CB_FILTER_QUADS 輪郭検索段階で抽出された偽の四角形をフィルタリングするために、追加の基準（輪郭の面積、周囲、四角形のような形など）を使用します。
"CALIB_CB_FAST_CHECK Run a fast check on the image that looks for chessboard corners, and shortcut the call if none is found. This can drastically speed up the call in the degenerate condition when no chessboard is observed.",CALIB_CB_FAST_CHECK チェスボードの角を探す高速チェックを画像上で実行し、角が見つからない場合は呼び出しをショートカットします。これにより、チェスボードが観察されない退化した状態での呼び出しが大幅に高速化されます。
"The function attempts to determine whether the input image is a view of the chessboard pattern and locate the internal chessboard corners. The function returns a non-zero value if all of the corners are found and they are placed in a certain order (row by row, left to right in every row). Otherwise, if the function fails to find all the corners or reorder them, it returns 0. For example, a regular chessboard has 8 x 8 squares and 7 x 7 internal corners, that is, points where the black squares touch each other. The detected coordinates are approximate, and to determine their positions more accurately, the function calls cornerSubPix. You also may use the function cornerSubPix with different parameters if returned coordinates are not accurate enough.Sample usage of detecting and drawing chessboard corners: :Size patternsize(8,6); //interior number of cornersMat gray = ....; //source imagevector<Point2f> corners; //this will be filled by the detected corners//CALIB_CB_FAST_CHECK saves a lot of time on images//that do not contain any chessboard cornersbool patternfound = findChessboardCorners(gray, patternsize, corners,        CALIB_CB_ADAPTIVE_THRESH + CALIB_CB_NORMALIZE_IMAGE        + CALIB_CB_FAST_CHECK);if(patternfound)  cornerSubPix(gray, corners, Size(11, 11), Size(-1, -1),    TermCriteria(CV_TERMCRIT_EPS + CV_TERMCRIT_ITER, 30, 0.1));drawChessboardCorners(img, patternsize, Mat(corners), patternfound);fragmentNoteThe function requires white space (like a square-thick border, the wider the better) around the board to make the detection more robust in various environments. Otherwise, if there is no border and the background is dark, the outer black squares cannot be segmented properly and so the square grouping and ordering algorithm fails.Examples: samples/cpp/tutorial_code/features2D/Homography/decompose_homography.cpp, samples/cpp/tutorial_code/features2D/Homography/homography_from_camera_displacement.cpp, and samples/cpp/tutorial_code/features2D/Homography/pose_from_homography.cpp.","この関数は，入力画像がチェスボードパターンを表しているかどうかを判定し，内部のチェスボードコーナーの位置を特定しようとします．この関数は，すべてのコーナーが見つかり，それらが一定の順序（行ごとに，左から右へ）で配置されていれば，0ではない値を返します．そうでない場合，この関数がすべてのコーナーを見つけることができなかったり，並べ替えることができなかったりすると，0を返します．例えば，通常のチェスボードは，8×8の正方形と7×7の内部コーナー，つまり，黒い正方形が互いに接触するポイントを持っています．検出された座標は近似値であり，その位置をより正確に決定するために，この関数は cornerSubPix を呼び出します．チェスボードの角を検出して描画するサンプルの使い方： :Size patternsize(8,6); //内部の角の数Mat gray = ....; //ソース imageevector<Point2f> corners; //これは，検出されたコーナーで埋められます/CALIB_CB_FAST_CHECK は，チェスボードのコーナーを含まない画像//に対して，多くの時間を節約しますbool patternfound = findChessboardCorners(gray, patternsize, corners, CALIB_CB_ADAPTIVE_THRESH + CALIB_CB_NORMALIZE_IMAGE + CALIB_CB_FAST_CHECK);if(patternfound) cornerSubPix(gray, corners, Size(11, 11), Size(-1, -1), TermCriteria(CV_TERMCRIT_EPS + CV_TERMCRIT_ITER, 30, 0.1));drawChessboardCorners(img, patternsize, Mat(corners), patternfound);fragmentNoteこの関数は，さまざまな環境下での検出をより強固なものにするために，チェスボードの周囲にホワイトスペース（正方形の厚さの境界線のようなもの，広ければ広いほどよい）を必要とします．例: samples/cpp/tutorial_code/features2D/Homography/decompose_homography.cpp, samples/cpp/tutorial_code/features2D/Homography/homography_from_camera_displacement.cpp, samples/cpp/tutorial_code/features2D/Homography/pose_from_homography.cpp."
Finds the positions of internal corners of the chessboard using a sector based approach.,チェスボードの内側の角の位置を、セクターベースのアプローチで求めます。
CALIB_CB_NORMALIZE_IMAGE Normalize the image gamma with equalizeHist before detection.,CALIB_CB_NORMALIZE_IMAGE 検出前に equalizeHist で画像のガンマを正規化します。
CALIB_CB_EXHAUSTIVE Run an exhaustive search to improve detection rate.,CALIB_CB_EXHAUSTIVE 検出率を向上させるために、網羅的な検索を実行します。
CALIB_CB_ACCURACY Up sample input image to improve sub-pixel accuracy due to aliasing effects.,CALIB_CB_ACCURACY エイリアシング効果によるサブピクセルの精度を向上させるために、入力画像をアップサンプルします。
CALIB_CB_LARGER The detected pattern is allowed to be larger than patternSize (see description).,CALIB_CB_LARGER 検出されたパターンは patternSize (説明を参照) よりも大きいことが許されます。
CALIB_CB_MARKER The detected pattern must have a marker (see description). This should be used if an accurate camera calibration is required.,CALIB_CB_MARKER 検出されたパターンはマーカー（説明を参照）を持たなければなりません。これは、正確なカメラのキャリブレーションが必要な場合に使用されます。
"meta : Optional output arrray of detected corners (CV_8UC1 and size = cv::Size(columns,rows)). Each entry stands for one corner of the pattern and can have one of the following values:","meta :オプションである，検出されたコーナーの出力配列（CV_8UC1 および size = cv::Size(columns,rows) ）．各エントリは，パターンの1つのコーナーを表し，以下のいずれかの値をとります．"
0 = no meta data attached,0 = メタデータが付加されていない
1 = left-top corner of a black cell,1 = 黒のセルの左上隅
2 = left-top corner of a white cell,2 = 白のセルの左上隅
3 = left-top corner of a black cell with a white marker dot,3 = 白のマーカードットが付いた黒のセルの左上隅
4 = left-top corner of a white cell with a black marker dot (pattern origin in case of markers otherwise first corner),4 = 黒のマーカードットがある白セルの左上隅（第1コーナー以外にマーカーがある場合は、パターンの原点）。
"The function is analog to findChessboardCorners but uses a localized radon transformation approximated by box filters being more robust to all sort of noise, faster on larger images and is able to directly return the sub-pixel position of the internal chessboard corners. The Method is based on the paper [61] ""Accurate Detection and Localization of Checkerboard Corners for",この関数は，findChessboardCorners に類似していますが，ボックスフィルタで近似された局所的なラドン変換を利用しており，あらゆる種類のノイズに対してより頑健で，大きな画像に対してより高速であり，チェスボード内部のコーナーのサブピクセルの位置を直接返すことができます．この手法は、論文[61]「Accurate Detection and Localization of Checkerboard Corners for Calibration」に基づいています。
"Calibration"" demonstrating that the returned sub-pixel positions are more accurate than the one returned by cornerSubPix allowing a precise camera calibration for demanding applications.In the case, the flags CALIB_CB_LARGER or CALIB_CB_MARKER are given, the result can be recovered from the optional meta array. Both flags are helpful to use calibration patterns exceeding the field of view of the camera. These oversized patterns allow more accurate calibrations as corners can be utilized, which are as close as possible to the image borders. For a consistent coordinate system across all images, the optional marker (see image below) can be used to move the origin of the board to the location where the black circle is located.NoteThe function requires a white boarder with roughly the same width as one of the checkerboard fields around the whole board to improve the detection in various environments. In addition, because of the localized radon transformation it is beneficial to use round corners for the field corners which are located on the outside of the board. The following figure illustrates a sample checkerboard optimized for the detection. However, any other checkerboard can be used as well. ","この方法は，論文 [61] ""Accurate Detection and Localization of Checkerboard Corners for Calibration"" に基づいており，返されるサブピクセル位置は cornerSubPix で返されるものよりも正確で，要求の厳しいアプリケーションに対して正確なカメラキャリブレーションを可能にすることを示しています．これらのフラグは、カメラの視野を超えるキャリブレーションパターンを使用する際に役立ちます。このような大規模なパターンでは、画像の境界にできるだけ近いコーナーを利用できるため、より正確なキャリブレーションが可能になります。この機能では、さまざまな環境下での検出を向上させるために、ボード全体の周囲にチェッカーボードの1つのフィールドとほぼ同じ幅の白いボーダーを設ける必要があります。また、局部的なラドン変換のため、ボードの外側に位置するフィールドのコーナーには丸い角を使用することが有効です。次の図は、検出に最適なチェッカーボードの例です。ただし、他のチェッカーボードを使用することも可能です。"
Checkerboard,チェッカーボード
finds subpixel-accurate positions of the chessboard corners,チェスボードのコーナーをサブピクセル単位の精度で検出
Renders the detected chessboard corners.,検出されたチェスボードの角をレンダリングします。
image : Destination image. It must be an 8-bit color image.,image : 出力画像．8ビットカラー画像である必要があります．
"patternSize : Number of inner corners per a chessboard row and column (patternSize = cv::Size(points_per_row,points_per_column)).","patternSize : チェスボードの行と列毎の内側コーナーの数 (patternSize = cv::Size(points_per_row,points_per_column))．"
"corners : Array of detected corners, the output of findChessboardCorners.",corners : 検出されたコーナーの配列，findChessboardCorners の出力．
patternWasFound : Parameter indicating whether the complete board was found or not. The return value of findChessboardCorners should be passed here.,patternWasFound : 完全なボードが見つかったかどうかを示すパラメータ．ここには，findChessboardCornersの戻り値を渡す必要があります．
"The function draws individual chessboard corners detected either as red circles if the board was not found, or as colored corners connected with lines if the board was found.Examples: samples/cpp/tutorial_code/features2D/Homography/pose_from_homography.cpp.",この関数は、検出された個々のチェスボードの角を、ボードが見つからなかった場合は赤丸で、ボードが見つかった場合は色付きの角を線で結んで描画します。例： samples/cpp/tutorial_code/features2D/Homography/pose_from_homography.cpp.
Draw axes of the world/object coordinate system from pose estimation.,ポーズ推定からワールド/オブジェクト座標系の軸を描画します。
image : Input/output image. It must have 1 or 3 channels. The number of channels is not altered.,image : 入力/出力画像．1チャンネルまたは3チャンネルを持つ必要があります．チャンネル数の変更はできません．
cameraMatrix : Input 3x3 floating-point matrix of camera intrinsic parameters. \(\cameramatrix{A}\),cameraMatrix :カメラの固有パラメータを表す，3×3の浮動小数点型行列を入力します．\maelstromatrix{A}\
"distCoeffs : Input vector of distortion coefficients \(\distcoeffs\). If the vector is empty, the zero distortion coefficients are assumed.",distCoeffs : ディストーション係数を表すベクトルを入力します．ベクトルが空の場合は、歪み係数が0であると仮定する。
"rvec : Rotation vector (see Rodrigues ) that, together with tvec, brings points from the model coordinate system to the camera coordinate system.",rvec :tvecと一緒にモデル座標系からカメラ座標系へと点を移動させる回転ベクトル（Rodrigues参照）。
tvec : Translation vector.,tvec :並進ベクトル
length : Length of the painted axes in the same unit than tvec (usually in meters).,length : tvecと同じ単位（通常はメートル単位）で描かれた軸の長さ
thickness : Line thickness of the painted axes.,thickness : 描画される軸の線の太さ
"See alsosolvePnPThis function draws the axes of the world/object coordinate system w.r.t. to the camera frame. OX is drawn in red, OY in green and OZ in blue.Examples: samples/cpp/tutorial_code/features2D/Homography/homography_from_camera_displacement.cpp, and samples/cpp/tutorial_code/features2D/Homography/pose_from_homography.cpp.",別項参照sosolvePnPこの関数は、カメラフレームに対するワールド／オブジェクト座標系の軸を描画します。例：samples/cpp/tutorial_code/features2D/Homography/homography_from_camera_displacement.cpp、samples/cpp/tutorial_code/features2D/Homography/pose_from_homography.cpp。
Finds centers in the grid of circles.,グリッド上の円の中心を見つけます。
image : grid view of input circles; it must be an 8-bit grayscale or color image.,image : 入力された円のグリッドビュー．8ビットのグレースケールまたはカラー画像でなければなりません．
"patternSize : number of circles per row and column ( patternSize = Size(points_per_row, points_per_colum) ).","patternSize : 1行，1列あたりの円の個数 ( patternSize = Size(points_per_row, points_per_colum) )."
centers : output array of detected centers.,centers : 検出された中心の出力配列．
flags : various operation flags that can be one of the following values:,flags : 以下の値のいずれかとなる様々な操作フラグ。
CALIB_CB_SYMMETRIC_GRID uses symmetric pattern of circles.,CALIB_CB_SYMMETRIC_GRID は、対称的な円のパターンを使用します。
CALIB_CB_ASYMMETRIC_GRID uses asymmetric pattern of circles.,CALIB_CB_ASYMMETRIC_GRID 非対称な円のパターンを使用します。
CALIB_CB_CLUSTERING uses a special algorithm for grid detection. It is more robust to perspective distortions but much more sensitive to background clutter.,CALIB_CB_CLUSTERING はグリッド検出に特別なアルゴリズムを使用します。これは視点の歪みに対してより頑健ですが、背景の乱雑さに対してはより敏感です。
blobDetector : feature detector that finds blobs like dark circles on light background. If blobDetector is NULL then image represents Point2f array of candidates.,blobDetector : 明るい背景上の暗い円のようなblobを検出する特徴検出器．blobDetectorがNULLの場合，画像は候補のPoint2f配列を表します．
parameters : struct for finding circles in a grid pattern.,parameters : グリッドパターンから円を見つけるための構造体．
"The function attempts to determine whether the input image contains a grid of circles. If it is, the function locates centers of the circles. The function returns a non-zero value if all of the centers have been found and they have been placed in a certain order (row by row, left to right in every row). Otherwise, if the function fails to find all the corners or reorder them, it returns 0.Sample usage of detecting and drawing the centers of circles: :Size patternsize(7,7); //number of centersMat gray = ...; //source imagevector<Point2f> centers; //this will be filled by the detected centersbool patternfound = findCirclesGrid(gray, patternsize, centers);drawChessboardCorners(img, patternsize, Mat(centers), patternfound);fragmentNoteThe function requires white space (like a square-thick border, the wider the better) around the board to make the detection more robust in various environments.","この関数は，入力画像に格子状の円が含まれているかどうかを調べます．この関数は，入力画像に格子状の円が含まれているかどうかを判定し，含まれている場合は，その円の中心を求めます．この関数は，すべての円の中心が見つかり，それらが一定の順序（行ごとに左から右へ）で配置されていれば，0以外の値を返します．そうでない場合，この関数がすべての角を見つけられなかったり，並べ替えることができなかったりすると，0を返します． 円の中心を検出して描画するサンプルの使い方:Size patternsize(7,7); //中心の数Mat gray = ...; //ソース imageevector<Point2f> centers; //これは，検出された中心で埋められますbool patternfound = findCirclesGrid(gray, patternsize, centers);drawChessboardCorners(img, patternsize, Mat(centres), patternfound);fragmentNoteこの関数は，様々な環境下で検出をより確実なものにするために，ボードの周囲にホワイトスペース（四角い厚さの境界線のようなもの，広ければ広いほど良い）を必要とします．"
Finds the camera intrinsic and extrinsic parameters from several views of a calibration pattern.,キャリブレーションパターンの複数のビューから、カメラの内在的および外在的パラメータを見つけます。
"objectPoints : In the new interface it is a vector of vectors of calibration pattern points in the calibration pattern coordinate space (e.g. std::vector<std::vector<cv::Vec3f>>). The outer vector contains as many elements as the number of pattern views. If the same calibration pattern is shown in each view and it is fully visible, all the vectors will be the same. Although, it is possible to use partially occluded patterns or even different patterns in different views. Then, the vectors will be different. Although the points are 3D, they all lie in the calibration pattern's XY coordinate plane (thus 0 in the Z-coordinate), if the used calibration pattern is a planar rig. In the old interface all the vectors of object points from different views are concatenated together.",objectPoints :新しいインターフェースでは，キャリブレーションパターンの座標空間におけるキャリブレーションパターンポイントのベクトル（std::vector<std::vector<cv::Vec3f>など）である．外側のベクトルには、パターンビューの数だけの要素が含まれる。各ビューに同じキャリブレーションパターンが表示され、それが完全に見える場合、すべてのベクターは同じになります。しかし、部分的にオックルされたパターンや、異なるビューに異なるパターンを使用することも可能です。そうすると、ベクターは異なります。ポイントは3Dですが、使用するキャリブレーションパターンが平面リグの場合、ポイントはすべてキャリブレーションパターンのXY座標平面上に位置します（つまりZ座標は0）。従来のインターフェースでは、異なるビューからのオブジェクトポイントのすべてのベクトルが連結されていました。
"imagePoints : In the new interface it is a vector of vectors of the projections of calibration pattern points (e.g. std::vector<std::vector<cv::Vec2f>>). imagePoints.size() and objectPoints.size(), and imagePoints[i].size() and objectPoints[i].size() for each i, must be equal, respectively. In the old interface all the vectors of object points from different views are concatenated together.",imagePoints :imagePoints.size() と objectPoints.size() ，そして各 i に対する imagePoints[i].size() と objectPoints[i].size() は，それぞれ等しくなければいけません．古いインターフェースでは，異なるビューからのオブジェクトポイントのすべてのベクトルが連結されています．
imageSize : Size of the image used only to initialize the camera intrinsic matrix.,imageSize : カメラ固有の行列を初期化するためにのみ利用される画像のサイズ．
"cameraMatrix : Input/output 3x3 floating-point camera intrinsic matrix \(\cameramatrix{A}\) . If CALIB_USE_INTRINSIC_GUESS and/or CALIB_FIX_ASPECT_RATIO, CALIB_FIX_PRINCIPAL_POINT or CALIB_FIX_FOCAL_LENGTH are specified, some or all of fx, fy, cx, cy must be initialized before calling the function.","cameraMatrix :入出力 3x3 浮動小数点型カメラ内部行列 ˶ˆ꒳ˆ˵ )CALIB_USE_INTRINSIC_GUESS, CALIB_FIX_ASPECT_RATIO, CALIB_FIX_PRINCIPAL_POINT, CALIB_FIX_FOCAL_LENGTH のいずれかが指定されている場合，関数を呼び出す前に fx, fy, cx, cy の一部または全部を初期化する必要があります．"
distCoeffs : Input/output vector of distortion coefficients \(\distcoeffs\).,distCoeffs : ディストーション係数の入出力ベクトル．
"rvecs : Output vector of rotation vectors (Rodrigues ) estimated for each pattern view (e.g. std::vector<cv::Mat>>). That is, each i-th rotation vector together with the corresponding i-th translation vector (see the next output parameter description) brings the calibration pattern from the object coordinate space (in which object points are specified) to the camera coordinate space. In more technical terms, the tuple of the i-th rotation and translation vector performs a change of basis from object coordinate space to camera coordinate space. Due to its duality, this tuple is equivalent to the position of the calibration pattern with respect to the camera coordinate space.",rvecs :各パターンビューに対して推定された回転ベクトル（Rodrigues ）の出力ベクトル（例：std::vector<cv::Mat>>）．つまり，各 i 番目の回転ベクトルは，対応する i 番目の並進ベクトル（次の出力パラメータの説明を参照してください）と共に，キャリブレーションパターンを（オブジェクトポイントが指定されている）オブジェクト座標空間からカメラ座標空間へと導きます．より専門的に言えば，i番目の回転ベクトルと並進ベクトルのタプルは，物体座標空間からカメラ座標空間への基底の変更を行うものである．このタプルは、その双対性により、カメラ座標空間に対するキャリブレーションパターンの位置に相当します。
"tvecs : Output vector of translation vectors estimated for each pattern view, see parameter describtion above.",tvecs :各パターンビューごとに推定された並進ベクトルの出力ベクトル。上記パラメータの説明を参照してください。
"stdDeviationsIntrinsics : Output vector of standard deviations estimated for intrinsic parameters. Order of deviations values: \((f_x, f_y, c_x, c_y, k_1, k_2, p_1, p_2, k_3, k_4, k_5, k_6 , s_1, s_2, s_3, s_4, \tau_x, \tau_y)\) If one of parameters is not estimated, it's deviation is equals to zero.","stdDeviationsIntrinsics : 固有のパラメータに対して推定された標準偏差の出力ベクトル。偏差値の順番。\(f_x, f_y, c_x, c_y, k_1, k_2, p_1, p_2, k_3, k_4, k_5, k_6 , s_1, s_2, s_3, s_4, ˶ᵔᵕᵔ˶)パラメータの1つが推定されていない場合、その偏差はゼロに等しい。"
"stdDeviationsExtrinsics : Output vector of standard deviations estimated for extrinsic parameters. Order of deviations values: \((R_0, T_0, \dotsc , R_{M - 1}, T_{M - 1})\) where M is the number of pattern views. \(R_i, T_i\) are concatenated 1x3 vectors.","stdDeviationsExtrinsics : 外部パラメータの推定標準偏差の出力ベクトル。偏差値の順番\M はパターンビューの数を表す。\R_i, T_i\）は、1x3のベクトルを連結したものである。"
perViewErrors : Output vector of the RMS re-projection error estimated for each pattern view.,perViewErrors :パターンビューごとに推定されたRMS再投影誤差の出力ベクトル．
flags : Different flags that may be zero or a combination of the following values:,flags :0または以下の値の組み合わせである異なるフラグ。
"CALIB_USE_INTRINSIC_GUESS cameraMatrix contains valid initial values of fx, fy, cx, cy that are optimized further. Otherwise, (cx, cy) is initially set to the image center ( imageSize is used), and focal distances are computed in a least-squares fashion. Note, that if intrinsic parameters are known, there is no need to use this function just to estimate extrinsic parameters. Use solvePnP instead.","CALIB_USE_INTRINSIC_GUESS cameraMatrix が，さらに最適化された有効な初期値 fx, fy, cx, cy を含む場合．それ以外の場合は，(cx, cy) が画像の中心に初期設定され（imageSize が使用される），最小二乗法で焦点距離が計算されます．なお，内部パラメータが既知の場合は，外部パラメータを推定するためだけにこの関数を利用する必要はありません．代わりに solvePnP を使用してください。"
CALIB_FIX_PRINCIPAL_POINT The principal point is not changed during the global optimization. It stays at the center or at a different location specified when CALIB_USE_INTRINSIC_GUESS is set too.,CALIB_FIX_PRINCIPAL_POINT 主点は大域的最適化の間、変更されません。中心に留まるか、CALIB_USE_INTRINSIC_GUESSが設定されているときに指定された別の場所に留まります。
"CALIB_FIX_ASPECT_RATIO The functions consider only fy as a free parameter. The ratio fx/fy stays the same as in the input cameraMatrix . When CALIB_USE_INTRINSIC_GUESS is not set, the actual input values of fx and fy are ignored, only their ratio is computed and used further.",CALIB_FIX_ASPECT_RATIO この関数は自由パラメータとして fy のみを考慮します。比 fx/fy は，入力 cameraMatrix と同じになります．CALIB_USE_INTRINSIC_GUESS が設定されていない場合，fx とfy の実際の入力値は無視され，それらの比のみが計算され，さらに利用されます。
"CALIB_ZERO_TANGENT_DIST Tangential distortion coefficients \((p_1, p_2)\) are set to zeros and stay zero.","CALIB_ZERO_TANGENT_DIST タンジェンシャルディストーション係数 ˶((p_1, p_2)˶)は 0 に設定され、0 のままです。"
CALIB_FIX_FOCAL_LENGTH The focal length is not changed during the global optimization if CALIB_USE_INTRINSIC_GUESS is set.,CALIB_FIX_FOCAL_LENGTH CALIB_USE_INTRINSIC_GUESS が設定されている場合、グローバル最適化の際に焦点距離を変更しません。
"CALIB_FIX_K1,..., CALIB_FIX_K6 The corresponding radial distortion coefficient is not changed during the optimization. If CALIB_USE_INTRINSIC_GUESS is set, the coefficient from the supplied distCoeffs matrix is used. Otherwise, it is set to 0.","CALIB_FIX_K1,..., CALIB_FIX_K6 対応する半径方向の歪み係数は，最適化の際には変更されません。CALIB_USE_INTRINSIC_GUESS が設定されている場合，与えられた distCoeffs 行列からの係数が使用されます。それ以外の場合は、0に設定されます。"
"CALIB_RATIONAL_MODEL Coefficients k4, k5, and k6 are enabled. To provide the backward compatibility, this extra flag should be explicitly specified to make the calibration function use the rational model and return 8 coefficients or more.",CALIB_RATIONAL_MODEL 係数k4、k5、およびk6が有効です。下位互換性を提供するために、キャリブレーション関数が有理モデルを使用し、8個以上の係数を返すようにするには、この追加フラグを明示的に指定する必要がある。
"CALIB_THIN_PRISM_MODEL Coefficients s1, s2, s3 and s4 are enabled. To provide the backward compatibility, this extra flag should be explicitly specified to make the calibration function use the thin prism model and return 12 coefficients or more.",CALIB_THIN_PRISM_MODEL 係数s1、s2、s3、s4が有効になる。下位互換性を提供するために、校正関数が薄いプリズム・モデルを使用し、12個以上の係数を返すようにするには、この追加フラグを明示的に指定する必要があります。
"CALIB_FIX_S1_S2_S3_S4 The thin prism distortion coefficients are not changed during the optimization. If CALIB_USE_INTRINSIC_GUESS is set, the coefficient from the supplied distCoeffs matrix is used. Otherwise, it is set to 0.",CALIB_FIX_S1_S2_S3_S4 薄いプリズムの歪み係数は最適化中には変更されません。CALIB_USE_INTRINSIC_GUESS が設定されている場合、与えられた distCoeffs 行列からの係数が使用されます。それ以外の場合は、0に設定されます。
"CALIB_TILTED_MODEL Coefficients tauX and tauY are enabled. To provide the backward compatibility, this extra flag should be explicitly specified to make the calibration function use the tilted sensor model and return 14 coefficients.",CALIB_TILTED_MODEL 係数 tauX および tauY が有効です。下位互換性を提供するために，キャリブレーション関数が傾いたセンサモデルを使用し，14の係数を返すようにするには，この追加フラグを明示的に指定する必要がある．
"CALIB_FIX_TAUX_TAUY The coefficients of the tilted sensor model are not changed during the optimization. If CALIB_USE_INTRINSIC_GUESS is set, the coefficient from the supplied distCoeffs matrix is used. Otherwise, it is set to 0.",CALIB_FIX_TAUX_TAUY 傾いたセンサモデルの係数は最適化中には変更されない。CALIB_USE_INTRINSIC_GUESS が設定されている場合は、与えられた distCoeffs 行列からの係数が使用されます。それ以外の場合は，0に設定される。
criteria : Termination criteria for the iterative optimization algorithm.,criteria :反復最適化アルゴリズムの終了基準．
"The function estimates the intrinsic camera parameters and extrinsic parameters for each of the views. The algorithm is based on [293] and [31] . The coordinates of 3D object points and their corresponding 2D projections in each view must be specified. That may be achieved by using an object with known geometry and easily detectable feature points. Such an object is called a calibration rig or calibration pattern, and OpenCV has built-in support for a chessboard as a calibration rig (see findChessboardCorners). Currently, initialization of intrinsic parameters (when CALIB_USE_INTRINSIC_GUESS is not set) is only implemented for planar calibration patterns (where Z-coordinates of the object points must be all zeros). 3D calibration rigs can also be used as long as initial cameraMatrix is provided.The algorithm performs the following steps:Compute the initial intrinsic parameters (the option only available for planar calibration patterns) or read them from the input parameters. The distortion coefficients are all set to zeros initially unless some of CALIB_FIX_K? are specified.",この関数は，カメラの内部パラメータと，各ビューの外部パラメータを推定します．このアルゴリズムは， [293] と [31] に基づいています．各ビューにおける3次元オブジェクトポイントの座標と，それに対応する2次元射影を指定する必要があります．これは、既知のジオメトリと簡単に検出できる特徴点を持つオブジェクトを使用することで実現できます。このようなオブジェクトは，キャリブレーションリグやキャリブレーションパターンと呼ばれます．OpenCVは，キャリブレーションリグとしてチェスボードをビルトインでサポートしています（findChessboardCornersを参照してください）．現在のところ，内部パラメータの初期化（CALIB_USE_INTRINSIC_GUESS がセットされていない場合）は，平面的なキャリブレーションパターン（オブジェクトポイントのZ座標はすべて0でなければいけない）に対してのみ実装されています．アルゴリズムは以下のステップを実行します：初期固有パラメータを計算する（平面キャリブレーションパターンでのみ利用可能なオプション）、または入力パラメータから読み取る。CALIB_FIX_K? の一部が指定されていない限り、歪み係数はすべてゼロに初期設定されます。
Estimate the initial camera pose as if the intrinsic parameters have been already known. This is done using solvePnP .,固有のパラメータが既に知られているかのように、初期のカメラポーズを推定する。これは solvePnP を用いて行われる。
"Run the global Levenberg-Marquardt optimization algorithm to minimize the reprojection error, that is, the total sum of squared distances between the observed feature points imagePoints and the projected (using the current estimates for camera parameters and the poses) object points objectPoints. See projectPoints for details.NoteIf you use a non-square (i.e. non-N-by-N) grid and findChessboardCorners for calibration, and calibrateCamera returns bad values (zero distortion coefficients, \(c_x\) and \(c_y\) very far from the image center, and/or large differences between \(f_x\) and \(f_y\) (ratios of 10:1 or more)), then you are probably using patternSize=cvSize(rows,cols) instead of using patternSize=cvSize(cols,rows) in findChessboardCorners.See alsocalibrateCameraRO, findChessboardCorners, solvePnP, initCameraMatrix2D, stereoCalibrate, undistort","グローバル Levenberg-Marquardt 最適化アルゴリズムを実行して，再投影誤差，つまり，観測された特徴点 imagePoints と，（カメラパラメータとポーズの現在の推定値を用いて）投影されたオブジェクト点 objectPoints との間の二乗距離の総和を最小化します．詳細は projectPoints を参照してください。非正方(non-N-by-N)グリッドを使用し、findChessboardCorners でキャリブレーションを行った場合、 calibrateCamera が悪い値(歪み係数がゼロ、\(c_x\)と\(c_y\)が画像の中心から非常に離れている、\(f_x\)と┄(f_yansen)の差が大きい(比率が10:1以上））の場合は、findChessboardCornersでpatternSize=cvSize(cols,rows)を使うのではなく、patternSize=cvSize(rows,cols)を使っていると思われます。関連項目：ocalibrateCameraRO, findChessboardCorners, solvePnP, initCameraMatrix2D, stereoCalibrate, undistort"
Computes useful camera characteristics from the camera intrinsic matrix.,カメラ固有の行列から有用なカメラ特性を計算します。
cameraMatrix : Input camera intrinsic matrix that can be estimated by calibrateCamera or stereoCalibrate .,cameraMatrix :calibrateCamera または stereoCalibrate によって推定される，入力カメラ固有の行列．
imageSize : Input image size in pixels.,imageSize : ピクセル単位の入力画像サイズ．
apertureWidth : Physical width in mm of the sensor.,apertureWidth : センサーの物理的な幅（mm）．
apertureHeight : Physical height in mm of the sensor.,apertureHeight : センサーの物理的な高さ（mm）．
fovx : Output field of view in degrees along the horizontal sensor axis.,fovx : 水平方向のセンサー軸に沿った角度での出力視野。
fovy : Output field of view in degrees along the vertical sensor axis.,fovy : 垂直方向のセンサー軸に沿った角度での出力視野。
focalLength : Focal length of the lens in mm.,focalLength : レンズの焦点距離をmm単位で表したもの。
principalPoint : Principal point in mm.,principalPoint ：主点（単位：mm）。
aspectRatio : \(f_y/f_x\),aspectRatio :\f_y/f_x˶
The function computes various useful camera characteristics from the previously estimated camera matrix.NoteDo keep in mind that the unity measure 'mm' stands for whatever unit of measure one chooses for the chessboard pitch (it can thus be any value).,この関数は，事前に推定されたカメラ行列から，様々な有用なカメラ特性を計算します．注意点として，統一的な尺度である「mm」は，チェスボードのピッチを表す任意の単位を表していることに注意してください（したがって，任意の値になります）．
Calibrates a stereo camera set up. This function finds the intrinsic parameters for each of the two cameras and the extrinsic parameters between the two cameras.,ステレオカメラセットのキャリブレーションを行います。この関数は、2台のカメラそれぞれの固有パラメータと、2台のカメラ間の外部パラメータを求めます。
"objectPoints : Vector of vectors of the calibration pattern points. The same structure as in calibrateCamera. For each pattern view, both cameras need to see the same object points. Therefore, objectPoints.size(), imagePoints1.size(), and imagePoints2.size() need to be equal as well as objectPoints[i].size(), imagePoints1[i].size(), and imagePoints2[i].size() need to be equal for each i.","objectPoints :キャリブレーションパターンのポイントを示すベクトル．calibrateCameraと同じ構造です。各パターンビューにおいて、両方のカメラは同じオブジェクトポイントを見る必要があります。したがって，objectPoints.size(), imagePoints1.size(), imagePoints2.size() は等しくなければならず，また，objectPoints[i].size(), imagePoints1[i].size(), imagePoints2[i].size() は各 i に対して等しくなければならない．"
"imagePoints1 : Vector of vectors of the projections of the calibration pattern points, observed by the first camera. The same structure as in calibrateCamera.",imagePoints1 : 1台目のカメラで観測された，キャリブレーションパターン点の投影結果のベクトル．calibrateCamera の場合と同じ構造である．
"imagePoints2 : Vector of vectors of the projections of the calibration pattern points, observed by the second camera. The same structure as in calibrateCamera.",imagePoints2 : 2台目のカメラで観測されたキャリブレーションパターン点の投影像のベクトル．calibrateCameraと同じ構造である。
"cameraMatrix1 : Input/output camera intrinsic matrix for the first camera, the same as in calibrateCamera. Furthermore, for the stereo case, additional flags may be used, see below.",cameraMatrix1 : 1台目のカメラの入出力カメラ固有の行列， calibrateCamera と同じ構造．さらに，ステレオの場合には，以下のような追加フラグが用いられます．
"distCoeffs1 : Input/output vector of distortion coefficients, the same as in calibrateCamera.",distCoeffs1 : calibrateCamera と同様に，歪み係数の入出力ベクトル．
cameraMatrix2 : Input/output second camera intrinsic matrix for the second camera. See description for cameraMatrix1.,cameraMatrix2 : 入出力される2台目のカメラの内部行列．cameraMatrix1の説明を参照してください．
distCoeffs2 : Input/output lens distortion coefficients for the second camera. See description for distCoeffs1.,distCoeffs2 : 入力／出力される第2カメラ用レンズ歪み係数．distCoeffs1 の説明を参照してください．
imageSize : Size of the image used only to initialize the camera intrinsic matrices.,imageSize : カメラ固有の行列を初期化するためにのみ利用される画像のサイズ．
"R : Output rotation matrix. Together with the translation vector T, this matrix brings points given in the first camera's coordinate system to points in the second camera's coordinate system. In more technical terms, the tuple of R and T performs a change of basis from the first camera's coordinate system to the second camera's coordinate system. Due to its duality, this tuple is equivalent to the position of the first camera with respect to the second camera coordinate system.",R : 出力される回転行列．この行列は，並進ベクトル T と共に，1 台目のカメラの座標系で与えられた点を，2 台目のカメラの座標系で与えられた点に変換します．より専門的に言えば，RとTのタプルは，第1のカメラの座標系から第2のカメラの座標系への基底の変更を行う．このタプルは、その双対性により、第2のカメラの座標系に対する第1のカメラの位置に相当する。
"T : Output translation vector, see description above.",T : 出力される並進ベクトル．
E : Output essential matrix.,E : 出力される必須行列。
F : Output fundamental matrix.,F : 基本行列を出力する。
"CALIB_FIX_INTRINSIC Fix cameraMatrix? and distCoeffs? so that only R, T, E, and F matrices are estimated.","CALIB_FIX_INTRINSIC R, T, E, F の各行列のみが推定されるように、cameraMatrix?"
CALIB_USE_INTRINSIC_GUESS Optimize some or all of the intrinsic parameters according to the specified flags. Initial values are provided by the user.,CALIB_USE_INTRINSIC_GUESS 指定されたフラグにしたがって、一部または全部の固有パラメータを最適化する。初期値はユーザーが提供します。
CALIB_USE_EXTRINSIC_GUESS R and T contain valid initial values that are optimized further. Otherwise R and T are initialized to the median value of the pattern views (each dimension separately).,CALIB_USE_EXTRINSIC_GUESS R と T に有効な初期値が含まれ、さらに最適化される。それ以外の場合、RとTはパターン・ビューの中央値に初期化されます（各次元ごと）。
CALIB_FIX_PRINCIPAL_POINT Fix the principal points during the optimization.,CALIB_FIX_PRINCIPAL_POINT 最適化の際の主点を固定します。
CALIB_FIX_FOCAL_LENGTH Fix \(f^{(j)}_x\) and \(f^{(j)}_y\) .,CALIB_FIX_FOCAL_LENGTH \(f^{(j)}_x\)と\(f^{(j)}_y\)を固定します。
CALIB_FIX_ASPECT_RATIO Optimize \(f^{(j)}_y\) . Fix the ratio \(f^{(j)}_x/f^{(j)}_y\),CALIB_FIX_ASPECT_RATIO ² ² ² ² ² ² を最適化します。比を固定します。
CALIB_SAME_FOCAL_LENGTH Enforce \(f^{(0)}_x=f^{(1)}_x\) and \(f^{(0)}_y=f^{(1)}_y\) .,CALIB_SAME_FOCAL_LENGTH Enforce \(f^{(0)}_x=f^{(1)}_x\) and \(f^{(0)}_y=f^{(1)}_y\) .
CALIB_ZERO_TANGENT_DIST Set tangential distortion coefficients for each camera to zeros and fix there.,CALIB_ZERO_TANGENT_DIST 各カメラの接線方向の歪み係数をゼロに設定して固定します。
"CALIB_FIX_K1,..., CALIB_FIX_K6 Do not change the corresponding radial distortion coefficient during the optimization. If CALIB_USE_INTRINSIC_GUESS is set, the coefficient from the supplied distCoeffs matrix is used. Otherwise, it is set to 0.","CALIB_FIX_K1,..., CALIB_FIX_K6 最適化の間，対応する半径方向の歪み係数を変更しません。CALIB_USE_INTRINSIC_GUESS が設定されている場合，与えられた distCoeffs 行列からの係数が使われます。それ以外の場合は，0に設定されます。"
"CALIB_RATIONAL_MODEL Enable coefficients k4, k5, and k6. To provide the backward compatibility, this extra flag should be explicitly specified to make the calibration function use the rational model and return 8 coefficients. If the flag is not set, the function computes and returns only 5 distortion coefficients.","CALIB_RATIONAL_MODEL 係数 k4, k5, および k6 を有効にします。下位互換性を提供するために、キャリブレーション関数が有理モデルを使用し、8つの係数を返すようにするには、この追加フラグを明示的に指定する必要がある。このフラグが設定されていない場合、この関数は5つの歪み係数のみを計算して返します。"
"CALIB_THIN_PRISM_MODEL Coefficients s1, s2, s3 and s4 are enabled. To provide the backward compatibility, this extra flag should be explicitly specified to make the calibration function use the thin prism model and return 12 coefficients. If the flag is not set, the function computes and returns only 5 distortion coefficients.",CALIB_THIN_PRISM_MODEL 係数s1、s2、s3、s4が有効になります。下位互換性を提供するために、キャリブレーション関数が薄いプリズム・モデルを使用し、12個の係数を返すようにするために、この追加フラグを明示的に指定する必要があります。このフラグが設定されていない場合、この関数は5つの歪み係数のみを計算して返します。
"CALIB_TILTED_MODEL Coefficients tauX and tauY are enabled. To provide the backward compatibility, this extra flag should be explicitly specified to make the calibration function use the tilted sensor model and return 14 coefficients. If the flag is not set, the function computes and returns only 5 distortion coefficients.",CALIB_TILTED_MODEL 係数 tauX および tauY が有効になる。後方互換性を確保するために，キャリブレーション関数が傾いたセンサモデルを使用し，14個の係数を返すようにするには，この追加フラグを明示的に指定する必要がある．このフラグが設定されていない場合，この関数は5つの歪み係数のみを計算して返します．
"The function estimates the transformation between two cameras making a stereo pair. If one computes the poses of an object relative to the first camera and to the second camera, ( \(R_1\), \(T_1\) ) and ( \(R_2\), \(T_2\)), respectively, for a stereo camera where the relative position and orientation between the two cameras are fixed, then those poses definitely relate to each other. This means, if the relative position and orientation ( \(R\), \(T\)) of the two cameras is known, it is possible to compute ( \(R_2\), \(T_2\)) when ( \(R_1\), \(T_1\)) is given. This is what the described function does. It computes ( \(R\), \(T\)) such that:\[R_2=R R_1\]\[T_2=R T_1 + T.\]Therefore, one can compute the coordinate representation of a 3D point for the second camera's coordinate system when given the point's coordinate representation in the first camera's coordinate system:\[\begin{bmatrix} X_2 \\ Y_2 \\ Z_2 \\ 1 \end{bmatrix} = \begin{bmatrix} R & T \\ 0 & 1 \end{bmatrix} \begin{bmatrix} X_1 \\ Y_1 \\ Z_1 \\ 1 \end{bmatrix}.\]Optionally, it computes the essential matrix E:\[E= \vecthreethree{0}{-T_2}{T_1}{T_2}{0}{-T_0}{-T_1}{T_0}{0} R\]where \(T_i\) are components of the translation vector \(T\) : \(T=[T_0, T_1, T_2]^T\) . And the function can also compute the fundamental matrix F:\[F = cameraMatrix2^{-T}\cdot E \cdot cameraMatrix1^{-1}\]Besides the stereo-related information, the function can also perform a full calibration of each of the two cameras. However, due to the high dimensionality of the parameter space and noise in the input data, the function can diverge from the correct solution. If the intrinsic parameters can be estimated with high accuracy for each of the cameras individually (for example, using calibrateCamera ), you are recommended to do so and then pass CALIB_FIX_INTRINSIC flag to the function along with the computed intrinsic parameters. Otherwise, if all the parameters are estimated at once, it makes sense to restrict some parameters, for example, pass CALIB_SAME_FOCAL_LENGTH and CALIB_ZERO_TANGENT_DIST flags, which is usually a reasonable assumption.Similarly to calibrateCamera, the function minimizes the total re-projection error for all the points in all the available views from both cameras. The function returns the final value of the re-projection error.","この関数は，ステレオペアを構成する2つのカメラ間の変換を推定します．2つのカメラ間の相対的な位置と姿勢が固定されているステレオカメラにおいて，1番目のカメラと2番目のカメラに対する物体の姿勢をそれぞれ( ˶ˆ꒳ˆ˵ ), ( ˶ˆ꒳ˆ˵ ), ( ˶ˆ꒳ˆ˵ ) と計算すると，それらの姿勢は確実に互いに関連していることになります．つまり、2つのカメラの相対的な位置と向き（\(R\)、\(T\)）がわかっていれば、（\(R_1\)、\(T_1\)）が与えられたときに、（\(R_2\)、\(T_2amer)）を計算することができます。これが，この関数の役割です．R_2=R R_1\]T_2=R T_1 + T.\]従って、1台目のカメラの座標系での3次元点の座標表現が与えられたとき、2台目のカメラの座標系での3次元点の座標表現を計算することができます。♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪R＆T」は、0＆1で構成されています。\\\\Optionally, the essential matrix E:\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\R\(t=[t_0, t_1, t_2]^t\) .また，この関数は，基本行列Fを計算することもできます：\[F = cameraMatrix2^{-T}\cdot E ˶cdot cameraMatrix1^{-1}\]ステレオ関連の情報の他に，2台のカメラそれぞれのフルキャリブレーションを行うこともできます。しかし，パラメータ空間の次元が高く，入力データにノイズが含まれているため，この関数は正しい解から乖離する可能性があります．各カメラの固有パラメータを個別に高精度に推定できる場合（例えば，calibrateCamera を用いて）は，そうしてから，計算された固有パラメータとともに CALIB_FIX_INTRINSIC フラグを関数に渡すことをお勧めします．また，すべてのパラメータが一度に推定される場合は，いくつかのパラメータを制限することに意味があります．例えば，CALIB_SAME_FOCAL_LENGTH フラグと CALIB_ZERO_TANGENT_DIST フラグを渡しますが，これは通常，妥当な仮定です． calibrateCamera と同様に，この関数は，両方のカメラから得られるすべての利用可能なビューのすべてのポイントに対して，合計再投影誤差を最小化します．この関数は，再投影誤差の最終的な値を返します．"
Computes rectification transforms for each head of a calibrated stereo camera.,キャリブレーションされたステレオカメラの各ヘッドに対して，平行化変換を計算します．
cameraMatrix1 : First camera intrinsic matrix.,cameraMatrix1 : 1台目のカメラの内部行列．
distCoeffs1 : First camera distortion parameters.,distCoeffs1 : 1番目のカメラの歪みパラメータ．
cameraMatrix2 : Second camera intrinsic matrix.,cameraMatrix2 : 2 番目のカメラの内部行列．
distCoeffs2 : Second camera distortion parameters.,distCoeffs2 : 第2カメラの歪みパラメータ．
imageSize : Size of the image used for stereo calibration.,imageSize : ステレオキャリブレーションに利用される画像のサイズ．
"R : Rotation matrix from the coordinate system of the first camera to the second camera, see stereoCalibrate.",R : 第一カメラの座標系から第二カメラへの回転行列（stereoCalibrate 参照）．
"T : Translation vector from the coordinate system of the first camera to the second camera, see stereoCalibrate.",T : 1 台目のカメラの座標系から 2 台目のカメラへの並進ベクトル（stereoCalibrate 参照）。
"R1 : Output 3x3 rectification transform (rotation matrix) for the first camera. This matrix brings points given in the unrectified first camera's coordinate system to points in the rectified first camera's coordinate system. In more technical terms, it performs a change of basis from the unrectified first camera's coordinate system to the rectified first camera's coordinate system.",R1 : 1台目のカメラに対する3x3の平行化変換（回転行列）の出力．この行列は，補正されていない第一カメラの座標系で与えられた点を，補正された第一カメラの座標系で与えられた点に変換します．専門的に言えば、補正されていない第一カメラの座標系から、補正された第一カメラの座標系への基底変更を行うものです。
"R2 : Output 3x3 rectification transform (rotation matrix) for the second camera. This matrix brings points given in the unrectified second camera's coordinate system to points in the rectified second camera's coordinate system. In more technical terms, it performs a change of basis from the unrectified second camera's coordinate system to the rectified second camera's coordinate system.",R2 : 2台目のカメラに対する3×3の平行化変換（回転行列）を出力．この行列は、補正されていない2台目のカメラの座標系で与えられた点を、補正された2台目のカメラの座標系の点にするものです。専門的に言えば、補正されていない2台目のカメラの座標系から、補正された2台目のカメラの座標系への基底変更を行います。
"P1 : Output 3x4 projection matrix in the new (rectified) coordinate systems for the first camera, i.e. it projects points given in the rectified first camera coordinate system into the rectified first camera's image.",P1 : 1台目のカメラの新しい（平行化された）座標系に3x4の射影行列を出力。つまり、平行化された1台目のカメラの座標系で与えられた点を、平行化された1台目のカメラの画像に投影します。
"P2 : Output 3x4 projection matrix in the new (rectified) coordinate systems for the second camera, i.e. it projects points given in the rectified first camera coordinate system into the rectified second camera's image.",P2 : 2台目のカメラの新しい（平行化された）座標系に3×4の射影行列を出力。つまり、平行化された1台目のカメラの座標系で与えられた点を、平行化された2台目のカメラの画像に射影する。
Q : Output \(4 \times 4\) disparity-to-depth mapping matrix (see reprojectImageTo3D).,Q : 視差-深度マッピング行列（reprojectImageTo3D 参照）を出力．
"flags : Operation flags that may be zero or CALIB_ZERO_DISPARITY . If the flag is set, the function makes the principal points of each camera have the same pixel coordinates in the rectified views. And if the flag is not set, the function may still shift the images in the horizontal or vertical direction (depending on the orientation of epipolar lines) to maximize the useful image area.",flags :0または CALIB_ZERO_DISPARITY である操作フラグ．このフラグがセットされている場合，この関数は，各カメラの主点が，平行化されたビューにおいて同じピクセル座標になるようにします．また，このフラグがセットされていない場合でも，有用な画像領域を最大化するために，（エピポーララインの向きに応じて）水平または垂直方向に画像を移動させることができます．
"alpha : Free scaling parameter. If it is -1 or absent, the function performs the default scaling. Otherwise, the parameter should be between 0 and 1. alpha=0 means that the rectified images are zoomed and shifted so that only valid pixels are visible (no black areas after rectification). alpha=1 means that the rectified image is decimated and shifted so that all the pixels from the original images from the cameras are retained in the rectified images (no source image pixels are lost). Any intermediate value yields an intermediate result between those two extreme cases.",alpha : フリースケーリングパラメータ．このフラグが-1または存在しない場合，この関数はデフォルトのスケーリングを行います．alpha=0 は，有効なピクセルのみが見えるように，平行化された画像がズームおよびシフトされることを意味します（平行化後に黒い部分がない）． alpha=1 は，カメラからのオリジナル画像のすべてのピクセルが平行化された画像に保持されるように，平行化された画像がデシメーションおよびシフトされることを意味します（ソース画像のピクセルは失われません）．中間的な値を設定すると、これら2つの極端なケースの中間的な結果が得られます。
"newImageSize : New image resolution after rectification. The same size should be passed to initUndistortRectifyMap (see the stereo_calib.cpp sample in OpenCV samples directory). When (0,0) is passed (default), it is set to the original imageSize . Setting it to a larger value can help you preserve details in the original image, especially when there is a big radial distortion.","newImageSize : 整形後の新しい画像の解像度．同じサイズを initUndistortRectifyMap に渡す必要があります（OpenCV samples ディレクトリにある stereo_calib.cpp サンプルを参照してください）．(0,0) が渡された場合（デフォルト），これは元の imageSize に設定されます．より大きな値を設定することで，特に大きな放射状の歪みがある場合に，元の画像の詳細を保持することができます．"
"validPixROI1 : Optional output rectangles inside the rectified images where all the pixels are valid. If alpha=0 , the ROIs cover the whole images. Otherwise, they are likely to be smaller (see the picture below).",validPixROI1 : 整形された画像の中で，すべてのピクセルが有効な長方形を出力するオプション．alpha=0の場合，このROIは画像全体をカバーします．そうでない場合は，小さくなる可能性があります（下図参照）．
"validPixROI2 : Optional output rectangles inside the rectified images where all the pixels are valid. If alpha=0 , the ROIs cover the whole images. Otherwise, they are likely to be smaller (see the picture below).",validPixROI2 : オプションで，矩形化された画像の中の，すべてのピクセルが有効な領域を出力します．alpha=0の場合，このROIは画像全体をカバーします．そうでない場合は，より小さな領域となります（下図を参照してください）．
"The function computes the rotation matrices for each camera that (virtually) make both camera image planes the same plane. Consequently, this makes all the epipolar lines parallel and thus simplifies the dense stereo correspondence problem. The function takes the matrices computed by stereoCalibrate as input. As output, it provides two rotation matrices and also two projection matrices in the new coordinates. The function distinguishes the following two cases:Horizontal stereo: the first and the second camera views are shifted relative to each other mainly along the x-axis (with possible small vertical shift). In the rectified images, the corresponding epipolar lines in the left and right cameras are horizontal and have the same y-coordinate. P1 and P2 look like:",この関数は，各カメラの回転行列を計算し，両カメラの画像平面を（仮想的に）同一平面にします．その結果，すべてのエピポーラ線が平行になり，密なステレオ対応関係の問題が簡単になります．この関数は， stereoCalibrate によって計算された行列を入力として受け取ります．出力として，2つの回転行列と，新しい座標への2つの投影行列が得られます．この関数は，以下の2つのケースを区別します：水平ステレオ：1番目と2番目のカメラのビューが，主にx軸に沿って相対的にシフトします（わずかな垂直方向のシフトもあり得ます）．整形された画像では，左右のカメラの対応するエピポーラ線は水平で，同じy座標を持ちます．P1とP2は次のようになります。
\[\texttt{P1} = \begin{bmatrix} f & 0 & cx_1 & 0 \\ 0 & f & cy & 0 \\ 0 & 0 & 1 & 0 \end{bmatrix}\],\P1} = ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ_ˆÇÇ
"\[\texttt{P2} = \begin{bmatrix} f & 0 & cx_2 & T_x*f \\ 0 & f & cy & 0 \\ 0 & 0 & 1 & 0 \end{bmatrix} ,\]","\\\\,\]"
where \(T_x\) is a horizontal shift between the cameras and \(cx_1=cx_2\) if CALIB_ZERO_DISPARITY is set.,ここで、T_x\はカメラ間の水平方向の移動量、CALIB_ZERO_DISPARITYが設定されている場合は、\(cx_1=cx_2\)となります。
Vertical stereo: the first and the second camera views are shifted relative to each other mainly in the vertical direction (and probably a bit in the horizontal direction too). The epipolar lines in the rectified images are vertical and have the same x-coordinate. P1 and P2 look like:,垂直方向のステレオ：1台目と2台目のカメラのビューは、主に垂直方向に相対的にシフトします（おそらく水平方向にも少しシフトします）。整形された画像のエピポーララインは垂直で，同じx座標を持っています．P1とP2は次のようになります。
\[\texttt{P1} = \begin{bmatrix} f & 0 & cx & 0 \\ 0 & f & cy_1 & 0 \\ 0 & 0 & 1 & 0 \end{bmatrix}\],\P1 = ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ_ˆÇÇ
"\[\texttt{P2} = \begin{bmatrix} f & 0 & cx & 0 \\ 0 & f & cy_2 & T_y*f \\ 0 & 0 & 1 & 0 \end{bmatrix},\]",\♪\\\\
"where \(T_y\) is a vertical shift between the cameras and \(cy_1=cy_2\) if CALIB_ZERO_DISPARITY is set.As you can see, the first three columns of P1 and P2 will effectively be the new ""rectified"" camera matrices. The matrices, together with R1 and R2 , can then be passed to initUndistortRectifyMap to initialize the rectification map for each camera.See below the screenshot from the stereo_calib.cpp sample. Some red horizontal lines pass through the corresponding image regions. This means that the images are well rectified, which is what most stereo correspondence algorithms rely on. The green rectangles are roi1 and roi2 . You see that their interiors are all valid pixels.image",ご覧のように、P1、P2の最初の3列は、実質的に新しい「平行化された」カメラ行列となります。この行列は，R1 と R2 とともに initUndistortRectifyMap に渡され，各カメラの平行化マップを初期化します。以下に，stereo_calib.cpp サンプルのスクリーンショットを示します。いくつかの赤い水平線が，対応する画像領域を通過しています．これは，画像がよく平行化されていることを意味しており，ほとんどのステレオ対応付けアルゴリズムがこれに依存しています．緑色の四角形は， roi1 と roi2 です．これらの内部は，すべて有効なピクセルであることが分かります．
Computes a rectification transform for an uncalibrated stereo camera.,キャリブレーションされていないステレオカメラに対して，平行化変換を行います．
points1 : Array of feature points in the first image.,points1 : 1枚目の画像における特徴点の配列．
points2 : The corresponding points in the second image. The same formats as in findFundamentalMat are supported.,points2 : 2番目の画像における，対応する点の配列．findFundamentalMat と同じフォーマットがサポートされています．
F : Input fundamental matrix. It can be computed from the same set of point pairs using findFundamentalMat .,F : 入力される基本行列．これは， findFundamentalMat を用いて，同じ点群から計算することができます．
imgSize : Size of the image.,imgSize : 画像のサイズ．
H1 : Output rectification homography matrix for the first image.,H1 : 1枚目の画像に対する出力される平行化ホモグラフィ行列．
H2 : Output rectification homography matrix for the second image.,H2 : 2番目の画像に対する平行化ホモグラフィ行列を出力．
"threshold : Optional threshold used to filter out the outliers. If the parameter is greater than zero, all the point pairs that do not comply with the epipolar geometry (that is, the points for which \(|\texttt{points2[i]}^T*\texttt{F}*\texttt{points1[i]}|>\texttt{threshold}\) ) are rejected prior to computing the homographies. Otherwise, all the points are considered inliers.",threshold : 外れ値を除外するために利用されるオプションの閾値．このパラメータが0よりも大きい場合，エピポーラ幾何学に従わないすべての点のペア（つまり，\(|texttt{points2[i]}^T*\tt{F}*\tt{points1[i]}|>texttt{threshold})が存在する点）は，ホモグラフィを計算する前に却下されます．それ以外の場合は，すべての点がインライアであると見なされます．
"The function computes the rectification transformations without knowing intrinsic parameters of the cameras and their relative position in the space, which explains the suffix ""uncalibrated"". Another related difference from stereoRectify is that the function outputs not the rectification transformations in the object (3D) space, but the planar perspective transformations encoded by the homography matrices H1 and H2 . The function implements the algorithm [104] .NoteWhile the algorithm does not need to know the intrinsic parameters of the cameras, it heavily depends on the epipolar geometry. Therefore, if the camera lenses have a significant distortion, it would be better to correct it before computing the fundamental matrix and calling this function. For example, distortion coefficients can be estimated for each head of stereo camera separately by using calibrateCamera . Then, the images can be corrected using undistort , or just the point coordinates can be corrected with undistortPoints .",この関数は，カメラの固有のパラメータや空間内の相対的な位置を知らずに，平行化変換を計算するので，「uncalibrated」という接尾辞が付きます．stereoRectify とのもう1つの関連する違いは，この関数が物体（3次元）空間における平行化変換ではなく，ホモグラフィ行列 H1 と H2 によってエンコードされた平面透視変換を出力することです．この関数は，アルゴリズム [104] を実装しています． 注意このアルゴリズムは，カメラの固有のパラメータを知る必要はありませんが，エピポーラ幾何学に大きく依存します．したがって，カメラレンズに大きな歪みがある場合は，基本行列を計算してこの関数を呼び出す前に，その歪みを補正した方が良いでしょう．例えば， calibrateCamera を使って，ステレオカメラの各ヘッドの歪み係数を個別に推定することができます．その後， undistort を用いて画像を補正したり， undistortPoints を用いて点座標だけを補正したりすることができます．
"computes the rectification transformations for 3-head camera, where all the heads are on the same line.",すべてのヘッドが同じ線上にある，3ヘッドカメラの平行化変換を求めます．
Returns the new camera intrinsic matrix based on the free scaling parameter.,フリースケーリングパラメータに基づいた，新しいカメラ固有の行列を返します．
cameraMatrix : Input camera intrinsic matrix.,cameraMatrix :入力カメラ固有の行列．
imageSize : Original image size.,imageSize : 元の画像サイズ．
alpha : Free scaling parameter between 0 (when all the pixels in the undistorted image are valid) and 1 (when all the source image pixels are retained in the undistorted image). See stereoRectify for details.,alpha : 0（歪んでいない画像のすべてのピクセルが有効な場合）から1（ソース画像のすべてのピクセルが歪んでいない画像に保持される場合）までのフリースケーリングパラメータ．詳細は stereoRectify を参照してください．
"newImgSize : Image size after rectification. By default, it is set to imageSize .",newImgSize : 整形後の画像サイズ．デフォルトでは imageSize に設定されています．
"validPixROI : Optional output rectangle that outlines all-good-pixels region in the undistorted image. See roi1, roi2 description in stereoRectify .","validPixROI : 歪んでいない画像の中の，すべての良いピクセルの領域の輪郭を示す，オプションの出力矩形．stereoRectify の roi1, roi2 の説明を参照してください．"
"centerPrincipalPoint : Optional flag that indicates whether in the new camera intrinsic matrix the principal point should be at the image center or not. By default, the principal point is chosen to best fit a subset of the source image (determined by alpha) to the corrected image.",centerPrincipalPoint : 新しいカメラ固有の行列において，主点を画像の中心に置くべきか否かを示すオプションのフラグ．デフォルトでは，ソース画像のサブセット（アルファ値で決定されます）が補正後の画像に最もフィットするように，主点が選択されます．
"The function computes and returns the optimal new camera intrinsic matrix based on the free scaling parameter. By varying this parameter, you may retrieve only sensible pixels alpha=0 , keep all the original image pixels if there is valuable information in the corners alpha=1 , or get something in between. When alpha>0 , the undistorted result is likely to have some black pixels corresponding to ""virtual"" pixels outside of the captured distorted image. The original camera intrinsic matrix, distortion coefficients, the computed new camera intrinsic matrix, and newImageSize should be passed to initUndistortRectifyMap to produce the maps for remap .",この関数は，フリースケーリングパラメータに基づいて，最適な新しいカメラ固有の行列を計算し，それを返します．このパラメータを変化させることで，意味のあるピクセルだけを取り出したり alpha=0 ，コーナーに貴重な情報がある場合に元画像のピクセルをすべて残したり alpha=1 ，その中間の値を得たりすることができます．alpha>0の場合，歪みのない結果には，キャプチャされた歪んだ画像の外側にある「仮想」ピクセルに対応するいくつかの黒いピクセルが含まれる可能性があります．元のカメラの内部行列，歪み係数，計算された新しいカメラの内部行列，そして newImageSize は，リマップ用のマップを生成するために initUndistortRectifyMap に渡されなければいけません．
Computes Hand-Eye calibration: \(_{}^{g}\textrm{T}_c\).,Hand-Eye Calibrationを計算します．\(_{}^{g}\textrm{T}_c\).
R_gripper2base : [in],R_gripper2base : [in].
t_gripper2base : [in],T_GRIPPER2BASE : [in] (グリッパー2ベース)
R_target2cam : [in],R_target2cam : [in].
t_target2cam : [in],t_target2cam : [in].
R_cam2gripper : [out],R_cam2gripper : [out] (R_cam2gripper)
t_cam2gripper : [out],t_cam2gripper : [out].
method : [in],メソッド : [in].
"The function performs the Hand-Eye calibration using various methods. One approach consists in estimating the rotation then the translation (separable solutions) and the following methods are implemented:R. Tsai, R. Lenz A New Technique for Fully Autonomous and Efficient 3D Robotics Hand/EyeCalibration [249]","この関数は，様々な手法を用いて手と目のキャリブレーションを行います．一つの方法は，回転と平行移動を推定することであり（分離可能な解），以下の方法が実装されています：R. Tsai, R. Lenz A.S.A.Tsai, R. Lenz A New Technique for Fully Autonomous and Efficient 3D Robotics Hand/EyeCalibration [249]."
"F. Park, B. Martin Robot Sensor Calibration: Solving AX = XB on the Euclidean Group [190]","F.Park, B. Martin Robot Sensor Calibration:ユークリッドグループ上でAX = XBを解決する [190]。"
"R. Horaud, F. Dornaika Hand-Eye Calibration [112]Another approach consists in estimating simultaneously the rotation and the translation (simultaneous solutions), with the following implemented methods:N. Andreff, R. Horaud, B. Espiau On-line Hand-Eye Calibration [11]","R.Horaud, F. Dornaika Hand-Eye Calibration [112]別のアプローチは，回転と平行移動を同時に推定すること（同時解）であり，次のような実装方法がある：N.Andreff, R. Horaud, B. Espiau On-line Hand-Eye Calibration [11]."
"K. Daniilidis Hand-Eye Calibration Using Dual Quaternions [52]The following picture describes the Hand-Eye calibration problem where the transformation between a camera (""eye"") mounted on a robot gripper (""hand"") has to be estimated. This configuration is called eye-in-hand.The eye-to-hand configuration consists in a static camera observing a calibration pattern mounted on the robot end-effector. The transformation from the camera to the robot base frame can then be estimated by inputting the suitable transformations to the function, see below.The calibration procedure is the following:a static calibration pattern is used to estimate the transformation between the target frame and the camera frame",K.Daniilidis Hand-Eye Calibration Using Dual Quaternions [52]次の図は、ロボットグリッパー（「ハンド」）に取り付けられたカメラ（「アイ」）間の変換を推定しなければならないハンドアイのキャリブレーション問題を説明しています。Eye-to-Hand構成は、ロボットのエンドエフェクタに取り付けられた校正パターンを観察する静的なカメラで構成されています。カメラからロボットのベースフレームへの変換は、適切な変換を関数に入力することで推定できます（以下参照）。
the robot gripper is moved in order to acquire several poses,ロボットのグリッパーを動かして，複数のポーズを取得する
"for each pose, the homogeneous transformation between the gripper frame and the robot base frame is recorded using for instance the robot kinematics ",各ポーズについて、グリッパーフレームとロボットベースフレームの間の同次変換が、例えばロボットキネマティクスを用いて記録されます。
\[ \begin{bmatrix} X_b\\ Y_b\\ Z_b\\ 1 \end{bmatrix} = \begin{bmatrix} _{}^{b}\textrm{R}_g & _{}^{b}\textrm{t}_g \\ 0_{1 \times 3} & 1 \end{bmatrix} \begin{bmatrix} X_g\\ Y_g\\ Z_g\\ 1 \end{bmatrix} \],\Mr.X_b\ Y_b\ Z_b\ 1 ˶ˆ꒳ˆ˵ ) = ˶ˆ꒳ˆ˵ ) = ˶ˆ꒳ˆ˵ )_{}^{b}\\{R}_g & _{}^{b}\\{t}_g\\0_{1 ୧⃛(๑⃙⃘⁼̴̀꒳⁼̴́๑⃙⃘)\\\\♪♪♪♪♪♪♪～\]
"for each pose, the homogeneous transformation between the calibration target frame and the camera frame is recorded using for instance a pose estimation method (PnP) from 2D-3D point correspondences ",各ポーズについて、キャリブレーションターゲットフレームとカメラフレームの間の同次変換を、例えば、2D-3D点対応からのポーズ推定法（PnP）を用いて記録します。
\[ \begin{bmatrix} X_c\\ Y_c\\ Z_c\\ 1 \end{bmatrix} = \begin{bmatrix} _{}^{c}\textrm{R}_t & _{}^{c}\textrm{t}_t \\ 0_{1 \times 3} & 1 \end{bmatrix} \begin{bmatrix} X_t\\ Y_t\\ Z_t\\ 1 \end{bmatrix} \]The Hand-Eye calibration procedure returns the following homogeneous transformation\[ \begin{bmatrix} X_g\\ Y_g\\ Z_g\\ 1 \end{bmatrix} = \begin{bmatrix} _{}^{g}\textrm{R}_c & _{}^{g}\textrm{t}_c \\ 0_{1 \times 3} & 1 \end{bmatrix} \begin{bmatrix} X_c\\ Y_c\\ Z_c\\ 1 \end{bmatrix} \]This problem is also known as solving the \(\mathbf{A}\mathbf{X}=\mathbf{X}\mathbf{B}\) equation:for an eye-in-hand configuration ,\˶ˆ꒳ˆ˵ )X_c\ Y_c%% Z_c%% 1 ˶ˆ꒳ˆ˵ ) = ˶ˆ꒳ˆ˵ )_{}^{c}\\{R}_t & _{}^{c}\\{t}_t 0_{1 ୧⃛(๑⃙⃘⁼̴̀꒳⁼̴́๑⃙⃘)\\\\♪ X t\\ Y t ♪ Z t ♪ 1\ハンドアイ キャリブレーションではX_g\ Y_g\ Z_g\ 1 ardent{bmatrix} = ˶‾᷄ -̫ ‾᷅˵˵♪♪♪♪♪～\\\\X_c\ Y_cadows Z_cadows 1 end{bmatrix}.\この問題は別名、目と手の関係を表す
\[ \begin{align*} ^{b}{\textrm{T}_g}^{(1)} \hspace{0.2em} ^{g}\textrm{T}_c \hspace{0.2em} ^{c}{\textrm{T}_t}^{(1)} &= \hspace{0.1em} ^{b}{\textrm{T}_g}^{(2)} \hspace{0.2em} ^{g}\textrm{T}_c \hspace{0.2em} ^{c}{\textrm{T}_t}^{(2)} \\ (^{b}{\textrm{T}_g}^{(2)})^{-1} \hspace{0.2em} ^{b}{\textrm{T}_g}^{(1)} \hspace{0.2em} ^{g}\textrm{T}_c &= \hspace{0.1em} ^{g}\textrm{T}_c \hspace{0.2em} ^{c}{\textrm{T}_t}^{(2)} (^{c}{\textrm{T}_t}^{(1)})^{-1} \\ \textrm{A}_i \textrm{X} &= \textrm{X} \textrm{B}_i \\ \end{align*} \],\♪♪～^{b}{\textrm{T}_g}^{(1)}\0.2em}になります。^{g}\textrm{T}_c \hspace{0.2em}^{c}{\\{T}_t}^{(1)} &= ˶ˆ꒳ˆ˵ )^{b}{\textrm{T}_g}^{(2)}\ω＾）ノ^{g}\textrm{T}_c \hspace{0.2em}^{c}{\textrm{T}_t}^{(2)}(^{b}{\\{T}_g}^{(2)})^{-1} ˶‾᷄ -̫ ‾᷅˵˵^{b}{\textrm{T}_g}^{(1)}\0.2em}になります。^{g}\\{T}_c &= ˶‾᷄ -̫ ‾᷅˵˵^{g}\textrm{T}_c \hspace{0.2em}^{c}{\\{T}_t}^{(2)} (^{c}{\\\{T}_t}^{(1)})^{-1} \\{A}_i ୨୧ &= ୨୧┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈୨୧\]
for an eye-to-hand configuration ,for an eye-to-hand configuration
\[ \begin{align*} ^{g}{\textrm{T}_b}^{(1)} \hspace{0.2em} ^{b}\textrm{T}_c \hspace{0.2em} ^{c}{\textrm{T}_t}^{(1)} &= \hspace{0.1em} ^{g}{\textrm{T}_b}^{(2)} \hspace{0.2em} ^{b}\textrm{T}_c \hspace{0.2em} ^{c}{\textrm{T}_t}^{(2)} \\ (^{g}{\textrm{T}_b}^{(2)})^{-1} \hspace{0.2em} ^{g}{\textrm{T}_b}^{(1)} \hspace{0.2em} ^{b}\textrm{T}_c &= \hspace{0.1em} ^{b}\textrm{T}_c \hspace{0.2em} ^{c}{\textrm{T}_t}^{(2)} (^{c}{\textrm{T}_t}^{(1)})^{-1} \\ \textrm{A}_i \textrm{X} &= \textrm{X} \textrm{B}_i \\ \end{align*} \]NoteAdditional information can be found on this website. ,\♪♪～^{g}{\textrm{T}_b}^{(1)}\0.2em}になります。^{b}\textrm{T}_c \hspace{0.2em}^{c}{\\{T}_t}^{(1)} &= ˶‾᷄ -̫ ‾᷅˵˵^{g}{\textrm{T}_b}^{(2)}\0.2em}になります。^{b}\textrm{T}_c \hspace{0.2em}^{c}{\textrm{T}_t}^{(2)}(^{g}{\\{T}_b}^{(2)})^{-1} ˶‾᷄ -̫ ‾᷅˵˵^{g}{\textrm{T}_b}^{(1)}\0.2em}になります。^{b}\\{T}_c &= \\\{T}_c^{b}\textrm{T}_c \hspace{0.2em}^{c}{\\{T}_t}^{(2)} (^{c}{\\\{T}_t}^{(1)})^{-1} ˶ˆ꒳ˆ˵ )\NoteAdditional information are found on this website.
"A minimum of 2 motions with non parallel rotation axes are necessary to determine the hand-eye transformation. So at least 3 different poses are required, but it is strongly recommended to use many more poses.",手目線の変換を判定するためには、回転軸が平行でないモーションが最低2つ必要です。そのため、最低でも3種類のポーズが必要ですが、それ以上のポーズを使用することを強く推奨します。
Computes Robot-World/Hand-Eye calibration: \(_{}^{w}\textrm{T}_b\) and \(_{}^{c}\textrm{T}_g\).,"Robot-World/Hand-Eye Calibrationを計算します。\(_{}^{w}\textrm{T}_b\), \(_{}^{c}\textrm{T}_g\)"
R_world2cam : [in],R_world2cam : [in].
t_world2cam : [in],t_world2cam : [in].
R_base2gripper : [in],R_base2gripper : [in] (英語)
t_base2gripper : [in],t_base2gripper : [in].
R_base2world : [out],R_base2world : [out] (R_base2world)
t_base2world : [out],t_base2world : [out] (英語)
R_gripper2cam : [out],R_gripper2cam : [out] (R_gripper2cam)
t_gripper2cam : [out],t_gripper2cam : [out].
"The function performs the Robot-World/Hand-Eye calibration using various methods. One approach consists in estimating the rotation then the translation (separable solutions):M. Shah, Solving the robot-world/hand-eye calibration problem using the kronecker product [221]Another approach consists in estimating simultaneously the rotation and the translation (simultaneous solutions), with the following implemented method:A. Li, L. Wang, and D. Wu, Simultaneous robot-world and hand-eye calibration using dual-quaternions and kronecker product [143]The following picture describes the Robot-World/Hand-Eye calibration problem where the transformations between a robot and a world frame and between a robot gripper (""hand"") and a camera (""eye"") mounted at the robot end-effector have to be estimated.The calibration procedure is the following:a static calibration pattern is used to estimate the transformation between the target frame and the camera frame","この関数は，様々な方法を用いて，Robot-World/Hand-Eyeのキャリブレーションを行います．一つの方法は，回転と平行移動を推定することです（分離可能な解）：M.Shah, Solving the robot-world/hand-eye calibration problem using the kronecker product [221]もう1つの方法は，回転と平行移動を同時に推定する方法（同時解法）で，次のように実装されています：A. Li, L. Wang, and D. Wu, Simultaneous robot-world/Hand-Eye calibration problem using the kronecker product [221].A. Li, L. Wang, and D. Wu, Simultaneous robot-world and hand-eye calibration using dual-quaternions and kronecker product [143]次の図は、ロボットとワールドフレームの間の変換、およびロボットグリッパー（「ハンド」）とロボットエンドエフェクタに取り付けられたカメラ（「アイ」）の間の変換を推定しなければならない、ロボット-ワールド/ハンド-アイのキャリブレーション問題を説明しています。"
\[ \begin{bmatrix} X_g\\ Y_g\\ Z_g\\ 1 \end{bmatrix} = \begin{bmatrix} _{}^{g}\textrm{R}_b & _{}^{g}\textrm{t}_b \\ 0_{1 \times 3} & 1 \end{bmatrix} \begin{bmatrix} X_b\\ Y_b\\ Z_b\\ 1 \end{bmatrix} \],\キャリブレーションの手順は以下の通りです。X_g\ Y_g\ Z_g\ 1 end{bmatrix} = ˶ˆ꒳ˆ˵ )_{}^{g}\\{R}/b & _{}^{g}\\{t}/b\\\\♪♪♪♪♪♪♪♪♪～\]
"for each pose, the homogeneous transformation between the calibration target frame (the world frame) and the camera frame is recorded using for instance a pose estimation method (PnP) from 2D-3D point correspondences ",各ポーズについて、キャリブレーション対象フレーム（ワールドフレーム）とカメラフレームの間の同次変換を、例えば、2D-3D点対応からのポーズ推定法（PnP）を用いて記録する。
"\[ \begin{bmatrix} X_c\\ Y_c\\ Z_c\\ 1 \end{bmatrix} = \begin{bmatrix} _{}^{c}\textrm{R}_w & _{}^{c}\textrm{t}_w \\ 0_{1 \times 3} & 1 \end{bmatrix} \begin{bmatrix} X_w\\ Y_w\\ Z_w\\ 1 \end{bmatrix} \]The Robot-World/Hand-Eye calibration procedure returns the following homogeneous transformations\[ \begin{bmatrix} X_w\\ Y_w\\ Z_w\\ 1 \end{bmatrix} = \begin{bmatrix} _{}^{w}\textrm{R}_b & _{}^{w}\textrm{t}_b \\ 0_{1 \times 3} & 1 \end{bmatrix} \begin{bmatrix} X_b\\ Y_b\\ Z_b\\ 1 \end{bmatrix} \]\[ \begin{bmatrix} X_c\\ Y_c\\ Z_c\\ 1 \end{bmatrix} = \begin{bmatrix} _{}^{c}\textrm{R}_g & _{}^{c}\textrm{t}_g \\ 0_{1 \times 3} & 1 \end{bmatrix} \begin{bmatrix} X_g\\ Y_g\\ Z_g\\ 1 \end{bmatrix} \]This problem is also known as solving the \(\mathbf{A}\mathbf{X}=\mathbf{Z}\mathbf{B}\) equation, with:\(\mathbf{A} \Leftrightarrow \hspace{0.1em} _{}^{c}\textrm{T}_w\)",\˶ˆ꒳ˆ˵ )X_c\ Y_citness Z_citness 1 ˶‾᷄ -̫ ‾᷅˵ = ˶‾᷄ -̫ ‾᷄˵ )_{}^{c}\\{R}_w & _{}^{c}\\{t}_w\\\\X w\\ Y wadows\Robot-World/Hand-Eyeキャリブレーション手順では、以下のような同次変換が行われます。X_w\ Y_w\ Z_w\ 1 end{bmatrix} = ˶ˆ꒳ˆ˵ )♪♪♪♪♪～\\\\♪♪♪♪♪♪♪♪♪～\♪♪♪♪♪～♪♪♪♪♪♪♪～_{}^{c}\\{R}_g & _{}^{c}\{t}_g\\\\X'g\ Y'g\ Z'g\\ 1 end{bmatrix}.この問題は別名、\\\\\\\\\\\\\\\\\\\\\とも呼ばれます。
\(\mathbf{X} \Leftrightarrow \hspace{0.1em} _{}^{w}\textrm{T}_b\),\♪♪♪♪♪♪♪～
\(\mathbf{Z} \Leftrightarrow \hspace{0.1em} _{}^{c}\textrm{T}_g\),\♪♪～
\(\mathbf{B} \Leftrightarrow \hspace{0.1em} _{}^{g}\textrm{T}_b\)NoteAt least 3 measurements are required (input vectors size must be greater or equal to 3).,\Note最低でも3つの測定値が必要です（入力ベクトルのサイズは3以上です）。
Converts points from Euclidean to homogeneous space.,点をユークリッド空間から同次元空間に変換します。
src : Input vector of N-dimensional points.,src : N次元点の入力ベクトル．
dst : Output vector of N+1-dimensional points.,dst : N+1次元点の出力ベクトル．
"The function converts points from Euclidean to homogeneous space by appending 1's to the tuple of point coordinates. That is, each point (x1, x2, ..., xn) is converted to (x1, x2, ..., xn, 1).","この関数は，点の座標のタプルに1を追加することで，点をユークリッド空間から同次空間に変換します．つまり、各点(x1, x2, ..., xn)は(x1, x2, ..., xn, 1)に変換されます。"
Converts points from homogeneous to Euclidean space.,点を同次空間からユークリッド空間に変換します。
dst : Output vector of N-1-dimensional points.,dst : N-1次元の点の出力ベクトル。
"The function converts points homogeneous to Euclidean space using perspective projection. That is, each point (x1, x2, ... x(n-1), xn) is converted to (x1/xn, x2/xn, ..., x(n-1)/xn). When xn=0, the output point coordinates will be (0,0,0,...).","この関数は、透視投影を用いて、同次元点をユークリッド空間に変換します。つまり、各点(x1, x2, ... x(n-1), xn)は、(x1/xn, x2/xn, ..., x(n-1)/xn)に変換されます。xn=0の場合、出力される点座標は(0,0,0,...)となります。"
Converts points to/from homogeneous coordinates.,点を同次座標に，または同次座標から変換します．
"src : Input array or vector of 2D, 3D, or 4D points.",src : 2次元，3次元，または4次元の点の入力配列またはベクトル．
"dst : Output vector of 2D, 3D, or 4D points.",dst : 2次元，3次元，または4次元の点の出力ベクトル．
The function converts 2D or 3D points from/to homogeneous coordinates by calling either convertPointsToHomogeneous or convertPointsFromHomogeneous.NoteThe function is obsolete. Use one of the previous two functions instead.,この関数は， convertPointsToHomogeneous または convertPointsFromHomogeneous を呼び出すことで，2次元または3次元の点を同次座標に変換します．代わりに，前の2つの関数のいずれかを使ってください．
Calculates a fundamental matrix from the corresponding points in two images.,2つの画像中の対応する点から，基本行列を求めます．
points1 : Array of N points from the first image. The point coordinates should be floating-point (single or double precision).,points1 : 1枚目の画像から得られるN個の点の配列．点の座標は，浮動小数点（単精度または倍精度）でなければいけません．
points2 : Array of the second image points of the same size and format as points1 .,points2 : points1 と同じサイズ，同じフォーマットの2枚目の画像の点の配列．
method : Method for computing a fundamental matrix.,method : 基本行列の計算方法．
FM_7POINT for a 7-point algorithm. \(N = 7\),7-point algorithm の場合は，FM_7POINT となります．\(N = 7\)
FM_8POINT for an 8-point algorithm. \(N \ge 8\),FM_8POINT 8-point algorithm.\FM_8POINT
FM_RANSAC for the RANSAC algorithm. \(N \ge 8\),RANSACアルゴリズムはFM_RANSACです。| FM_RANSAC
FM_LMEDS for the LMedS algorithm. \(N \ge 8\),LMedSアルゴリズムのFM_LMEDS\♪♪～
"ransacReprojThreshold : Parameter used only for RANSAC. It is the maximum distance from a point to an epipolar line in pixels, beyond which the point is considered an outlier and is not used for computing the final fundamental matrix. It can be set to something like 1-3, depending on the accuracy of the point localization, image resolution, and the image noise.",ransacReprojThreshold : RANSACでのみ使われるパラメータ。ある点からエピポーラ線までの最大距離をピクセル単位で表したもので，これを超えるとその点は外れ値とみなされ，最終的な基本行列の計算には用いられません．これは，点のローカライズの精度，画像の解像度，画像のノイズに応じて，1〜3程度に設定できます．
confidence : Parameter used for the RANSAC and LMedS methods only. It specifies a desirable level of confidence (probability) that the estimated matrix is correct.,confidence : RANSAC法とLMedS法でのみ使用されるパラメータ．これは，推定された行列が正しいことを示す望ましい信頼度（確率）を指定します．
mask : [out],mask : [out].
maxIters : The maximum number of robust method iterations.,maxIters : ロバスト法の反復回数の最大値．
"The epipolar geometry is described by the following equation:\[[p_2; 1]^T F [p_1; 1] = 0\]where \(F\) is a fundamental matrix, \(p_1\) and \(p_2\) are corresponding points in the first and the second images, respectively.The function calculates the fundamental matrix using one of four methods listed above and returns the found fundamental matrix. Normally just one matrix is found. But in case of the 7-point algorithm, the function may return up to 3 solutions ( \(9 \times 3\) matrix that stores all 3 matrices sequentially).The calculated fundamental matrix may be passed further to computeCorrespondEpilines that finds the epipolar lines corresponding to the specified points. It can also be passed to stereoRectifyUncalibrated to compute the rectification transformation. :// Example. Estimation of fundamental matrix using the RANSAC algorithmint point_count = 100;vector<Point2f> points1(point_count);vector<Point2f> points2(point_count);// initialize the points here ...for( int i = 0; i < point_count; i++ ){    points1[i] = ...;    points2[i] = ...;}Mat fundamental_matrix = findFundamentalMat(points1, points2, FM_RANSAC, 3, 0.99);fragment","この関数は，上述した4つの手法のうちの1つを用いて基本行列を計算し，その結果を返します．通常は，1つの行列だけが求められます．計算された基本行列は，指定された点に対応するエピポーラ線を求める computeCorrespondEpilines に渡すことができます．また，stereoRectifyUncalibrated に渡して，平行化変換を計算することもできます． :// 例．RANSAC アルゴリズムを用いた基本行列の推定int point_count = 100;vector<Point2f> points1(point_count);vector<Point2f> points2(point_count);// ここでポイントを初期化します ....for( int i = 0; i < point_count; i++ ){ points1[i] = ...; points2[i] = ...;}Mat fundamental_matrix = findFundamentalMat(points1, points2, FM_RANSAC, 3, 0.99);fragment"
"For points in an image of a stereo pair, computes the corresponding epilines in the other image.",ステレオペアの画像中の点に対して，もう片方の画像中の対応するエピラインを計算します．
points : Input points. \(N \times 1\) or \(1 \times N\) matrix of type CV_32FC2 or vector<Point2f> .,points :入力点．\CV_32FC2 型の行列，または vector<Point2f> 型の行列．
whichImage : Index of the image (1 or 2) that contains the points .,whichImage : 点を含む画像（1または2）のインデックス．
F : Fundamental matrix that can be estimated using findFundamentalMat or stereoRectify .,F : findFundamentalMat または stereoRectify によって推定されるファンダメンタルマトリックス．
"lines : Output vector of the epipolar lines corresponding to the points in the other image. Each line \(ax + by + c=0\) is encoded by 3 numbers \((a, b, c)\) .","lines : 他の画像中の点に対応するエピポーラ線の出力ベクトル．各直線は，3つの数字 ˶((a, b, c)˶) によって表現されます．"
"For every point in one of the two images of a stereo pair, the function finds the equation of the corresponding epipolar line in the other image.From the fundamental matrix definition (see findFundamentalMat ), line \(l^{(2)}_i\) in the second image for the point \(p^{(1)}_i\) in the first image (when whichImage=1 ) is computed as:\[l^{(2)}_i = F p^{(1)}_i\]And vice versa, when whichImage=2, \(l^{(1)}_i\) is computed from \(p^{(2)}_i\) as:\[l^{(1)}_i = F^T p^{(2)}_i\]Line coefficients are defined up to a scale. They are normalized so that \(a_i^2+b_i^2=1\) .",この関数は，ステレオペアの2つの画像のうち，片方の画像内の各点に対して，もう片方の画像内の対応するエピポーラ線の方程式を求めます．基本行列の定義（ findFundamentalMat 参照）から，1枚目の画像中の点\(p^{(1)}_i\)に対する2枚目の画像中の線\(l^{(2)}_i\)は，次のように計算されます（whichImage=1 の場合）．\また、whichImage=2の場合、\(p^{(2)}_i\)から、\(l^{(1)}_i\)が計算されます：\[l^{(1)}_i = F^T p^{(2)}_i\]線の係数は、ある尺度まで定義されます。a_i^2+b_i^2=1\）となるように規格化されています。
This function reconstructs 3-dimensional points (in homogeneous coordinates) by using their observations with a stereo camera.,この関数は，ステレオカメラによる観測結果を用いて，3次元の点（同次座標）を再構成します．
"projMatr1 : 3x4 projection matrix of the first camera, i.e. this matrix projects 3D points given in the world's coordinate system into the first image.",projMatr1 : 3x4 の第1カメラの投影行列，つまり，世界座標系で与えられた3次元点を第1画像に投影する行列．
"projMatr2 : 3x4 projection matrix of the second camera, i.e. this matrix projects 3D points given in the world's coordinate system into the second image.",projMatr2 : 3x4 の第2カメラの射影行列，つまりこの行列は，世界座標系で与えられた3次元点を第2画像に射影します．
"projPoints1 : 2xN array of feature points in the first image. In the case of the c++ version, it can be also a vector of feature points or two-channel matrix of size 1xN or Nx1.",projPoints1 : 1枚目の画像における特徴点の2xN配列．c++ バージョンの場合は，特徴点のベクトルや，サイズが 1xN または Nx1 の 2 チャンネル行列を指定することもできます．
"projPoints2 : 2xN array of corresponding points in the second image. In the case of the c++ version, it can be also a vector of feature points or two-channel matrix of size 1xN or Nx1.",projPoints2 : 2枚目の画像中の対応点を表す2xN 個の配列．c++ 版の場合は，特徴点のベクトルや，サイズが 1xN または Nx1 の2チャンネル行列でもよいです．
points4D : 4xN array of reconstructed points in homogeneous coordinates. These points are returned in the world's coordinate system.,points4D : 同次座標系における再構成点の4xN配列．これらの点は，世界座標系で返されます．
NoteKeep in mind that all input data should be of float type in order for this function to work.,注意この関数が動作するためには，すべての入力データが float 型でなければならないことに注意してください．
"If the projection matrices from stereoRectify are used, then the returned points are represented in the first camera's rectified coordinate system.See alsoreprojectImageTo3D",stereoRectifyからの投影行列が使われた場合，返される点は，最初のカメラの平行化された座標系で表されます． 参照：soreprojectImageTo3D
Refines coordinates of corresponding points.,対応する点の座標を修正します．
F : 3x3 fundamental matrix.,F : 3x3 の基本行列．
points1 : 1xN array containing the first set of points.,points1 : 最初の点群を含む 1xN の配列．
points2 : 1xN array containing the second set of points.,points2 : 2 番目の点の集合を含む 1xN の配列．
newPoints1 : The optimized points1.,newPoints1 : 最適化された points1.
newPoints2 : The optimized points2.,newPoints2 : 最適化された points2.
"The function implements the Optimal Triangulation Method (see Multiple View Geometry for details). For each given point correspondence points1[i] <-> points2[i], and a fundamental matrix F, it computes the corrected correspondences newPoints1[i] <-> newPoints2[i] that minimize the geometric error \(d(points1[i], newPoints1[i])^2 + d(points2[i],newPoints2[i])^2\) (where \(d(a,b)\) is the geometric distance between points \(a\) and \(b\) ) subject to the epipolar constraint \(newPoints2^T * F * newPoints1 = 0\) .","この関数は，Optimal Triangulation Method を実装しています（詳細は，「多視点幾何学」を参照してください）．この関数は，与えられた各点の対応点 points1[i] <-> points2[i]，および基本行列 F に対して，幾何学的誤差 ˶ˆ꒳ˆ˵ (d(points1[i],d(points1[i], newPoints1[i])^2 + d(points2[i],newPoints2[i])^2\) (ここで、\(d(a,b)\)は、点\(a)と点\(b)の間の幾何学的距離です)。"
Filters off small noise blobs (speckles) in the disparity map.,視差マップに含まれる小さなノイズの塊（スペックル）を除去します．
img : The input 16-bit signed disparity image,img : 入力された16ビット符号付き視差画像
newVal : The disparity value used to paint-off the speckles,newVal : スペックルを除去するために用いられる視差値．
maxSpeckleSize : The maximum speckle size to consider it a speckle. Larger blobs are not affected by the algorithm,maxSpeckleSize : スペックルとみなすための最大スペックルサイズ。これより大きいblobはアルゴリズムの影響を受けません
"maxDiff : Maximum difference between neighbor disparity pixels to put them into the same blob. Note that since StereoBM, StereoSGBM and may be other algorithms return a fixed-point disparity map, where disparity values are multiplied by 16, this scale factor should be taken into account when specifying this parameter value.","maxDiff : 隣接する視差画素を同じblobに入れるための最大差。StereoBM, StereoSGBM およびその他のアルゴリズムは、固定小数点の視差マップを返すので、このパラメータ値を指定する際には、このスケールファクターを考慮しなければならないことに注意してください。"
buf : The optional temporary buffer to avoid memory allocation within the function.,buf :関数内でのメモリ割り当てを避けるための，オプションの一時的なバッファ．
computes valid disparity ROI from the valid ROIs of the rectified images (that are returned by stereoRectify),stereoRectify によって返される）平行化された画像の有効な ROI から，有効な視差の ROI を計算します．
"validates disparity using the left-right check. The matrix ""cost"" should be computed by the stereo correspondence algorithm",左右のチェックを利用して，視差を検証します．行列の「コスト」は，ステレオ対応点探索アルゴリズムによって計算されます．
Reprojects a disparity image to 3D space.,視差画像を，3次元空間に再投影します．
"disparity : Input single-channel 8-bit unsigned, 16-bit signed, 32-bit signed or 32-bit floating-point disparity image. The values of 8-bit / 16-bit signed formats are assumed to have no fractional bits. If the disparity is 16-bit signed format, as computed by StereoBM or StereoSGBM and maybe other algorithms, it should be divided by 16 (and scaled to float) before being used here.",disparity : シングルチャンネルの，8ビット符号なし，16ビット符号あり，32ビット符号あり，または32ビット浮動小数点の視差画像を入力します．8bit / 16bit 符号付きフォーマットの値は，端数ビットを持たないと仮定されます．StereoBM や StereoSGBM などのアルゴリズムで計算された 16 ビット符号付きフォーマットの視差は，ここで利用する前に 16 で割って（float に変換して）から利用する必要があります．
"_3dImage : Output 3-channel floating-point image of the same size as disparity. Each element of _3dImage(x,y) contains 3D coordinates of the point (x,y) computed from the disparity map. If one uses Q obtained by stereoRectify, then the returned points are represented in the first camera's rectified coordinate system.","_3dImage : 視差量と同じサイズの，3チャンネル浮動小数点型画像を出力します．_3dImage(x,y) の各要素には，視差マップから計算された点 (x,y) の3次元座標が入ります．stereoRectify で得られた Q を利用する場合，返される点は，1番目のカメラの平行化された座標系で表されます．"
Q : \(4 \times 4\) perspective transformation matrix that can be obtained with stereoRectify.,Q : stereoRectify で得られる，遠近感変換行列．
"handleMissingValues : Indicates, whether the function should handle missing values (i.e. points where the disparity was not computed). If handleMissingValues=true, then pixels with the minimal disparity that corresponds to the outliers (see StereoMatcher::compute ) are transformed to 3D points with a very large Z value (currently set to 10000).",handleMissingValues : この関数が，欠損値（つまり，視差が計算されなかった点）を扱うべきかどうかを示します．handleMissingValues=true の場合，外れ値（ StereoMatcher::compute を参照してください）に対応する最小の視差を持つピクセルは，非常に大きなZ値（現在は 10000 にセットされています）を持つ3次元点に変換されます．
"ddepth : The optional output array depth. If it is -1, the output image will have CV_32F depth. ddepth can also be set to CV_16S, CV_32S or CV_32F.","ddepth : オプションである出力配列のビット深度．これが -1 の場合，出力画像の深度は CV_32F になります． ddepth は， CV_16S, CV_32S, CV_32F に設定することもできます．"
"The function transforms a single-channel disparity map to a 3-channel image representing a 3D surface. That is, for each pixel (x,y) and the corresponding disparity d=disparity(x,y) , it computes:\[\begin{bmatrix} X \\ Y \\ Z \\ W \end{bmatrix} = Q \begin{bmatrix} x \\ y \\ \texttt{disparity} (x,y) \\ z \end{bmatrix}.\]See alsoTo reproject a sparse set of points {(x,y,d),...} to 3D space, use perspectiveTransform.","この関数は，シングルチャンネルの視差マップを，3次元表面を表す3チャンネルの画像に変換します．つまり，各ピクセル（x,y）と，それに対応する視差 d=disparity(x,y) に対して，次のように計算します：[˶‾᷄ -̫ ‾᷅˵][˶‾᷅˵][˶‾᷄ -̫ ‾᷄˵].Disparity d=disparity(x,y)を計算します。(x,y) \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\⁾⁾⁾⁾⁾⁾。"
Computes an optimal affine transformation between two 3D point sets.,2つの3次元点群の間で，最適なアフィン変換を計算します．
"src : First input 3D point set containing \((X,Y,Z)\).","src : 1番目に入力された，\((X,Y,Z)\)を含む3次元点群．"
"dst : Second input 3D point set containing \((x,y,z)\).","dst : ˶((x,y,z)˶)を含む 2 番目の 3 次元点群．"
out : Output 3D affine transformation matrix \(3 \times 4\) of the form ,out :出力される3次元アフィン変換行列は、次のような形式です。
\[ \begin{bmatrix} a_{11} & a_{12} & a_{13} & b_1\\ a_{21} & a_{22} & a_{23} & b_2\\ a_{31} & a_{32} & a_{33} & b_3\\ \end{bmatrix} \],\a_{11} & a_{12} & a_{13} & b_1\ a_{21} & a_{22} & a_{23} & b_2\ a_{31} & a_{32} & a_{33} & b_3\ end{bmatrix} ˶‾᷄ -̫ ‾᷅˵˵\]
"inliers : Output vector indicating which points are inliers (1-inlier, 0-outlier).","inliers : どの点がインライアであるかを示す出力ベクトル（1-inlier, 0-outlier）．"
ransacThreshold : Maximum reprojection error in the RANSAC algorithm to consider a point as an inlier.,ransacThreshold : ある点をインライアとみなすRANSACアルゴリズムの最大再投影誤差。
"confidence : Confidence level, between 0 and 1, for the estimated transformation. Anything between 0.95 and 0.99 is usually good enough. Values too close to 1 can slow down the estimation significantly. Values lower than 0.8-0.9 can result in an incorrectly estimated transformation.",confidence : 推定変換に対する0から1の間の信頼度．0.95から0.99の間であれば，通常は十分です．1に近すぎる値は，推定の速度を著しく低下させます．0.8から0.9よりも低い値では，正しくない推定変換が行われる可能性があります。
It computes\[ \begin{bmatrix} x\\ y\\ z\\ \end{bmatrix} = \begin{bmatrix} a_{11} & a_{12} & a_{13}\\ a_{21} & a_{22} & a_{23}\\ a_{31} & a_{32} & a_{33}\\ \end{bmatrix} \begin{bmatrix} X\\ Y\\ Z\\ \end{bmatrix} + \begin{bmatrix} b_1\\ b_2\\ b_3\\ \end{bmatrix} \]The function estimates an optimal 3D affine transformation between two 3D point sets using the RANSAC algorithm.,計算結果は、\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject reject r\♪♪～♪♪♪♪♪♪♪～+ b_1\\ b_2kPa\RANSACアルゴリズムを用いて、2つの3次元点群間の最適な3次元アフィン変換を推定する機能です。
Calculates the Sampson Distance between two points.,2つの点の間の Sampson Distance を計算します．
pt1 : first homogeneous 2d point,pt1 : 第1の同次2次元点
pt2 : second homogeneous 2d point,pt2 : 2 番目の同種の 2 次元点
F : fundamental matrix,F : 基本行列．
"The function cv::sampsonDistance calculates and returns the first order approximation of the geometric error as:\[ sd( \texttt{pt1} , \texttt{pt2} )= \frac{(\texttt{pt2}^t \cdot \texttt{F} \cdot \texttt{pt1})^2} {((\texttt{F} \cdot \texttt{pt1})(0))^2 + ((\texttt{F} \cdot \texttt{pt1})(1))^2 + ((\texttt{F}^t \cdot \texttt{pt2})(0))^2 + ((\texttt{F}^t \cdot \texttt{pt2})(1))^2} \]The fundamental matrix may be calculated using the findFundamentalMat function. See [103] 11.4.3 for details.","関数 cv::sampsonDistance は，幾何誤差の一次近似値を次のように計算して返します：˶[ sd( ˶ˆ꒳ˆ˵ ) , ˶ˆ꒳ˆ˵ ) = ˶ˆ꒳ˆ˵ ){((˶‾᷄꒫‾᷅˵)(0))^2 + ((˶‾᷅˵)(1))^2 + ((˶‾᷅˵)(0))^2 + ((˶‾᷄꒫‾᷄˵)(1))^2}.\基本行列は，findFundamentalMat関数を使って計算できます。詳細は，[103] 11.4.3 を参照してください．"
"Decompose a homography matrix to rotation(s), translation(s) and plane normal(s).",ホモグラフィ行列を，回転（複数），並進（複数），平面法線（複数）に分解します．
H : The input homography matrix between two images.,H : 2つの画像間の入力ホモグラフィ行列．
K : The input camera intrinsic matrix.,K : 入力されたカメラ固有の行列．
rotations : Array of rotation matrices.,rotations :回転行列の配列．
translations : Array of translation matrices.,並進 :並進行列の配列．
normals : Array of plane normal matrices.,normals :平面法線行列の配列。
"This function extracts relative camera motion between two views of a planar object and returns up to four mathematical solution tuples of rotation, translation, and plane normal. The decomposition of the homography matrix H is described in detail in [160].If the homography H, induced by the plane, gives the constraint\[s_i \vecthree{x'_i}{y'_i}{1} \sim H \vecthree{x_i}{y_i}{1}\]on the source image points \(p_i\) and the destination image points \(p'_i\), then the tuple of rotations[k] and translations[k] is a change of basis from the source camera's coordinate system to the destination camera's coordinate system. However, by decomposing H, one can only get the translation normalized by the (typically unknown) depth of the scene, i.e. its direction but with normalized length.If point correspondences are available, at least two solutions may further be invalidated, by applying positive depth constraint, i.e. all points must be in front of the camera.Examples: samples/cpp/tutorial_code/features2D/Homography/decompose_homography.cpp.",この関数は，平面物体の2つのビュー間の相対的なカメラの動きを抽出し，回転，並進，平面法線の最大4つの数学的解のタプルを返す．ホモグラフィ行列Hの分解については，[160]で詳しく述べられています．平面によって誘導されたホモグラフィHが、ソース画像点\(p_i\)とデスティネーション画像点\(p'_i\)に制約を与える場合、回転と平行移動のタプルを返します。とすると，回転[k]と並進[k]のタプルは，ソースカメラの座標系からデスティネーションカメラの座標系への基底の変更となる．ただし，H を分解することで，シーンの（通常は未知の）深度で正規化された並進，つまり，方向は正規化された長さでしか得られません．点の対応が得られる場合は，正の深度制約，つまり，すべての点がカメラの前になければならないという制約を適用することで，少なくとも2つの解がさらに無効になることがあります．
Filters homography decompositions based on additional information.,追加情報に基づいて、ホモグラフィーの分解をフィルタリングします。
rotations : Vector of rotation matrices.,rotations :回転行列のベクトル
normals : Vector of plane normal matrices.,normals :平面法線行列のベクトル
beforePoints : Vector of (rectified) visible reference points before the homography is applied,beforePoints :ホモグラフィー適用前の（修正された）可視参照点のベクトル
afterPoints : Vector of (rectified) visible reference points after the homography is applied,afterPoints :ホモグラフィーが適用された後の（平行化された）可視参照点のベクトル
possibleSolutions : Vector of int indices representing the viable solution set after filtering,possibleSolutions :フィルタリング後の実行可能なソリューションセットを表すint型インデックスのベクトル
pointsMask : optional Mat/Vector of 8u type representing the mask for the inliers as given by the findHomography function,pointsMask : findHomography 関数によって与えられるインライアのマスクを表す，8u 型の Mat/Vector（オプション）．
"This function is intended to filter the output of the decomposeHomographyMat based on additional information as described in [160] . The summary of the method: the decomposeHomographyMat function returns 2 unique solutions and their ""opposites"" for a total of 4 solutions. If we have access to the sets of points visible in the camera frame before and after the homography transformation is applied, we can determine which are the true potential solutions and which are the opposites by verifying which homographies are consistent with all visible reference points being in front of the camera. The inputs are left unchanged; the filtered solution set is returned as indices into the existing one.",この関数は， decomposeHomographyMat の出力を， [160] で述べられているような追加情報に基づいてフィルタリングするためのものです．このメソッドの概要： decomposeHomographyMat 関数は，2つのユニークな解とその「反対」の解，合計4つの解を返します．ホモグラフィー変換の適用前後で，カメラフレーム内に見える点の集合にアクセスできるならば，どのホモグラフィーが，すべての可視参照点がカメラの前にあることと矛盾しないかを検証することで，どれが真の潜在的な解で，どれが反対の解かを決定することができます．入力は変更されません。フィルタリングされた解答セットは、既存の解答セットへのインデックスとして返されます。
Transforms an image to compensate for lens distortion.,レンズの歪みを補正するために，画像を変換します．
src : Input (distorted) image.,src : 入力（歪んだ）画像．
dst : Output (corrected) image that has the same size and type as src .,dst : src と同じサイズ，同じ種類の出力（補正された）画像．
cameraMatrix : Input camera matrix \(A = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}\) .,cameraMatrix :Input camera matrix ˶ˆ꒳ˆ˵ (A = ˶ˆ꒳ˆ˵) .
"distCoeffs : Input vector of distortion coefficients \((k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6[, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\) of 4, 5, 8, 12 or 14 elements. If the vector is NULL/empty, the zero distortion coefficients are assumed.","distCoeffs : 入力された，4，5，8，12，14個の要素を持つ歪み係数\((k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6[, s_1, s_2, s_3, s_4[, ˶˙º̬˙˶]]]])のベクトル．ベクトルがNULL/空の場合は，歪み係数が0であると仮定される．"
"newCameraMatrix : Camera matrix of the distorted image. By default, it is the same as cameraMatrix but you may additionally scale and shift the result by using a different matrix.",newCameraMatrix :歪み補正された画像のカメラ行列．デフォルトでは， cameraMatrix と同じですが，さらに別の行列を利用して，結果をスケーリングしたりシフトしたりすることができます．
"The function transforms an image to compensate radial and tangential lens distortion.The function is simply a combination of initUndistortRectifyMap (with unity R ) and remap (with bilinear interpolation). See the former function for details of the transformation being performed.Those pixels in the destination image, for which there is no correspondent pixels in the source image, are filled with zeros (black color).A particular subset of the source image that will be visible in the corrected image can be regulated by newCameraMatrix. You can use getOptimalNewCameraMatrix to compute the appropriate newCameraMatrix depending on your requirements.The camera matrix and the distortion parameters can be determined using calibrateCamera. If the resolution of images is different from the resolution used at the calibration stage, \(f_x, f_y, c_x\) and \(c_y\) need to be scaled accordingly, while the distortion coefficients remain the same.",この関数は，半径方向と接線方向のレンズ歪みを補正するために画像を変換します．この関数は，単に initUndistortRectifyMap（ユニティーR付き）と remap（バイリニア補間付き）を組み合わせたものです．この関数は， initUndistortRectifyMap (with unity R ) と remap (with bilinear interpolation) を単純に組み合わせたものです．実行される変換の詳細については，前者の関数を参照してください．出力画像の中で，ソース画像に対応するピクセルが存在しないピクセルは，ゼロ（黒色）で埋められます．getOptimalNewCameraMatrix を使えば，必要に応じて適切な newCameraMatrix を計算することができます．カメラ行列と歪みパラメータは， calibrateCamera を使って決定することができます．画像の解像度がキャリブレーションの段階で使用された解像度と異なる場合，歪み係数はそのままで，それに応じて ˶ˆ꒳ˆ˵ / ˶ˆ꒳ˆ˵ / ˶ˆ꒳ˆ˵
Computes the undistortion and rectification transformation map.,歪み補正と平行化の変換マップを計算します．
cameraMatrix : Input camera matrix \(A=\vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}\) .,cameraMatrix :入力カメラ行列．
"R : Optional rectification transformation in the object space (3x3 matrix). R1 or R2 , computed by stereoRectify can be passed here. If the matrix is empty, the identity transformation is assumed. In cvInitUndistortMap R assumed to be an identity matrix.",R : オプションである，物体空間における平行化変換（3x3 の行列）．stereoRectify によって計算された R1 または R2 をここに渡すことができます．この行列が空の場合は，恒等変換が仮定されます．cvInitUndistortMap では，R は恒等行列であると仮定されます．
newCameraMatrix : New camera matrix \(A'=\vecthreethree{f_x'}{0}{c_x'}{0}{f_y'}{c_y'}{0}{0}{1}\).,newCameraMatrix :new camera matrix ˶‾᷄ -̫ ‾᷅˵ A'=\\{f_x'}{0}{c_x'}{0}{f_y'}{c_y'}{0}{0}{1}˶}.
size : Undistorted image size.,size : 歪んでいない画像サイズ
"m1type : Type of the first output map that can be CV_32FC1, CV_32FC2 or CV_16SC2, see convertMaps","m1type :CV_32FC1, CV_32FC2, CV_16SC2 のいずれかで， convertMaps を参照してください．"
map1 : The first output map.,map1 : 1番目に出力されるマップ．
map2 : The second output map.,map2 : 2番目に出力されるマップ．
"The function computes the joint undistortion and rectification transformation and represents the result in the form of maps for remap. The undistorted image looks like original, as if it is captured with a camera using the camera matrix =newCameraMatrix and zero distortion. In case of a monocular camera, newCameraMatrix is usually equal to cameraMatrix, or it can be computed by getOptimalNewCameraMatrix for a better control over scaling. In case of a stereo camera, newCameraMatrix is normally set to P1 or P2 computed by stereoRectify .Also, this new camera is oriented differently in the coordinate space, according to R. That, for example, helps to align two heads of a stereo camera so that the epipolar lines on both images become horizontal and have the same y- coordinate (in case of a horizontally aligned stereo camera).The function actually builds the maps for the inverse mapping algorithm that is used by remap. That is, for each pixel \((u, v)\) in the destination (corrected and rectified) image, the function computes the corresponding coordinates in the source image (that is, in the original image from camera). The following process is applied:\[ \begin{array}{l} x \leftarrow (u - {c'}_x)/{f'}_x \\ y \leftarrow (v - {c'}_y)/{f'}_y \\ {[X\,Y\,W]} ^T \leftarrow R^{-1}*[x \, y \, 1]^T \\ x' \leftarrow X/W \\ y' \leftarrow Y/W \\ r^2 \leftarrow x'^2 + y'^2 \\ x'' \leftarrow x' \frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + 2p_1 x' y' + p_2(r^2 + 2 x'^2) + s_1 r^2 + s_2 r^4\\ y'' \leftarrow y' \frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + p_1 (r^2 + 2 y'^2) + 2 p_2 x' y' + s_3 r^2 + s_4 r^4 \\ s\vecthree{x'''}{y'''}{1} = \vecthreethree{R_{33}(\tau_x, \tau_y)}{0}{-R_{13}((\tau_x, \tau_y)} {0}{R_{33}(\tau_x, \tau_y)}{-R_{23}(\tau_x, \tau_y)} {0}{0}{1} R(\tau_x, \tau_y) \vecthree{x''}{y''}{1}\\ map_x(u,v) \leftarrow x''' f_x + c_x \\ map_y(u,v) \leftarrow y''' f_y + c_y \end{array} \]where \((k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6[, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])\) are the distortion coefficients.In case of a stereo camera, this function is called twice: once for each camera head, after stereoRectify, which in its turn is called after stereoCalibrate. But if the stereo camera was not calibrated, it is still possible to compute the rectification transformations directly from the fundamental matrix using stereoRectifyUncalibrated. For each camera, the function computes homography H as the rectification transformation in a pixel domain, not a rotation matrix R in 3D space. R can be computed from H as\[\texttt{R} = \texttt{cameraMatrix} ^{-1} \cdot \texttt{H} \cdot \texttt{cameraMatrix}\]where cameraMatrix can be chosen arbitrarily.","この関数は，歪み補正と平行化の変換を合同で計算し，その結果をリマップ用のマップの形で表現します．歪みのない画像は，カメラ行列 =newCameraMatrix を用いたカメラで撮影されたかのように，元の画像と同じように見え，歪みもありません．単眼カメラの場合，newCameraMatrix は通常 cameraMatrix と等しくなりますが，スケーリングをより正確に制御するために， getOptimalNewCameraMatrix によって計算することもできます．ステレオカメラの場合，newCameraMatrix は通常，stereoRectify によって求められた P1 または P2 に設定されます．また，この新しいカメラは，R に従って，座標空間内で異なる向きに配置されます．例えば，ステレオカメラの2つのヘッドを調整して，両方の画像のエピポーラ線が水平になり，同じ y-座標を持つようにすることができます（水平に配置されたステレオカメラの場合）．つまり，この関数は，補正・平行化された出力画像の各ピクセル ˶((u, v)˶))に対して，元画像（つまり，カメラから出力されたオリジナル画像）における対応する座標を計算します．このような処理を行います。^T ardarrow R^{-1}*[x ˶, y˶,1]^T ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ !!!!+ 2p_1 x' y' + p_2(r^2 + 2 x'^2) + s_1 r^2 + s_2 r^4\ y'' ୨୧{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6}。+ p_1 (r^2 + 2 y'^2) + 2 p_2 x' y' + s_3 r^2 + s_4 r^4 ˶ˆ꒳ˆ˵ = ˶ˆ꒳ˆ˵{R_{33}(˶ˆ꒳ˆ˵)}{0}{R_{13}((˶ˆ꒳ˆ˵)}){0}{R_{33}(˶‾᷄ -̫ ‾᷅˵)}{R_{23}(˶‾᷅˵)}{0}{0}{1}R(\tau_x, \tau_y) \vecthree{x''}{y''}{1}\ map_x(u,v) ˶ˆ꒳ˆ˵ ˶ˆ꒳ˆ˵ ˶ˆ꒳ˆ˵ ˶ˆ꒳ˆ˵ ˶ˆ꒳ˆ˵\ステレオカメラの場合，この関数は，各カメラヘッドに対して，stereoRectify の後，stereoCalibrate の後に 2 回呼び出されます．しかし，ステレオカメラがキャリブレーションされていない場合は， stereoRectifyUncalibrated を用いて，基本行列から直接，平行化変換を計算することができます．各カメラに対して，この関数は，3次元空間の回転行列 R ではなく，ピクセル領域の平行化変換としてホモグラフィ H を求めます．R は，H から次のようにして求められます：[˶‾᷄ -̫ ‾᷅˵] = ˶‾᷄ -̫ ‾᷄˵ )^{-1} ˶ˆ꒳ˆ˵ )\ここで，CameraMatrix は任意に選択できます．"
initializes maps for remap for wide-angle,広角用のリマップのために，マップを初期化します．
Returns the default new camera matrix.,デフォルトの新しいカメラ行列を返します．
cameraMatrix : Input camera matrix.,cameraMatrix :入力カメラ行列．
imgsize : Camera view image size in pixels.,imgsize : カメラビューの画像サイズ（ピクセル単位）．
centerPrincipalPoint : Location of the principal point in the new camera matrix. The parameter indicates whether this location should be at the image center or not.,centerPrincipalPoint : 新しいカメラ行列における主点の位置．このパラメータは，この位置が画像の中心にあるべきかどうかを示します．
"The function returns the camera matrix that is either an exact copy of the input cameraMatrix (when centerPrinicipalPoint=false ), or the modified one (when centerPrincipalPoint=true).In the latter case, the new camera matrix will be:\[\begin{bmatrix} f_x && 0 && ( \texttt{imgSize.width} -1)*0.5 \\ 0 && f_y && ( \texttt{imgSize.height} -1)*0.5 \\ 0 && 0 && 1 \end{bmatrix} ,\]where \(f_x\) and \(f_y\) are \((0,0)\) and \((1,1)\) elements of cameraMatrix, respectively.By default, the undistortion functions in OpenCV (see initUndistortRectifyMap, undistort) do not move the principal point. However, when you work with stereo, it is important to move the principal points in both views to the same y-coordinate (which is required by most of stereo correspondence algorithms), and may be to the same x-coordinate too. So, you can form the new camera matrix for each view where the principal points are located at the center.","この関数は，入力された cameraMatrix の完全なコピー（centerPrinicipalPoint=false の場合），あるいは修正されたカメラ行列（centerPrincipalPoint=true の場合）を返します．後者の場合，新しいカメラ行列は次のようになります：˶‾᷄ -̫ ‾᷅˵ f_x && 0 && ( ˶‾᷄ -̫ ‾᷄˵ ) 0 && f_y && ( ˶‾᷄ -̫ ‾᷄˵ ) 0 && 0 && 1 ˶‾᷅˵デフォルトでは，OpenCV の歪み補正関数（ initUndistortRectifyMap, undistort 参照）は，主点を移動させません．しかし，ステレオを扱う場合，両方のビューの主点を同じ y 座標に移動させることが重要であり（これは，ほとんどのステレオ対応点探索アルゴリズムで要求されます），同じ x 座標に移動させることもあります．そこで、各ビューに対して、主点が中心に位置する新しいカメラ行列を形成します。"
Computes the ideal point coordinates from the observed point coordinates.,観測された点座標から，理想的な点座標を計算します．
"src : Observed point coordinates, 2xN/Nx2 1-channel or 1xN/Nx1 2-channel (CV_32FC2 or CV_64FC2) (or vector<Point2f> ).",src : 観測された点座標，2xN/Nx2 1-channel または 1xN/Nx1 2-channel (CV_32FC2 または CV_64FC2) （または vector<Point2f> ）．
"dst : Output ideal point coordinates (1xN/Nx1 2-channel or vector<Point2f> ) after undistortion and reverse perspective transformation. If matrix P is identity or omitted, dst will contain normalized point coordinates.",dst : 歪み補正と逆透視変換を行った理想的な点座標（1xN/Nx1 2ch または vector<Point2f> ）を出力します．行列 P が identity または省略された場合，dst には正規化された点座標が出力されます．
cameraMatrix : Camera matrix \(\vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}\) .,cameraMatrix :CameraMatrix : カメラ行列 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ !
"R : Rectification transformation in the object space (3x3 matrix). R1 or R2 computed by stereoRectify can be passed here. If the matrix is empty, the identity transformation is used.",R : オブジェクト空間での平行移動（3x3 の行列）．stereoRectify によって計算された R1 または R2 をここに渡すことができます．この行列が空の場合は，恒等変換が利用されます．
"P : New camera matrix (3x3) or new projection matrix (3x4) \(\begin{bmatrix} {f'}_x & 0 & {c'}_x & t_x \\ 0 & {f'}_y & {c'}_y & t_y \\ 0 & 0 & 1 & t_z \end{bmatrix}\). P1 or P2 computed by stereoRectify can be passed here. If the matrix is empty, the identity new camera matrix is used.",P : 新しいカメラ行列（3x3），または新しい投影行列（3x4）．{0 & {f'}_y & {c'}_y & t_y 0 & 0 & 1 & t_z 0 & 0 & 1 & t_z 0 & end{bmatrix}\)。stereoRectify で計算された P1 や P2 をここに渡すことができます．この行列が空の場合は，恒等式の新しいカメラ行列が用いられます．
"The function is similar to undistort and initUndistortRectifyMap but it operates on a sparse set of points instead of a raster image. Also the function performs a reverse transformation to projectPoints. In case of a 3D object, it does not reconstruct its 3D coordinates, but for a planar object, it does, up to a translation vector, if the proper R is specified.For each observed point coordinate \((u, v)\) the function computes:\[ \begin{array}{l} x^{""} \leftarrow (u - c_x)/f_x \\ y^{""} \leftarrow (v - c_y)/f_y \\ (x',y') = undistort(x^{""},y^{""}, \texttt{distCoeffs}) \\ {[X\,Y\,W]} ^T \leftarrow R*[x' \, y' \, 1]^T \\ x \leftarrow X/W \\ y \leftarrow Y/W \\ \text{only performed if P is specified:} \\ u' \leftarrow x {f'}_x + {c'}_x \\ v' \leftarrow y {f'}_y + {c'}_y \end{array} \]where undistort is an approximate iterative algorithm that estimates the normalized original point coordinates out of the normalized distorted point coordinates (""normalized"" means that the coordinates do not depend on the camera matrix).The function can be used for both a stereo camera head or a monocular camera (when R is empty).Examples: samples/cpp/tutorial_code/features2D/Homography/pose_from_homography.cpp.","この関数は， undistort および initUndistortRectifyMap に似ていますが，ラスタ画像ではなく，疎な点の集合を操作します．また，この関数は，projectPoints とは逆の変換を行います．3次元物体の場合は，その3次元座標を再構成しませんが，平面物体の場合は，適切なRが指定されていれば，並進ベクトルまで再構成します．この関数は，観測された各点座標 ˶((u, v)˶)に対して，次のように計算します\y^{""}」となります。♪\\\\\\\\\\\\^T ^T ^T ^T ^T ^T ^T ^T ^T ^T ^T ^T ^T ^T ^T ^T ^T ^T ^T ^T ^T ^T ^T ^T ^T ^T ^T ^T ^T ^T ^T ^T ^T ^T ^T ^T ^T ^T ^T ^T ^T ^T ^T ^T ^T ^T ^T ^T ^T ^T ^T ^T ^T ^T ^T ^T ^T\\ ¶ u' ¶ leftarrow x {f'}_x + {c'}_x ¶ v' ¶ leftarrow y {f'}_y + {c'}_y ¶end{array}.\ここで，undistort は，正規化された歪んだ点座標から，正規化された元の点座標を推定する近似的な反復アルゴリズムです（「正規化」とは，座標がカメラ行列に依存しないことを意味します）．"
"Recovers the relative camera rotation and the translation from an estimated essential matrix and the corresponding points in two images, using cheirality check. Returns the number of inliers that pass the check.",推定された本質的な行列と，それに対応する2つの画像中の点から，カメラの相対的な回転と並進を，不正性のチェックを用いて復元します．このチェックに合格したインライアの数を返します．
E : The input essential matrix.,E : 入力される必須行列．
points1 : Array of N 2D points from the first image. The point coordinates should be floating-point (single or double precision).,points1 : 1枚目の画像から得られるN個の2次元点の配列．点の座標は，浮動小数点型（単精度または倍精度）でなければいけません．
cameraMatrix : Camera intrinsic matrix \(\cameramatrix{A}\) . Note that this function assumes that points1 and points2 are feature points from cameras with the same camera intrinsic matrix.,cameraMatrix :カメラの本質的な行列．なお，この関数は，point1 と points2 が，同じカメラ固有の行列を持つカメラの特徴点であることを仮定しています．
"R : Output rotation matrix. Together with the translation vector, this matrix makes up a tuple that performs a change of basis from the first camera's coordinate system to the second camera's coordinate system. Note that, in general, t can not be used for this tuple, see the parameter described below.",R : 出力される回転行列．この行列は，並進ベクトルと合わせて，1台目のカメラの座標系から2台目のカメラの座標系への基底変更を行うタプルを構成します．なお、このタプルには一般的にtは使用できませんので、後述のパラメータを参照してください。
"t : Output translation vector. This vector is obtained by decomposeEssentialMat and therefore is only known up to scale, i.e. t is the direction of the translation vector and has unit length.",t : 出力並進ベクトル．このベクトルは， decomposeEssentialMat によって得られるので，スケールまでしかわかりません．つまり，t は並進ベクトルの方向であり，単位長さを持ちます．
"mask : Input/output mask for inliers in points1 and points2. If it is not empty, then it marks inliers in points1 and points2 for then given essential matrix E. Only these inliers will be used to recover pose. In the output mask only inliers which pass the cheirality check.",mask : points1 と points2 のインライアに対する入出力マスク．これが空でなければ，与えられた必須行列Eに対するpoints1とpoints2のインライアをマークします．これらのインライアのみが，ポーズの復元に利用されます．出力マスクには，不正性チェックを通過したインライアのみが表示されます．
"This function decomposes an essential matrix using decomposeEssentialMat and then verifies possible pose hypotheses by doing cheirality check. The cheirality check means that the triangulated 3D points should have positive depth. Some details can be found in [186].This function can be used to process the output E and mask from findEssentialMat. In this scenario, points1 and points2 are the same input for findEssentialMat :// Example. Estimation of fundamental matrix using the RANSAC algorithmint point_count = 100;vector<Point2f> points1(point_count);vector<Point2f> points2(point_count);// initialize the points here ...for( int i = 0; i < point_count; i++ ){    points1[i] = ...;    points2[i] = ...;}// cametra matrix with both focal lengths = 1, and principal point = (0, 0)Mat cameraMatrix = Mat::eye(3, 3, CV_64F);Mat E, R, t, mask;E = findEssentialMat(points1, points2, cameraMatrix, RANSAC, 0.999, 1.0, mask);recoverPose(E, points1, points2, cameraMatrix, R, t, mask);fragment","この関数は， decomposeEssentialMat を用いて本質的な行列を分解した後，頬性チェックを行うことで，可能性のあるポーズの仮説を検証します．チアラリティチェックとは，三角形化された3次元点が正の深度を持つことを意味します．この関数は， findEssentialMat からの出力 E と mask を処理するために利用できます．このシナリオでは，point1 と points2 は findEssentialMat の同じ入力です： // 例．RANSAC アルゴリズムを用いた基本行列の推定int point_count = 100;vector<Point2f> points1(point_count);vector<Point2f> points2(point_count);// ここでポイントを初期化します ...for( int i = 0; i < point_count; i++ ){ points1[i] = ...; points2[i] = ....;}// 両方の焦点距離 = 1, 主点 = (0, 0)Mat cameraMatrix = Mat::eye(3, 3, CV_64F);Mat E, R, t, mask;E = findEssentialMat(points1, points2, cameraMatrix, RANSAC, 0.999, 1.0, mask);recoverPose(E, points1, points2, cameraMatrix, R, t, mask);fragment"
"This is an overloaded member function, provided for convenience. It differs from the above function only in what argument(s) it accepts.",この関数は，便宜上，オーバーロードされたメンバ関数です．上の関数との違いは，どのような引数を受け取るかだけです．
"R : Output rotation matrix. Together with the translation vector, this matrix makes up a tuple that performs a change of basis from the first camera's coordinate system to the second camera's coordinate system. Note that, in general, t can not be used for this tuple, see the parameter description below.",R : 出力される回転行列．この行列は，並進ベクトルとともに，第1のカメラの座標系から第2のカメラの座標系への基底変更を行うタプルを構成する．なお，このタプルには一般的にtは使用できません．
focal : Focal length of the camera. Note that this function assumes that points1 and points2 are feature points from cameras with same focal length and principal point.,focal :カメラの焦点距離．なお，この関数は，point1 と points2 が，同じ焦点距離と主点を持つカメラの特徴点であることを前提としています．
pp : principal point of the camera.,pp : カメラの主点．
This function differs from the one above that it computes camera intrinsic matrix from focal length and principal point:\[A = \begin{bmatrix} f & 0 & x_{pp} \\ 0 & f & y_{pp} \\ 0 & 0 & 1 \end{bmatrix}\],この関数は，上述の関数とは異なり，焦点距離と主点からカメラの固有マトリクスを求めます： ˶‾᷄ -̫ ‾᷅˵ A = ˶‾᷅˵ f & 0 & x_{pp} ˶‾᷅˵ A = ˶‾᷄ -̫ ‾᷄˵\\ 0 & f & y_{pp}\\ 0 & 0 & 1 ″end{bmatrix}\ ″となります。］
points2 : Array of the second image points of the same size and format as points1.,points2 ： points1 と同じサイズ，フォーマットの 2 番目の画像ポイントの配列．
distanceThresh : threshold distance which is used to filter out far away points (i.e. infinite points).,distanceThresh : 遠くにある点（無限遠点）を除外するために用いられる閾値距離．
triangulatedPoints : 3D points which were reconstructed by triangulation.,triangulatedPoints :三角測量によって再構成された3次元点．
This function differs from the one above that it outputs the triangulated 3D point that are used for the cheirality check.,この関数は，上記の関数とは異なり，不正性のチェックに使用される三角測量された3次元点を出力します．
Calculates an essential matrix from the corresponding points in two images.,2つの画像中の対応する点から，必須行列を計算します．
points1 : Array of N (N >= 5) 2D points from the first image. The point coordinates should be floating-point (single or double precision).,points1 : 1枚目の画像から得られる，N (N >= 5) 個の2次元点の配列．点の座標は，浮動小数点型（単精度または倍精度）でなければいけません．
"cameraMatrix : Camera intrinsic matrix \(\cameramatrix{A}\) . Note that this function assumes that points1 and points2 are feature points from cameras with the same camera intrinsic matrix. If this assumption does not hold for your use case, use undistortPoints with P = cv::NoArray() for both cameras to transform image points to normalized image coordinates, which are valid for the identity camera intrinsic matrix. When passing these coordinates, pass the identity matrix for this parameter.",cameraMatrix :カメラ固有の行列．なお，この関数は，point1 と points2 が，同じカメラ固有の行列を持つカメラの特徴点であることを仮定しています．この仮定が成り立たない場合は，両方のカメラに対して P = cv::NoArray() の undistortPoints を用いて，画像上の点を正規化された画像座標に変換します．これは，カメラの内部行列が同一の場合に有効です．これらの座標を渡す場合，このパラメータには，単位行列を渡します．
method : Method for computing an essential matrix.,method : 固有マトリクスを計算するためのメソッド．
RANSAC for the RANSAC algorithm.,RANSAC : RANSACアルゴリズムのためのRANSAC．
LMEDS for the LMedS algorithm.,LMEDS : LMedSアルゴリズムのためのLMEDS．
prob : Parameter used for the RANSAC or LMedS methods only. It specifies a desirable level of confidence (probability) that the estimated matrix is correct.,prob : RANSACまたはLMedS法にのみ使用されるパラメータ．推定された行列が正しいことを確信する望ましいレベル（確率）を指定します．
"threshold : Parameter used for RANSAC. It is the maximum distance from a point to an epipolar line in pixels, beyond which the point is considered an outlier and is not used for computing the final fundamental matrix. It can be set to something like 1-3, depending on the accuracy of the point localization, image resolution, and the image noise.",threshold : RANSACで使用されるパラメータ．点からエピポーラ線までの最大距離をピクセル単位で表したもので，これを超えるとその点は外れ値とみなされ，最終的な基本行列の計算には用いられません．これは，点の位置決めの精度，画像の解像度，画像のノイズなどに応じて，1〜3程度に設定できます．
"mask : Output array of N elements, every element of which is set to 0 for outliers and to 1 for the other points. The array is computed only in the RANSAC and LMedS methods.",mask : N 個の要素からなる出力配列．この配列の各要素は，外れ値に対しては 0 に，その他の点に対しては 1 に設定されます．この配列は，RANSAC 法と LMedS 法でのみ計算されます．
"This function estimates essential matrix based on the five-point algorithm solver in [186] . [228] is also a related. The epipolar geometry is described by the following equation:\[[p_2; 1]^T K^{-T} E K^{-1} [p_1; 1] = 0\]where \(E\) is an essential matrix, \(p_1\) and \(p_2\) are corresponding points in the first and the second images, respectively. The result of this function may be passed further to decomposeEssentialMat or recoverPose to recover the relative pose between cameras.",この関数は，[186]の5点アルゴリズムソルバーに基づいて，必須行列を推定します．また，[228]も関連しています．エピポーラ形状は，次の式で表されます： ˶[[p_2; 1]^T K^{-T}].E K^{-1} [p_1; 1] = 0\]ここで，\(E\)は本質的な行列で，\(p_1)，\(p_2)はそれぞれ1枚目と2枚目の画像の対応点です．この関数の結果は，カメラ間の相対的な姿勢を復元するために，さらに decomposeEssentialMat や recoverPose に渡すことができます．
Projects points using fisheye model.,魚眼モデルを用いてポイントを投影します．
"objectPoints : Array of object points, 1xN/Nx1 3-channel (or vector<Point3f> ), where N is the number of points in the view.",objectPoints :1xN/Nx1 3-channel（あるいは vector<Point3f> ）のオブジェクトポイントの配列．
"imagePoints : Output array of image points, 2xN/Nx2 1-channel or 1xN/Nx1 2-channel, or vector<Point2f>.",imagePoints :2xN/Nx2 の 1 チャンネル，あるいは 1xN/Nx1 の 2 チャンネル，または vector<Point2f> で表される，画像点の出力配列．
K : Camera intrinsic matrix \(cameramatrix{K}\).,K : カメラ固有の行列 ˶ˆ꒳ˆ˵ (cameramatrix{K}˶)
D : Input vector of distortion coefficients \(\distcoeffsfisheye\).,D : 入力された歪曲係数のベクトルです。
alpha : The skew coefficient.,alpha : スキュー係数です。
"jacobian : Optional output 2Nx15 jacobian matrix of derivatives of image points with respect to components of the focal lengths, coordinates of the principal point, distortion coefficients, rotation vector, translation vector, and the skew. In the old interface different components of the jacobian are returned via different output parameters.",jacobian : オプションで，焦点距離，主点の座標，歪み係数，回転ベクトル，並進ベクトル，スキューの各成分に対する画像点の微分の2Nx15ヤコビアン行列を出力します．従来のインターフェースでは，ヤコビ行列の異なる成分が，異なる出力パラメータによって返されていました．
"The function computes projections of 3D points to the image plane given intrinsic and extrinsic camera parameters. Optionally, the function computes Jacobians - matrices of partial derivatives of image points coordinates (as functions of all the input parameters) with respect to the particular parameters, intrinsic and/or extrinsic.",この関数は，カメラの内部および外部パラメータを与えて，3次元点の画像平面への投影を求めます．オプションとして，この関数は，（すべての入力パラメータの関数としての）画像点座標の偏微分の行列であるヤコビアンを，特定のパラメータ（内在および外在）に関して計算します．
Distorts 2D points using fisheye model.,魚眼モデルを用いて，2次元点を歪めます．
"undistorted : Array of object points, 1xN/Nx1 2-channel (or vector<Point2f> ), where N is the number of points in the view.",非歪曲．1xN/Nx1 2チャンネルのオブジェクトポイントの配列（または vector<Point2f> ），ここで N はビュー内のポイント数．
"distorted : Output array of image points, 1xN/Nx1 2-channel, or vector<Point2f> .",distorted :1xN/Nx1 2-channel（または vector<Point2f> ）の画像点の出力配列．
Note that the function assumes the camera intrinsic matrix of the undistorted points to be identity. This means if you want to transform back points undistorted with fisheye::undistortPoints you have to multiply them with \(P^{-1}\).,この関数は，歪んでいない点のカメラ固有の行列を，恒等式と仮定していることに注意してください．つまり，fisheye::undistortPoints によって歪められていない点を元に戻したい場合は，それらの点に ˶˙º̬˙˶ を掛けなければいけません．
Undistorts 2D points using fisheye model.,魚眼モデルを使って2Dポイントの歪みを補正します。
"distorted : Array of object points, 1xN/Nx1 2-channel (or vector<Point2f> ), where N is the number of points in the view.",distorted :1xN/Nx1 2チャンネルのオブジェクトポイントの配列（または vector<Point2f> ），ここで N はビュー内のポイント数です．
"R : Rectification transformation in the object space: 3x3 1-channel, or vector: 3x1/1x3 1-channel or 1x1 3-channel",R : 物体空間における射影変換：3x3 1チャンネル，またはベクトル．3x1/1x3 1ch または 1x1 3ch
P : New camera intrinsic matrix (3x3) or new projection matrix (3x4),P : 新しいカメラ固有マトリクス（3x3）または新しいプロジェクションマトリクス（3x4）．
"undistorted : Output array of image points, 1xN/Nx1 2-channel, or vector<Point2f> .",undistorted :画像点の出力配列，1xN/Nx1 2-channel，または vector<Point2f> ．
"Computes undistortion and rectification maps for image transform by remap. If D is empty zero distortion is used, if R or P is empty identity matrixes are used.",remap による画像変換に対して，歪み補正マップと平行化マップを計算します．D が空の場合は，ゼロディストーションが利用され，R または P が空の場合は，単位行列が利用されます．
m1type : Type of the first output map that can be CV_32FC1 or CV_16SC2 . See convertMaps for details.,m1type :CV_32FC1 または CV_16SC2 である，最初の出力マップの種類．詳細は， convertMaps を参照してください．
Transforms an image to compensate for fisheye lens distortion.,魚眼レンズの歪みを補正するために，画像を変換します．
distorted : image with fisheye lens distortion.,distorted : 魚眼レンズの歪みを持つ画像．
undistorted : Output image with compensated fisheye lens distortion.,undistorted :魚眼レンズの歪みが補正された出力画像．
"Knew : Camera intrinsic matrix of the distorted image. By default, it is the identity matrix but you may additionally scale and shift the result by using a different matrix.",Knew : 歪んだ画像のカメラ固有の行列．デフォルトでは，これは単位行列ですが，追加で別の行列を使って結果を拡大・縮小することもできます．
new_size : the new size,new_size : 新しいサイズ
"The function transforms an image to compensate radial and tangential lens distortion.The function is simply a combination of fisheye::initUndistortRectifyMap (with unity R ) and remap (with bilinear interpolation). See the former function for details of the transformation being performed.See below the results of undistortImage.a) result of undistort of perspective camera model (all possible coefficients (k_1, k_2, k_3, k_4, k_5, k_6) of distortion were optimized under calibration)",この関数は，半径方向と接線方向のレンズ歪みを補正するために画像を変換します．この関数は，単純に fisheye::initUndistortRectifyMap（ユニティーR付き）と remap（バイリニア補間付き）を組み合わせたものです．この関数は，fisheye: initUndistortRectifyMap (with unity R ) と remap (with bilar interpolation) を単純に組み合わせたもので，実行される変換の詳細については，前者の関数を参照してください．undistortImageの結果は以下のとおりです．）
"b) result of fisheye::undistortImage of fisheye camera model (all possible coefficients (k_1, k_2, k_3, k_4) of fisheye distortion were optimized under calibration)","b) 魚眼カメラモデルの fisheye::undistortImage の結果 (魚眼レンズの歪みのすべての係数 (k_1, k_2, k_3, k_4) がキャリブレーションにより最適化されている)"
"c) original image was captured with fisheye lensPictures a) and b) almost the same. But if we consider points of image located far from the center of image, we can notice that on image a) these points are distorted.image",c) 元の画像を魚眼レンズで撮影したもの写真a)とb)はほぼ同じです。しかし、画像の中心から離れた場所にある画像の点を考慮すると、画像a)ではこれらの点が歪んでいることがわかります。
Estimates new camera intrinsic matrix for undistortion or rectification.,歪み補正や平行化のために，新しいカメラ固有の行列を推定します．
image_size : Size of the image,image_size : 画像のサイズ
"balance : Sets the new focal length in range between the min focal length and the max focal length. Balance is in range of [0, 1].","balance : 新しい焦点距離を、最小焦点距離と最大焦点距離の間で設定します。balanceは[0, 1]の範囲で指定します。"
fov_scale : Divisor for new focal length.,fov_scale : 新しい焦点距離の除算値です。
Performs camera calibaration.,カメラのキャリブレーションを行います。
objectPoints : vector of vectors of calibration pattern points in the calibration pattern coordinate space.,objectPoints : キャリブレーションパターン座標空間におけるキャリブレーションパターンポイントのベクトル
imagePoints : vector of vectors of the projections of calibration pattern points. imagePoints.size() and objectPoints.size() and imagePoints[i].size() must be equal to objectPoints[i].size() for each i.,imagePoints : キャリブレーションパターンの点を投影したベクトル． imagePoints.size() と objectPoints.size() および imagePoints[i].size() は，各 i について objectPoints[i].size() と等しくなければならない．
image_size : Size of the image used only to initialize the camera intrinsic matrix.,image_size : カメラ固有の行列を初期化するためにのみ利用される画像のサイズ．
"K : Output 3x3 floating-point camera intrinsic matrix \(\cameramatrix{A}\) . If fisheye::CALIB_USE_INTRINSIC_GUESS is specified, some or all of fx, fy, cx, cy must be initialized before calling the function.","K : 出力される 3x3 浮動小数点型のカメラ内部マトリックス ˶ˆ꒳ˆ˵ ) .fisheye::CALIB_USE_INTRINSIC_GUESS が指定されている場合，この関数を呼び出す前に fx, fy, cx, cy の一部または全部を初期化する必要があります．"
D : Output vector of distortion coefficients \(\distcoeffsfisheye\).,D : ディストーション係数の出力ベクトル\(distcoeffsfisheye\)
"rvecs : Output vector of rotation vectors (see Rodrigues ) estimated for each pattern view. That is, each k-th rotation vector together with the corresponding k-th translation vector (see the next output parameter description) brings the calibration pattern from the model coordinate space (in which object points are specified) to the world coordinate space, that is, a real position of the calibration pattern in the k-th pattern view (k=0.. M -1).",rvecs :各パターンビューに対して推定された回転ベクトル（Rodrigues参照）の出力ベクトル．すなわち、k番目の回転ベクトルは、対応するk番目の並進ベクトル（次の出力パラメータの説明を参照）とともに、キャリブレーションパターンをモデル座標空間（物体の点が指定されている）からワールド座標空間、すなわちk番目のパターンビュー（k=0...M -1）におけるキャリブレーションパターンの実位置に近づけるものである。
tvecs : Output vector of translation vectors estimated for each pattern view.,tvecs :各パターンビューに対して推定された平行移動ベクトルの出力ベクトル．
"fisheye::CALIB_USE_INTRINSIC_GUESS cameraMatrix contains valid initial values of fx, fy, cx, cy that are optimized further. Otherwise, (cx, cy) is initially set to the image center ( imageSize is used), and focal distances are computed in a least-squares fashion.","fisheye::CALIB_USE_INTRINSIC_GUESS cameraMatrix には，有効な初期値 fx, fy, cx, cy が含まれており，さらに最適化されます．それ以外の場合，(cx, cy)は初期値として画像中心に設定され（imageSizeが使用されます），焦点距離は最小二乗法で計算されます．"
fisheye::CALIB_RECOMPUTE_EXTRINSIC Extrinsic will be recomputed after each iteration of intrinsic optimization.,fisheye::CALIB_RECOMPUTE_EXTRINSIC 本質的な最適化の各反復の後、本質的なものが再計算されます。
fisheye::CALIB_CHECK_COND The functions will check validity of condition number.,fisheye::CALIB_CHECK_COND 条件番号の有効性をチェックする関数です。
fisheye::CALIB_FIX_SKEW Skew coefficient (alpha) is set to zero and stay zero.,fisheye::CALIB_FIX_SKEW スキュー係数(α)がゼロに設定され、ゼロのままになります。
"fisheye::CALIB_FIX_K1,..., fisheye::CALIB_FIX_K4 Selected distortion coefficients are set to zeros and stay zero.","fisheye::CALIB_FIX_K1,..., fisheye::CALIB_FIX_K4 選択されたディストーション係数がゼロに設定され、ゼロのままになります。"
fisheye::CALIB_FIX_PRINCIPAL_POINT The principal point is not changed during the global optimization. It stays at the center or at a different location specified when fisheye::CALIB_USE_INTRINSIC_GUESS is set too.,fisheye::CALIB_FIX_PRINCIPAL_POINT 主点は全体最適化の間は変更されません。中心に留まるか、fisheye::CALIB_USE_INTRINSIC_GUESSが設定されているときに指定された別の場所に留まります。
"fisheye::CALIB_FIX_FOCAL_LENGTH The focal length is not changed during the global optimization. It is the \(max(width,height)/\pi\) or the provided \(f_x\), \(f_y\) when fisheye::CALIB_USE_INTRINSIC_GUESS is set too.","fisheye::CALIB_FIX_FOCAL_LENGTH 焦点距離は全体最適化の際には変更されません。fisheye::CALIB_USE_INTRINSIC_GUESSが設定されている場合は、\(max(width,height)/\pi\)または \(f_x\)、 ⅷ(f_yonce)となります。"
Stereo rectification for fisheye camera model.,魚眼カメラモデルのステレオ平行化を行います。
K1 : First camera intrinsic matrix.,K1 : First camera intrinsic matrix.
D1 : First camera distortion parameters.,D1 : 第一カメラの歪みパラメータ．
K2 : Second camera intrinsic matrix.,K2 : Second camera intrinsic matrix.
D2 : Second camera distortion parameters.,D2 : Second camera distortion parameters.
R : Rotation matrix between the coordinate systems of the first and the second cameras.,R : 第一カメラと第二カメラの座標系間の回転行列
tvec : Translation vector between coordinate systems of the cameras.,tvec :カメラの座標系間の移動ベクトル
R1 : Output 3x3 rectification transform (rotation matrix) for the first camera.,R1 : 1台目のカメラの3×3平行化変換（回転行列）を出力。
R2 : Output 3x3 rectification transform (rotation matrix) for the second camera.,R2 : 2台目のカメラの3x3直交変換（回転行列）を出力。
P1 : Output 3x4 projection matrix in the new (rectified) coordinate systems for the first camera.,P1 : 1台目のカメラの新しい（平行化された）座標系への3x4投影行列を出力。
P2 : Output 3x4 projection matrix in the new (rectified) coordinate systems for the second camera.,P2 : 2台目のカメラの新しい（平行化された）座標系における3x4の射影行列を出力。
Q : Output \(4 \times 4\) disparity-to-depth mapping matrix (see reprojectImageTo3D ).,Q : 視差-深度マッピング行列（reprojectImageTo3D 参照）を出力．
"flags : Operation flags that may be zero or fisheye::CALIB_ZERO_DISPARITY . If the flag is set, the function makes the principal points of each camera have the same pixel coordinates in the rectified views. And if the flag is not set, the function may still shift the images in the horizontal or vertical direction (depending on the orientation of epipolar lines) to maximize the useful image area.",flags :ゼロまたは fisheye::CALIB_ZERO_DISPARITY のいずれかの操作フラグ．このフラグがセットされている場合，この関数は，各カメラの主点が，平行化されたビューにおいて同じピクセル座標になるようにします．また，このフラグがセットされていない場合でも，有用な画像領域を最大化するために，（エピポーラ線の向きに応じて）画像を水平方向あるいは垂直方向に移動させることができます．
"newImageSize : New image resolution after rectification. The same size should be passed to initUndistortRectifyMap (see the stereo_calib.cpp sample in OpenCV samples directory). When (0,0) is passed (default), it is set to the original imageSize . Setting it to larger value can help you preserve details in the original image, especially when there is a big radial distortion.","newImageSize : 整形後の新しい画像解像度．同じサイズを initUndistortRectifyMap に渡す必要があります（OpenCV のサンプルディレクトリにある stereo_calib.cpp のサンプルを参照してください）．(0,0) が渡された場合（デフォルト），これは元の imageSize に設定されます．より大きな値を設定することで，特に半径方向に大きな歪みがある場合に，元の画像の詳細を保持することができます．"
Performs stereo calibration.,ステレオキャリブレーションを行います．
objectPoints : Vector of vectors of the calibration pattern points.,objectPoints :キャリブレーションパターンのポイントを示すベクター
"imagePoints1 : Vector of vectors of the projections of the calibration pattern points, observed by the first camera.",imagePoints1 : 1台目のカメラで観測された，キャリブレーションパターン点の投影像のベクトル．
"imagePoints2 : Vector of vectors of the projections of the calibration pattern points, observed by the second camera.",imagePoints2 : 2台目のカメラで観測したキャリブレーションパターン点の投影結果のベクトル。
"K1 : Input/output first camera intrinsic matrix: \(\vecthreethree{f_x^{(j)}}{0}{c_x^{(j)}}{0}{f_y^{(j)}}{c_y^{(j)}}{0}{0}{1}\) , \(j = 0,\, 1\) . If any of fisheye::CALIB_USE_INTRINSIC_GUESS , fisheye::CALIB_FIX_INTRINSIC are specified, some or all of the matrix components must be initialized.","K1 : 1台目のカメラの固有マトリクスを入出力します。\(\vecthreethree{f_x^{(j)}}{0}{c_x^{(j)}}{0}{f_y^{(j)}}{c_y^{(j)}}{0}{0}{1}\), \(j = 0,\, 1\) .fisheye::CALIB_USE_INTRINSIC_GUESS , fisheye::CALIB_FIX_INTRINSIC のいずれかが指定された場合は，行列成分の一部または全部を初期化する必要がある。"
D1 : Input/output vector of distortion coefficients \(\distcoeffsfisheye\) of 4 elements.,D1 ：4要素の歪曲係数ベクトル\(distcoeffsfisheye\)。
K2 : Input/output second camera intrinsic matrix. The parameter is similar to K1 .,K2 : 2 台目のカメラの固有マトリクスを入出力します．K1 と同様のパラメータです。
D2 : Input/output lens distortion coefficients for the second camera. The parameter is similar to D1 .,D2 ：入出力されるセカンドカメラのレンズディストーション係数。このパラメータはD1と同様です．
imageSize : Size of the image used only to initialize camera intrinsic matrix.,imageSize : カメラ固有の行列を初期化するためにのみ使用される画像のサイズ．
R : Output rotation matrix between the 1st and the 2nd camera coordinate systems.,R : 第1カメラと第2カメラの座標系間の回転行列を出力．
T : Output translation vector between the coordinate systems of the cameras.,T : カメラの座標系間の並進ベクトルを出力．
"fisheye::CALIB_FIX_INTRINSIC Fix K1, K2? and D1, D2? so that only R, T matrices are estimated.","fisheye::CALIB_FIX_INTRINSIC K1, K2? とD1, D2? を固定し、R, T行列のみが推定されるようにします。"
"fisheye::CALIB_USE_INTRINSIC_GUESS K1, K2 contains valid initial values of fx, fy, cx, cy that are optimized further. Otherwise, (cx, cy) is initially set to the image center (imageSize is used), and focal distances are computed in a least-squares fashion.","fisheye::CALIB_USE_INTRINSIC_GUESS K1, K2 に有効な初期値 fx, fy, cx, cy が含まれており、さらに最適化される。それ以外の場合は，(cx, cy)が画像中心に初期設定され（imageSizeが使用される），最小二乗法で焦点距離が計算される。"
Computes disparity map for the specified stereo pair.,指定されたステレオペアの視差マップを計算します．
left : Left 8-bit single-channel image.,left : 8ビットシングルチャンネルの左画像．
right : Right image of the same size and the same type as the left one.,right : 左の画像と同じサイズ，同じ種類の右の画像．
"disparity : Output disparity map. It has the same size as the input images. Some algorithms, like StereoBM or StereoSGBM compute 16-bit fixed-point disparity map (where each disparity value has 4 fractional bits), whereas other algorithms output 32-bit floating-point disparity map.",disparity : 視差マップの出力．入力画像と同じサイズになります．StereoBMやStereoSGBMのように，16ビット固定小数点型の視差マップ（各視差の値が4ビットの小数である）を計算するアルゴリズムもあれば，32ビット浮動小数点型の視差マップを出力するアルゴリズムもあります．
Implemented in cv::cuda::StereoSGM.,cv::cuda::StereoSGM で実装されています．
"Class for computing stereo correspondence using the block matching algorithm, introduced and contributed to OpenCV by K. Konolige. ",K. KonoligeによってOpenCVに導入・貢献された，ブロックマッチングアルゴリズムを用いてステレオ対応点を計算するためのクラス．
Creates StereoBM object.,StereoBM オブジェクトを作成します．
numDisparities : the disparity search range. For each pixel algorithm will find the best disparity from 0 (default minimum disparity) to numDisparities. The search range can then be shifted by changing the minimum disparity.,numDisparities : 視差の探索範囲．各ピクセルに対して，アルゴリズムは，0（デフォルトの最小視差）から numDisparities までの範囲で，最適な視差を見つけます．そして，最小視差を変更することで，探索範囲を移動させることができます．
"blockSize : the linear size of the blocks compared by the algorithm. The size should be odd (as the block is centered at the current pixel). Larger block size implies smoother, though less accurate disparity map. Smaller block size gives more detailed disparity map, but there is higher chance for algorithm to find a wrong correspondence.",blockSize : アルゴリズムによって比較されるブロックの線形サイズです．このサイズは奇数でなければいけません（ブロックの中心が現在のピクセルになるからです）．ブロックサイズを大きくすると，視差マップの精度は落ちるものの，より滑らかになります．ブロックサイズが小さければ，より詳細な視差マップが得られますが，アルゴリズムが間違った対応関係を見つけてしまう可能性が高くなります．
The function create StereoBM object. You can then call StereoBM::compute() to compute disparity for a specific stereo pair.,この関数は，StereoBM オブジェクトを作成します．そして，StereoBM::compute() を呼び出して，特定のステレオペアの視差を計算することができます．
The class implements the modified H. Hirschmuller algorithm [111] that differs from the original one as follows: ,このクラスは，修正された H. Hirschmuller アルゴリズム [111] を実装しており，オリジナルのアルゴリズムとは以下のように異なります．
"By default, the algorithm is single-pass, which means that you consider only 5 directions instead of 8. Set mode=StereoSGBM::MODE_HH in createStereoSGBM to run the full variant of the algorithm but beware that it may consume a lot of memory.",createStereoSGBM で mode=StereoSGBM::MODE_HH を設定すると，アルゴリズムの完全なバリエーションが実行されますが，大量のメモリを消費する可能性があることに注意してください．
"The algorithm matches blocks, not individual pixels. Though, setting blockSize=1 reduces the blocks to single pixels.",このアルゴリズムは、個々のピクセルではなく、ブロックをマッチングします。ただし、blockSize=1を設定すると、ブロックが単一のピクセルに縮小されます。
"Mutual information cost function is not implemented. Instead, a simpler Birchfield-Tomasi sub-pixel metric from [23] is used. Though, the color images are supported as well.",相互情報コスト関数は実装されていません。代わりに、[23]のより単純なBirchfield-Tomasiサブピクセルメトリックが使用されます。ただし、カラー画像にも対応しています。
"Some pre- and post- processing steps from K. Konolige algorithm StereoBM are included, for example: pre-filtering (StereoBM::PREFILTER_XSOBEL type) and post-filtering (uniqueness check, quadratic interpolation and speckle filtering).",K. KonoligeのアルゴリズムであるStereoBMのいくつかの前処理および後処理ステップが含まれています。例えば，プレフィルタリング（StereoBM::PREFILTER_XSOBEL型）およびポストフィルタリング（一意性チェック，2次補間，スペックルフィルタリング）があります。
(Python) An example illustrating the use of the StereoSGBM matching algorithm can be found at opencv_source_code/samples/python/stereo_match.py ,(Python) StereoSGBM マッチングアルゴリズムの使用例は、opencv_source_code/samples/python/stereo_match.py にあります。
Creates StereoSGBM object.,StereoSGBMオブジェクトを作成します。
"minDisparity : Minimum possible disparity value. Normally, it is zero but sometimes rectification algorithms can shift images, so this parameter needs to be adjusted accordingly.",minDisparity : 視差の最小値を指定します．通常は0ですが，平行化アルゴリズムによって画像がずれることがあるので，このパラメータを適宜調整する必要があります．
"numDisparities : Maximum disparity minus minimum disparity. The value is always greater than zero. In the current implementation, this parameter must be divisible by 16.",numDisparities :最大視差量から最小視差量を引いた値．この値は常にゼロよりも大きくなります．現在の実装では，このパラメータは16で割り切れる値でなければいけません．
"blockSize : Matched block size. It must be an odd number >=1 . Normally, it should be somewhere in the 3..11 range.",blockSize : 適合するブロックサイズ．奇数 >=1 である必要があります．通常は，3～11の範囲内の値を指定します．
P1 : The first parameter controlling the disparity smoothness. See below.,P1 : 視差の滑らかさを制御する最初のパラメータです．後述します。
"P2 : The second parameter controlling the disparity smoothness. The larger the values are, the smoother the disparity is. P1 is the penalty on the disparity change by plus or minus 1 between neighbor pixels. P2 is the penalty on the disparity change by more than 1 between neighbor pixels. The algorithm requires P2 > P1 . See stereo_match.cpp sample where some reasonably good P1 and P2 values are shown (like 8*number_of_image_channels*blockSize*blockSize and 32*number_of_image_channels*blockSize*blockSize , respectively).",P2 : 視差の滑らかさを制御する2番目のパラメータです。値が大きくなるほど、視差が滑らかになります。P1は，隣接するピクセル間のプラスマイナス1の視差の変化に対するペナルティです．P2は，隣接するピクセル間で1以上の視差の変化があった場合のペナルティです．このアルゴリズムでは，P2 > P1 である必要があります．stereo_match.cpp のサンプルには，適度な P1 と P2 の値が示されています（それぞれ，8*number_of_image_channels*blockSize*blockSize と 32*number_of_image_channels*blockSize*blockSize など）．
disp12MaxDiff : Maximum allowed difference (in integer pixel units) in the left-right disparity check. Set it to a non-positive value to disable the check.,disp12MaxDiff ：左右の視差チェックで許容される最大の差（整数ピクセル単位）。チェックを無効にするには，この値を非正の値に設定してください．
"preFilterCap : Truncation value for the prefiltered image pixels. The algorithm first computes x-derivative at each pixel and clips its value by [-preFilterCap, preFilterCap] interval. The result values are passed to the Birchfield-Tomasi pixel cost function.","preFilterCap : 事前にフィルタリングされた画像のピクセルに対する切り捨て値．このアルゴリズムでは、まず各画素のx-derivativeを計算し、その値を[-preFilterCap, preFilterCap]間隔で切り詰めます。その結果を、Birchfield-Tomasiのピクセルコスト関数に渡します。"
"uniquenessRatio : Margin in percentage by which the best (minimum) computed cost function value should ""win"" the second best value to consider the found match correct. Normally, a value within the 5-15 range is good enough.",uniquenessRatio :最良（最小）のコスト関数の値が、2番目に良い値に「勝つ」ことで、見つかったマッチが正しいと判断されるマージンをパーセントで指定します。通常は、5〜15の範囲内の値であれば十分です。
"speckleWindowSize : Maximum size of smooth disparity regions to consider their noise speckles and invalidate. Set it to 0 to disable speckle filtering. Otherwise, set it somewhere in the 50-200 range.",speckleWindowSize : ノイズ・スペックルを考慮して無効化する，滑らかな視差領域の最大サイズ．0に設定するとスペックルフィルタリングが無効になります。それ以外の場合は、50-200の範囲で設定してください。
"speckleRange : Maximum disparity variation within each connected component. If you do speckle filtering, set the parameter to a positive value, it will be implicitly multiplied by 16. Normally, 1 or 2 is good enough.",speckleRange : 各連結成分内の最大視差変動量。speckle filteringを行う場合、パラメータに正の値を設定すると、暗黙のうちに16倍されます。通常は、1または2で十分です。
"mode : Set it to StereoSGBM::MODE_HH to run the full-scale two-pass dynamic programming algorithm. It will consume O(W*H*numDisparities) bytes, which is large for 640x480 stereo and huge for HD-size pictures. By default, it is set to false .",mode :StereoSGBM::MODE_HHに設定すると，本格的な2パス・ダイナミック・プログラミング・アルゴリズムが実行される。これは，O(W*H*numDisparities)バイトを消費するが，640x480ステレオでは大きく，HDサイズの画像では巨大である．デフォルトでは，これは false に設定されています．
"The first constructor initializes StereoSGBM with all the default parameters. So, you only have to set StereoSGBM::numDisparities at minimum. The second constructor enables you to set each parameter to a custom value.",最初のコンストラクタは，すべてのデフォルトパラメータで StereoSGBM を初期化します。そのため，StereoSGBM::numDisparities を最小値に設定するだけでよいです。2番目のコンストラクタでは，各パラメータをカスタム値に設定することができます。
Sets/resets the break-on-error mode.,ブレークオンエラーモードの設定/リセット。
"When the break-on-error mode is set, the default error handler issues a hardware exception, which can make debugging more convenient.Examples: samples/cpp/stitching_detailed.cpp.",ブレークオンエラーモードが設定されていると、デフォルトのエラーハンドラがハードウェア例外を発行するので、デバッグが便利になります。例：samples/cpp/stitching_detailed.cpp.
Sets the new error handler and the optional user data.,新しいエラーハンドラと、オプションのユーザデータを設定します。
"errCallback : the new error handler. If NULL, the default error handler is used.",errCallback : 新しいエラーハンドラです。NULLの場合は、デフォルトのエラーハンドラが使用されます。
"userdata : the optional user data pointer, passed to the callback.",userdata : コールバックに渡される，オプションのユーザーデータポインタ．
prevUserdata : the optional output parameter where the previous user data pointer is stored,prevUserdata : 前のユーザデータポインタが格納される，オプションの出力パラメータ．
"The function sets the new error handler, called from cv::error().",この関数は，cv::error()から呼び出される新しいエラーハンドラを設定します．
Examples: samples/cpp/train_HOG.cpp.,例： samples/cpp/train_HOG.cpp.
OpenCV will try to set the number of threads for the next parallel region.,OpenCV は，次の並列領域のスレッド数の設定を試みます．
"If threads == 0, OpenCV will disable threading optimizations and run all it's functions sequentially. Passing threads < 0 will reset threads number to system default. This function must be called outside of parallel region.OpenCV will try to run its functions with specified threads number, but some behaviour differs from framework:TBB - User-defined parallel constructions will run with the same threads number, if another is not specified. If later on user creates his own scheduler, OpenCV will use it.",threads == 0 の場合，OpenCV はスレッド最適化を無効にし，すべての関数を逐次実行します．threads < 0 を渡すと，スレッド数がシステムのデフォルトにリセットされます．OpenCV は，指定されたスレッド数で関数を実行しようとしますが，いくつかの動作はフレームワークと異なります： TBB - ユーザ定義の並列構造は，別のスレッドが指定されなければ，同じスレッド数で実行されます．後にユーザが独自のスケジューラを作成した場合，OpenCVはそれを使用します．
OpenMP - No special defined behaviour.,OpenMP - 特別に定義された動作はありません．
"Concurrency - If threads == 1, OpenCV will disable threading optimizations and run its functions sequentially.",Concurrency - threads == 1 の場合，OpenCV はスレッドの最適化を無効にし，その関数を順次実行します．
GCD - Supports only values <= 0.,GCD - <= 0 の値のみをサポートします．
C= - No special defined behaviour. Parameters,C= - 特別な定義はありません．パラメータ
    nthreadsNumber of threads used by OpenCV. ,    nthreads OpenCV が利用するスレッド数．
"See alsogetNumThreads, getThreadNum","関連項目： getNumThreads, getThreadNum"
Returns the number of threads used by OpenCV for parallel regions.,OpenCVが並列領域で利用するスレッド数を返します．
"Always returns 1 if OpenCV is built without threading support.The exact meaning of return value depends on the threading framework used by OpenCV library:TBB - The number of threads, that OpenCV will try to use for parallel regions. If there is any tbb::thread_scheduler_init in user code conflicting with OpenCV, then function returns default number of threads used by TBB library.",OpenCVがスレッドをサポートせずに構築されている場合は，常に1を返します．戻り値の正確な意味は，OpenCVライブラリで利用されているスレッドフレームワークに依存します：TBB - OpenCVが並列領域に利用しようとするスレッド数．ユーザコード内の tbb::thread_scheduler_init が OpenCV と衝突している場合，この関数は TBB ライブラリで利用されるデフォルトのスレッド数を返します．
OpenMP - An upper bound on the number of threads that could be used to form a new team.,OpenMP - 新しいチームを形成するために使用されるスレッド数の上限。
"Concurrency - The number of threads, that OpenCV will try to use for parallel regions.",Concurrency - OpenCV が並列領域に利用しようとするスレッド数．
GCD - Unsupported; returns the GCD thread pool limit (512) for compatibility.,GCD - サポートされていません．互換性のために，GCD スレッドプールの上限（512）を返します．
"C= - The number of threads, that OpenCV will try to use for parallel regions, if before called setNumThreads with threads > 0, otherwise returns the number of logical CPUs, available for the process. See alsosetNumThreads, getThreadNum","C= - OpenCV が並列領域で利用しようとするスレッド数． setNumThreads を呼び出す前に threads > 0 であれば，プロセスで利用可能な論理 CPU の数が返されます．関連項目： setNumThreads, getThreadNum"
Returns the index of the currently executed thread within the current parallel region. Always returns 0 if called outside of parallel region.,現在の並列領域内で、現在実行されているスレッドのインデックスを 返します。並列領域の外から呼び出された場合は、常に 0 を返します。
Deprecated:Current implementation doesn't corresponding to this documentation.The exact meaning of the return value depends on the threading framework used by OpenCV library:TBB - Unsupported with current 4.1 TBB release. Maybe will be supported in future.,Deprecated：現在の実装は，このドキュメントに対応していません．戻り値の正確な意味は，OpenCV ライブラリで利用されているスレッドフレームワークに依存します： TBB - 現在の 4.1 TBB リリースではサポートされていません．将来的にはサポートされるかもしれません．
"OpenMP - The thread number, within the current team, of the calling thread.",OpenMP - 呼び出したスレッドの，現在のチーム内のスレッド番号．
"Concurrency - An ID for the virtual processor that the current context is executing on (0 for master thread and unique number for others, but not necessary 1,2,3,...).","Concurrency - 現在のコンテキストが実行されている仮想プロセッサのID (マスタースレッドは0，その他は一意の番号，ただし，1,2,3,...は不要)．"
GCD - System calling thread's ID. Never returns 0 inside parallel region.,GCD - システムを呼び出すスレッドのID。並列領域内では決して0を返しません。
"C= - The index of the current parallel task. See alsosetNumThreads, getNumThreads","C= - 現在の並列タスクのインデックスです．関連項目： setNumThreads, getNumThreads"
Returns full configuration time cmake output.,完全な設定時間の cmake 出力を返します。
"Returned value is raw cmake output including version control system revision, compiler version, compiler flags, enabled modules and third party libraries, etc. Output format depends on target architecture.",返される値は、バージョンコントロールシステムのリビジョン、コンパイラのバージョン、コンパイラフラグ、有効なモジュールやサードパーティライブラリなどを含む生の cmake 出力です。出力形式はターゲットのアーキテクチャに依存します。
Returns library version string.,ライブラリのバージョンを文字列で返します。
"For example ""3.4.1-dev"".See alsogetMajorVersion, getMinorVersion, getRevisionVersion","getMajorVersion, getMinorVersion, getRevisionVersionも参照してください。"
Returns major library version.,ライブラリのメジャーバージョンを返します。
Returns minor library version.,ライブラリのメジャーバージョンを返します マイナーバージョンを返します
Returns revision field of the library version.,ライブラリのバージョンのリビジョン欄を返します
Returns the number of ticks.,刻みの数を返します。
"The function returns the number of ticks after the certain event (for example, when the machine was turned on). It can be used to initialize RNG or to measure a function execution time by reading the tick count before and after the function call.See alsogetTickFrequency, TickMeterExamples: fld_lines.cpp, samples/cpp/facedetect.cpp, samples/cpp/image_alignment.cpp, samples/cpp/peopledetect.cpp, samples/cpp/stitching_detailed.cpp, samples/cpp/watershed.cpp, samples/tapi/hog.cpp, and samples/tapi/squares.cpp.",この関数は、特定のイベント（例えば、マシンの電源が入った時など）の後のティック数を返します。他にもgetTickFrequency、TickMeterExamples: fld_lines.cpp、samples/cpp/facedetect.cpp、samples/cpp/image_alignment.cpp、samples/cpp/peopledetect.cpp、samples/cpp/stitching_detailed.cpp、samples/cpp/watershed.cpp、samples/tapi/hog.cpp、samples/tapi/squares.cppを参照してください。
Returns the number of ticks per second.,1秒あたりの目盛りの数を返します。
"The function returns the number of ticks per second. That is, the following code computes the execution time in seconds:double t = (double)getTickCount();// do something ...t = ((double)getTickCount() - t)/getTickFrequency();fragmentSee alsogetTickCount, TickMeterExamples: fld_lines.cpp, samples/cpp/facedetect.cpp, samples/cpp/image_alignment.cpp, samples/cpp/peopledetect.cpp, samples/cpp/stitching_detailed.cpp, samples/cpp/watershed.cpp, samples/dnn/classification.cpp, samples/dnn/object_detection.cpp, samples/dnn/segmentation.cpp, samples/tapi/hog.cpp, and samples/tapi/squares.cpp.","この関数は、1秒あたりの目盛りの数を返します。つまり、次のコードは、実行時間を秒単位で計算します。double t = (double)getTickCount();// 何かをする ...t = ((double)getTickCount() - t)/getTickFrequency();fragmentSee alsogetTickCount, TickMeterExamples: fld_lines.cpp, samples/cpp/facedetect.cpp、samples/cpp/image_alignment.cpp、samples/cpp/peopledetect.cpp、samples/cpp/stitching_detailed.cpp、samples/cpp/watershed.cpp、samples/dnn/classification.cpp、samples/dnn/object_detection.cpp、samples/dnn/segmentation.cpp、samples/tapi/hog.cpp、samples/tapi/squares.cppがあります。"
Returns the number of CPU ticks.,CPUティック数を返します。
"The function returns the current number of CPU ticks on some architectures (such as x86, x64, PowerPC). On other platforms the function is equivalent to getTickCount. It can also be used for very accurate time measurements, as well as for RNG initialization. Note that in case of multi-CPU systems a thread, from which getCPUTickCount is called, can be suspended and resumed at another CPU with its own counter. So, theoretically (and practically) the subsequent calls to the function do not necessary return the monotonously increasing values. Also, since a modern CPU varies the CPU frequency depending on the load, the number of CPU clocks spent in some code cannot be directly converted to time units. Therefore, getTickCount is generally a preferable solution for measuring execution time.",この関数は、一部のアーキテクチャ（x86、x64、PowerPCなど）では、現在のCPUチック数を返します。その他のプラットフォームでは，この関数はgetTickCountと同等です。この関数は，非常に正確な時間の測定や，RNGの初期化にも使用できます．マルチCPUシステムの場合，getCPUTickCountが呼び出されたスレッドは中断され，別のCPUで独自のカウンタを使って再開できることに注意してください。そのため，理論的には（そして実際には），この関数の後続の呼び出しは，単調に増加する値を返す必要はありません。また，最近のCPUは負荷に応じてCPUの周波数を変化させるため，あるコードに費やされたCPUクロック数を時間単位に直接変換することはできません。そのため、実行時間を計測するには、一般的にgetTickCountの方が望ましいとされています。
Returns true if the specified feature is supported by the host hardware.,指定された機能がホストハードウェアでサポートされている場合、trueを返します。
"feature : The feature of interest, one of cv::CpuFeatures",feature : 目的とする機能，cv::CpuFeatures の1つ．
"The function returns true if the host hardware supports the specified feature. When user calls setUseOptimized(false), the subsequent calls to checkHardwareSupport() will return false until setUseOptimized(true) is called. This way user can dynamically switch on and off the optimized code in OpenCV.",この関数は，ホストハードウェアが指定された機能をサポートしていれば，真を返します．ユーザが setUseOptimized(false) を呼び出すと，それ以降の checkHardwareSupport() は，setUseOptimized(true) が呼び出されるまで false を返します．このようにして，ユーザは OpenCV で最適化されたコードを動的にオン・オフすることができます．
Returns feature name by ID.,ID によるフィーチャー名を返します．
Returns empty string if feature is not defined,機能が定義されていない場合は，空の文字列を返します．
Returns list of CPU features enabled during compilation.,コンパイル時に有効な CPU 機能のリストを返します．
Returned value is a string containing space separated list of CPU features with following markers:no markers - baseline features,返される値は，CPU機能のリストをスペースで区切った文字列で，以下のマーカーが付いています： no markers - ベースライン機能
prefix * - features enabled in dispatcher,prefix * - ディスパッチャーで有効になった機能
suffix ? - features enabled but not available in HWExample: SSE SSE2 SSE3 *SSE4.1 *SSE4.2 *FP16 *AVX *AVX2 *AVX512-SKX?,suffix ?- 有効だがHWExampleでは使用できない機能。sse sse2 sse3 *sse4.1 *sse4.2 *fp16 *avx *avx2 *avx512-skx?
Returns the number of logical CPUs available for the process.,プロセスで使用可能な論理 CPU の数を返します。
Enables or disables the optimized code.,最適化されたコードを有効にしたり無効にしたりします。
onoff : The boolean flag specifying whether the optimized code should be used (onoff=true) or not (onoff=false).,onoff : 最適化されたコードを使用する(onoff=true)かしない(onoff=false)かを指定するブーリアンフラグです。
"The function can be used to dynamically turn on and off optimized dispatched code (code that uses SSE4.2, AVX/AVX2, and other instructions on the platforms that support it). It sets a global flag that is further checked by OpenCV functions. Since the flag is not checked in the inner OpenCV loops, it is only safe to call the function on the very top level in your application where you can be sure that no other OpenCV function is currently executed.By default, the optimized code is enabled unless you disable it in CMake. The current status can be retrieved using useOptimized.",この関数は，最適化されたディスパッチコード（SSE4.2やAVX/AVX2などの命令をサポートするプラットフォームで使用されるコード）を動的にオン／オフするために使用できます。これは，OpenCVの関数によってさらにチェックされるグローバルフラグを設定します．このフラグは，OpenCV の内部ループではチェックされないので，アプリケーションの最上位で，他の OpenCV 関数が実行されていないことが確認できる場合にのみ，この関数を呼び出すのが安全です．デフォルトでは，CMake で無効にしない限り，最適化されたコードが有効になります．現在の状態は， useOptimized で取得できます．
Returns the status of optimized code usage.,最適化されたコードの使用状況を返します．
"The function returns true if the optimized code is enabled. Otherwise, it returns false.",この関数は，最適化されたコードが有効な場合は true を返します。それ以外の場合は false を返します。
Computes the source location of an extrapolated pixel.,外挿したピクセルのソース位置を計算します。
"p : 0-based coordinate of the extrapolated pixel along one of the axes, likely <0 or >= len",p :外挿されたピクセルの，いずれかの軸に沿った0ベースの座標，<0 or >= len
len : Length of the array along the corresponding axis.,len : 対応する軸に沿った配列の長さ．
"borderType : Border type, one of the BorderTypes, except for BORDER_TRANSPARENT and BORDER_ISOLATED . When borderType==BORDER_CONSTANT , the function always returns -1, regardless of p and len.",borderType :BORDER_TRANSPARENT と BORDER_ISOLATED を除く BorderTypes のうちの 1 つ。borderType==BORDER_CONSTANT の場合，この関数は，p や len に関わらず常に -1 を返します．
"The function computes and returns the coordinate of a donor pixel corresponding to the specified extrapolated pixel when using the specified extrapolation border mode. For example, if you use cv::BORDER_WRAP mode in the horizontal direction, cv::BORDER_REFLECT_101 in the vertical direction and want to compute value of the ""virtual"" pixel Point(-5, 100) in a floating-point image img , it looks like:float val = img.at<float>(borderInterpolate(100, img.rows, cv::BORDER_REFLECT_101),                          borderInterpolate(-5, img.cols, cv::BORDER_WRAP));fragmentNormally, the function is not called directly. It is used inside filtering functions and also in copyMakeBorder.See alsocopyMakeBorder","この関数は，指定された外挿分割モードを用いた場合に，指定された外挿分割ピクセルに対応するドナーピクセルの座標を計算して返します．例えば，水平方向に cv::BORDER_WRAP モード，垂直方向に cv::BORDER_REFLECT_101 モードを利用し，浮動小数点型画像 img の「仮想」ピクセル Point(-5, 100) の値を求める場合，次のようになります： float val = img.at<float>(borderInterpolate(100, img.rows, cv::BORDER_REFLECT_101), borderInterpolate(-5, img.cols, cv::BORDER_WRAP));fragment通常，この関数が直接呼ばれることはありません．この関数は，フィルタリング関数の内部や，copyMakeBorderの中で利用されます．"
Forms a border around an image.,画像の周りに境界線を形成します。
src : Source image.,src : コピー元画像。
"dst : Destination image of the same type as src and the size Size(src.cols+left+right, src.rows+top+bottom) .","dst : src と同じ種類で，サイズが Size(src.cols+left+right, src.rows+top+bottom) である出力画像．"
top : the top pixels,top : 上側のピクセル
bottom : the bottom pixels,bottom : 下側のピクセル
left : the left pixels,left : 左側のピクセル
"right : Parameter specifying how many pixels in each direction from the source image rectangle to extrapolate. For example, top=1, bottom=1, left=1, right=1 mean that 1 pixel-wide border needs to be built.","right : ソース画像の矩形領域から各方向に何ピクセルずつ外挿するかを指定するパラメータ．例えば、top=1, bottom=1, left=1, right=1とすると、1ピクセル幅のボーダーを作成する必要があります。"
borderType : Border type. See borderInterpolate for details.,borderType :ボーダータイプ．詳細は borderInterpolateを参照してください。
value : Border value if borderType==BORDER_CONSTANT .,value : borderType==BORDER_CONSTANTの場合のボーダー値．
"The function copies the source image into the middle of the destination image. The areas to the left, to the right, above and below the copied source image will be filled with extrapolated pixels. This is not what filtering functions based on it do (they extrapolate pixels on-fly), but what other more complex functions, including your own, may do to simplify image boundary handling.The function supports the mode when src is already in the middle of dst . In this case, the function does not copy src itself but simply constructs the border, for example:// let border be the same in all directionsint border=2;// constructs a larger image to fit both the image and the borderMat gray_buf(rgb.rows + border*2, rgb.cols + border*2, rgb.depth());// select the middle part of it w/o copying dataMat gray(gray_canvas, Rect(border, border, rgb.cols, rgb.rows));// convert image from RGB to grayscalecvtColor(rgb, gray, COLOR_RGB2GRAY);// form a border in-placecopyMakeBorder(gray, gray_buf, border, border,               border, border, BORDER_REPLICATE);// now do some custom filtering ......fragmentNoteWhen the source image is a part (ROI) of a bigger image, the function will try to use the pixels outside of the ROI to form a border. To disable this feature and always do extrapolation, as if src was not a ROI, use borderType | BORDER_ISOLATED.See alsoborderInterpolateExamples: samples/cpp/tutorial_code/ImgTrans/copyMakeBorder_demo.cpp.","この関数は，入力画像を出力画像の中央にコピーします．コピーされたソース画像の左、右、上、下の領域は、外挿されたピクセルで埋められます。これは，この関数を基にしたフィルタリング関数が行っていること（オンザフライでピクセルを外挿すること）とは異なりますが，あなた自身の関数を含む他のより複雑な関数が，画像の境界処理を簡単にするために行っていることもあります． この関数は，src が既に dst の中央にある場合のモードをサポートしています．この場合，この関数は src 自身をコピーせず，単に境界を作成します．例えば，次のようにします： // 境界はすべての方向で同じであるとするint border=2;// 画像と境界の両方に合うように，より大きな画像を作成するMat gray_buf(rgb.rows + border*2, rgb.cols + border*2, rgb.depth());// dataMat gray(gray_canvas, Rect(border, border, rgb.cols, rgb.rows));// 画像を RGB からグレースケールに変換するecvtColor(rgb, gray, COLOR_RGB2GRAY);// その場で境界線を形成するcopyMakeBorder(gray, gray_buf, border, border, border, BORDER_REPLICATE);// 独自のフィルタリングを行う ......fragment注：入力画像が大きな画像の一部（ROI）である場合、この関数は ROI の外側のピクセルを使って境界線を形成しようとします。この機能を無効にして、あたかもsrcがROIでないかのように常に外挿するには、 borderType | BORDER_ISOLATEDを使用してください。他にもborderInterpolateExamples: samples/cpp/tutorial_code/ImgTrans/copyMakeBorder_demo.cppを参照してください。"
Calculates the per-element sum of two arrays or an array and a scalar.,2つの配列，または配列とスカラの要素毎の和を求めます．
src1 : first input array or a scalar.,src1 : 1 番目の入力配列，またはスカラ．
src2 : second input array or a scalar.,src2 : 2 番目の入力配列，またはスカラ．
dst : output array that has the same size and number of channels as the input array(s); the depth is defined by dtype or src1/src2.,dst : 入力配列と同じサイズ，同じチャンネル数の出力配列．深さは dtype または src1/src2 によって定義されます．
"mask : optional operation mask - 8-bit single channel array, that specifies elements of the output array to be changed.",mask : オプションである操作マスク．8ビットのシングルチャンネル配列で，変更される出力配列の要素を指定します．
dtype : optional depth of the output array (see the discussion below).,dtype : オプションで，出力配列の深さを指定します（後述の説明を参照してください）．
The function add calculates:Sum of two arrays when both input arrays have the same size and the same number of channels: ,関数 add は，同じサイズ，同じチャンネル数の入力配列を持つ2つの配列の和を求めます．
\[\texttt{dst}(I) = \texttt{saturate} ( \texttt{src1}(I) + \texttt{src2}(I)) \quad \texttt{if mask}(I) \ne0\],\I） = ˶ˆ꒳ˆ˵ ( ˶ˆ꒳ˆ˵ )\♪♪～
Sum of an array and a scalar when src2 is constructed from Scalar or has the same number of elements as src1.channels(): ,src2 が Scalar で構成されているか，または src1.channels() と同じ数の要素を持つ場合，配列とスカラの和になります．
\[\texttt{dst}(I) = \texttt{saturate} ( \texttt{src1}(I) + \texttt{src2} ) \quad \texttt{if mask}(I) \ne0\],\I） = ˶ˆ꒳ˆ˵ ( ˶ˆ꒳ˆ˵ )\I (if mask}(I)
Sum of a scalar and an array when src1 is constructed from Scalar or has the same number of elements as src2.channels(): ,src1 が Scalar で構成されている場合や， src2.channels() と同じ数の要素を持つ場合に，スカラと配列の和を求めます．
\[\texttt{dst}(I) = \texttt{saturate} ( \texttt{src1} + \texttt{src2}(I) ) \quad \texttt{if mask}(I) \ne0\],\I） = ˶ˆ꒳ˆ˵ ( ˶ˆ꒳ˆ˵ )+ ˶ˆ꒳ˆ˵ )\I (I)
" where I is a multi-dimensional index of array elements. In case of multi-channel arrays, each channel is processed independently.The first function in the list above can be replaced with matrix expressions:dst = src1 + src2;dst += src1; // equivalent to add(dst, src1, dst);fragmentThe input arrays and the output array can all have the same or different depths. For example, you can add a 16-bit unsigned array to a 8-bit signed array and store the sum as a 32-bit floating-point array. Depth of the output array is determined by the dtype parameter. In the second and third cases above, as well as in the first case, when src1.depth() == src2.depth(), dtype can be set to the default -1. In this case, the output array will have the same depth as the input array, be it src1, src2 or both.NoteSaturation is not applied when the output array has the depth CV_32S. You may even get result of an incorrect sign in the case of overflow.See alsosubtract, addWeighted, scaleAdd, Mat::convertToExamples: modules/shape/samples/shape_example.cpp."," ここで I は，配列要素の多次元インデックスです．上のリストの最初の関数は，行列式に置き換えることができます： dst = src1 + src2;dst += src1; // add(dst, src1, dst);fragment入力配列と出力配列のビット深度は，すべて同じでも異なっていても構いません．例えば，16 ビットの符号なし配列と 8 ビットの符号あり配列を加算し，その和を 32 ビットの浮動小数点配列として格納することができます．出力配列の深さは、dtypeパラメータで決定されます。上記の 2 番目と 3 番目のケース，そして最初のケースと同様に， src1.depth() == src2.depth() の場合， dtype をデフォルトの -1 に設定することができます． この場合，出力配列のビット深度は， src1，src2，またはその両方の入力配列と同じになります． 注意出力配列のビット深度が CV_32S の場合，彩度は適用されません．subtract, addWeighted, scaleAdd, Mat::convertToExamples も参照してください： modules/shape/samples/shape_example.cpp."
Calculates the per-element difference between two arrays or array and a scalar.,2 つの配列同士，あるいは配列とスカラの要素毎の差を求めます．
dst : output array of the same size and the same number of channels as the input array.,dst : 入力配列と同じサイズ，同じチャンネル数の出力配列．
mask : optional operation mask; this is an 8-bit single channel array that specifies elements of the output array to be changed.,mask : オプションである操作マスク．これは，8ビットのシングルチャンネル配列で，変更される出力配列の要素を指定します．
dtype : optional depth of the output array,dtype : オプションである，出力配列の深さ
"The function subtract calculates:Difference between two arrays, when both input arrays have the same size and the same number of channels: ",関数 subtract は，入力配列が同じサイズ，同じチャンネル数の場合に，2つの配列の差を計算します．
\[\texttt{dst}(I) = \texttt{saturate} ( \texttt{src1}(I) - \texttt{src2}(I)) \quad \texttt{if mask}(I) \ne0\],\I） = ˶ˆ꒳ˆ˵ ( ˶ˆ꒳ˆ˵ )\I (if mask}(I)
"Difference between an array and a scalar, when src2 is constructed from Scalar or has the same number of elements as src1.channels(): ",src2 が Scalar で構成されている場合や src1.channels() と同じ数の要素を持つ場合の，配列とスカラの違い。
\[\texttt{dst}(I) = \texttt{saturate} ( \texttt{src1}(I) - \texttt{src2} ) \quad \texttt{if mask}(I) \ne0\],\I） = ˶ˆ꒳ˆ˵ ( ˶ˆ꒳ˆ˵ )\I (if mask}(I)
"Difference between a scalar and an array, when src1 is constructed from Scalar or has the same number of elements as src2.channels(): ",src1 が Scalar で構成されている場合や， src2.channels() と同じ数の要素を持つ場合の，スカラと配列の違いです．
\[\texttt{dst}(I) = \texttt{saturate} ( \texttt{src1} - \texttt{src2}(I) ) \quad \texttt{if mask}(I) \ne0\],\I） = ˶ˆ꒳ˆ˵ ( ˶ˆ꒳ˆ˵ )- src2}(I)\♪♪～
The reverse difference between a scalar and an array in the case of SubRS: ,SubRSの場合、スカラとアレイの違いは逆です。
\[\texttt{dst}(I) = \texttt{saturate} ( \texttt{src2} - \texttt{src1}(I) ) \quad \texttt{if mask}(I) \ne0\],\\\\ ( ˶ˆ꒳ˆ˵ )\I (if mask}(I)
" where I is a multi-dimensional index of array elements. In case of multi-channel arrays, each channel is processed independently.The first function in the list above can be replaced with matrix expressions:dst = src1 - src2;dst -= src1; // equivalent to subtract(dst, src1, dst);fragmentThe input arrays and the output array can all have the same or different depths. For example, you can subtract to 8-bit unsigned arrays and store the difference in a 16-bit signed array. Depth of the output array is determined by dtype parameter. In the second and third cases above, as well as in the first case, when src1.depth() == src2.depth(), dtype can be set to the default -1. In this case the output array will have the same depth as the input array, be it src1, src2 or both.NoteSaturation is not applied when the output array has the depth CV_32S. You may even get result of an incorrect sign in the case of overflow.See alsoadd, addWeighted, scaleAdd, Mat::convertToExamples: samples/cpp/image_alignment.cpp."," ここで I は，配列要素の多次元インデックスです．上のリストの最初の関数は，行列式に置き換えることができます： dst = src1 - src2;dst -= src1; // subtract(dst, src1, dst) と等価です;fragment入力配列と出力配列のビット深度は，すべて同じでも異なっていても構いません．例えば，8 ビットの符号なし配列に対して減算を行い，その差を 16 ビットの符号付き配列に格納することができます．出力配列の深さは，dtypeパラメータで決まります．上記の 2 番目と 3 番目のケース，そして最初のケースと同様に， src1.depth() == src2.depth() の場合， dtype をデフォルトの -1 に設定することができます． この場合，出力配列のビット深度は， src1，src2，またはその両方の入力配列と同じになります．See alsoadd, addWeighted, scaleAdd, Mat::convertToExamples: samples/cpp/image_alignment.cpp."
Calculates the per-element scaled product of two arrays.,2 つの配列の要素毎のスケーリングされた積を求めます．
src1 : first input array.,src1 : 1 番目の入力配列．
src2 : second input array of the same size and the same type as src1.,src2 : src1 と同じサイズ，同じ型の 2 番目の入力配列．
dst : output array of the same size and type as src1.,dst : src1 と同じサイズ，同じ型の出力配列．
scale : optional scale factor.,scale : オプションのスケールファクター．
"The function multiply calculates the per-element product of two arrays:\[\texttt{dst} (I)= \texttt{saturate} ( \texttt{scale} \cdot \texttt{src1} (I) \cdot \texttt{src2} (I))\]There is also a MatrixExpressions -friendly variant of the first function. See Mat::mul .For a not-per-element matrix product, see gemm .NoteSaturation is not applied when the output array has the depth CV_32S. You may even get result of an incorrect sign in the case of overflow.See alsoadd, subtract, divide, scaleAdd, addWeighted, accumulate, accumulateProduct, accumulateSquare, Mat::convertTo","関数 multiply は，2つの配列の要素毎の積を計算します： ˶ˆ꒳ˆ˵ )(I)= \\\ ( ˶ˆ꒳ˆ˵ )\♪♪♪♪♪～(I）\\\\(I))には，MatrixExpressions に適した最初の関数もあります．Mat::mul を参照してください．また，要素毎ではない行列積については， gemm を参照してください．また，add, subtract, divide, scaleAdd, addWeighted, accumulate, accumulateProduct, accumulateSquare, Mat::convertTo も参照してください．"
Performs per-element division of two arrays or a scalar by an array.,2 つの配列，あるいはスカラを配列で割る，要素毎の除算を行います．
src2 : second input array of the same size and type as src1.,src2 : src1 と同じサイズ，同じ型の 2 番目の入力配列．
scale : scalar factor.,scale : スカラ倍数．
dst : output array of the same size and type as src2.,dst : src2 と同じサイズ，同じ型の出力配列．
"dtype : optional depth of the output array; if -1, dst will have depth src2.depth(), but in case of an array-by-array division, you can only pass -1 when src1.depth()==src2.depth().",dtype : オプションである出力配列のビット深度． -1 の場合，dst のビット深度は src2.depth() となりますが，配列毎の分割の場合は， src1.depth()==src2.depth() の時にのみ -1 を渡すことができます．
"The function cv::divide divides one array by another:\[\texttt{dst(I) = saturate(src1(I)*scale/src2(I))}\]or a scalar by an array when there is no src1 :\[\texttt{dst(I) = saturate(scale/src2(I))}\]Different channels of multi-channel arrays are processed independently.For integer types when src2(I) is zero, dst(I) will also be zero.NoteIn case of floating point data there is no special defined behavior for zero src2(I) values. Regular floating-point division is used. Expect correct IEEE-754 behaviour for floating-point data (with NaN, Inf result values).",関数 cv::divide は，ある配列を別の配列で分割します．また，src1 が存在しない場合は，スカラを配列で分割します．整数型の場合，src2(I)が0のとき，dst(I)も0になります．注：浮動小数点データの場合，src2(I)が0のときの特別な動作は定義されていません．通常の浮動小数点演算が行われます。浮動小数点データ（NaN，Infの結果値を含む）に対するIEEE754の正しい動作を期待してください。
"Saturation is not applied when the output array has the depth CV_32S. You may even get result of an incorrect sign in the case of overflow.See alsomultiply, add, subtractExamples: samples/dnn/classification.cpp.","出力配列のビット深度が CV_32S の場合，Saturation は適用されません．他にも，omultiply, add, subtractExamples: samples/dnn/classification.cpp を参照してください．"
Calculates the sum of a scaled array and another array.,スケーリングされた配列と別の配列の和を計算します。
alpha : scale factor for the first array.,alpha : 最初の配列のスケールファクタ．
"The function scaleAdd is one of the classical primitive linear algebra operations, known as DAXPY or SAXPY in BLAS. It calculates the sum of a scaled array and another array:\[\texttt{dst} (I)= \texttt{scale} \cdot \texttt{src1} (I) + \texttt{src2} (I)\]The function can also be emulated with a matrix expression, for example:Mat A(3, 3, CV_64F);...A.row(0) = A.row(1)*2 + A.row(2);fragmentSee alsoadd, addWeighted, subtract, Mat::dot, Mat::convertTo","関数 scaleAdd は，古典的な原始線形代数演算の一つで，BLAS では DAXPY または SAXPY として知られています．スケーリングされた配列と別の配列の和を計算します。(I)= \\\\\\\\(I) + ˶‾᷄ -̫ ‾᷅˵˵(Mat A(3, 3, CV_64F);...A.row(0) = A.row(1)*2 + A.row(2);fragmentSee alsoadd, addWeighted, subtract, Mat::dot, Mat::convertTo"
Calculates the weighted sum of two arrays.,2 つの配列の加重和を求めます．
alpha : weight of the first array elements.,alpha : 1 番目の配列要素の重み．
src2 : second input array of the same size and channel number as src1.,src2 : src1 と同じサイズ，同じチャンネル番号の 2 番目の入力配列．
beta : weight of the second array elements.,beta : 2 番目の配列要素の重み．
gamma : scalar added to each sum.,gamma : それぞれの和に加えられるスカラ．
dst : output array that has the same size and number of channels as the input arrays.,dst : 入力配列と同じサイズ，同じチャンネル数を持つ出力配列．
"dtype : optional depth of the output array; when both input arrays have the same depth, dtype can be set to -1, which will be equivalent to src1.depth().",dtype : オプションとして，出力配列のビット深度を指定します．両方の入力配列が同じビット深度の場合は， dtype を -1 に設定することができ，これは src1.depth() と同じになります．
"The function addWeighted calculates the weighted sum of two arrays as follows:\[\texttt{dst} (I)= \texttt{saturate} ( \texttt{src1} (I)* \texttt{alpha} + \texttt{src2} (I)* \texttt{beta} + \texttt{gamma} )\]where I is a multi-dimensional index of array elements. In case of multi-channel arrays, each channel is processed independently. The function can be replaced with a matrix expression:dst = src1*alpha + src2*beta + gamma;fragmentNoteSaturation is not applied when the output array has the depth CV_32S. You may even get result of an incorrect sign in the case of overflow.See alsoadd, subtract, scaleAdd, Mat::convertToExamples: samples/cpp/tutorial_code/HighGUI/AddingImagesTrackbar.cpp, samples/cpp/tutorial_code/ImgTrans/Sobel_Demo.cpp, and samples/dnn/segmentation.cpp.","関数 addWeighted は，次のように2つの配列の加重和を計算します： ˶‾᷄ -̫ ‾᷅˵ ˶‾᷅˵ ˶‾᷅˵ ˶‾᷄ -̫ ‾᷄˵(I)= ˶ˆ꒳ˆ˵ ( ˶ˆ꒳ˆ˵ )(I）* ˶ˆ꒳ˆ˵ + ˶ˆ꒳ˆ˵(I)* ˶ˆ꒳ˆ˵ )]ここで I は，配列要素の多次元インデックスです．マルチチャンネル配列の場合は，各チャンネルが独立して処理されます．この関数は，行列式に置き換えることができます： dst = src1*alpha + src2*beta + gamma;fragment注 出力配列のビット深度が CV_32S の場合，彩度は適用されません．Add, subtract, scaleAdd, Mat::convertToExamples: samples/cpp/tutorial_code/HighGUI/AddingImagesTrackbar.cpp, samples/cpp/tutorial_code/ImgTrans/Sobel_Demo.cpp, samples/dnn/segmentation.cpp も参照してください．"
"Scales, calculates absolute values, and converts the result to 8-bit.",スケーリングを行い、絶対値を計算し、結果を8ビットに変換します。
src : input array.,src : 入力配列．
dst : output array.,dst : 出力配列．
alpha : optional scale factor.,alpha : オプションのスケールファクター．
beta : optional delta added to the scaled values.,beta : オプションで，スケーリングされた値に追加されるデルタ値．
"On each element of the input array, the function convertScaleAbs performs three operations sequentially: scaling, taking an absolute value, conversion to an unsigned 8-bit type:\[\texttt{dst} (I)= \texttt{saturate\_cast<uchar>} (| \texttt{src} (I)* \texttt{alpha} + \texttt{beta} |)\]In case of multi-channel arrays, the function processes each channel independently. When the output is not 8-bit, the operation can be emulated by calling the Mat::convertTo method (or by using matrix expressions) and then by calculating an absolute value of the result. For example:Mat_<float> A(30,30);randu(A, Scalar(-100), Scalar(100));Mat_<float> B = A*5 + 3;B = abs(B);// Mat_<float> B = abs(A*5+3) will also do the job,// but it will allocate a temporary matrixfragmentSee alsoMat::convertTo, cv::abs(const Mat&)Examples: samples/cpp/laplace.cpp, and samples/cpp/tutorial_code/ImgTrans/Sobel_Demo.cpp.","関数 convertScaleAbs は，入力配列の各要素に対して，スケーリング，絶対値の取得，符号なし8ビット型への変換，という3つの処理を順次行います．(I)= \\\\(| | src})(I)* ˶ˆ꒳ˆ˵ )]マルチチャンネル配列の場合は，各チャンネルを独立して処理します．出力が 8 ビットではない場合， Mat::convertTo メソッドを呼び出して（あるいは，行列式を利用して），その結果の絶対値を計算することで，この処理をエミュレートすることができます．例えば，Mat_<float> A(30,30);randu(A, Scalar(-100), Scalar(100));Mat_<float> B = A*5 + 3;B = abs(B);// Mat_<float> B = abs(A*5+3) でも動作しますが，一時的な行列フラグメントが確保されます． 参照：Mat::convertTo, cv::abs(const Mat&)例： samples/cpp/laplace.cpp, および samples/cpp/tutorial_code/ImgTrans/Sobel_Demo.cpp."
Converts an array to half precision floating number.,配列を半精度浮動小数点数に変換します．
"This function converts FP32 (single precision floating point) from/to FP16 (half precision floating point). CV_16S format is used to represent FP16 data. There are two use modes (src -> dst): CV_32F -> CV_16S and CV_16S -> CV_32F. The input array has to have type of CV_32F or CV_16S to represent the bit depth. If the input array is neither of them, the function will raise an error. The format of half precision floating point is defined in IEEE 754-2008.",この関数は，FP32（単精度浮動小数点）をFP16（半精度浮動小数点）と相互に変換します．CV_16S フォーマットは，FP16 データを表現するために利用されます．2つの使用モード（ src -> dst ）があります．CV_32F -> CV_16S と CV_16S -> CV_32F です．CV_32F -> CV_16S と CV_16S -> CV_32F です．ビット深度を表すためには，入力配列の型が CV_32F か CV_16S でなければいけません．入力配列がそのどちらでもない場合，この関数はエラーを発生させます．半精度浮動小数点のフォーマットは，IEEE 754-2008 で定義されています．
Performs a look-up table transform of an array.,配列に対して，ルックアップテーブル変換を行います．
src : input array of 8-bit elements.,src : 8ビット要素の入力配列．
"lut : look-up table of 256 elements; in case of multi-channel input array, the table should either have a single channel (in this case the same table is used for all channels) or the same number of channels as in the input array.",lut : 256 個の要素を持つルックアップテーブル．マルチチャンネルの入力配列の場合，テーブルはシングルチャンネル（この場合，すべてのチャンネルに同じテーブルが用いられます）か，入力配列と同じ数のチャンネルを持つ必要があります．
"dst : output array of the same size and number of channels as src, and the same depth as lut.",dst : src と同じサイズ，同じチャンネル数，そして lut と同じビット深度を持つ出力配列．
"The function LUT fills the output array with values from the look-up table. Indices of the entries are taken from the input array. That is, the function processes each element of src as follows:\[\texttt{dst} (I) \leftarrow \texttt{lut(src(I) + d)}\]where\[d = \fork{0}{if \(\texttt{src}\) has depth \(\texttt{CV_8U}\)}{128}{if \(\texttt{src}\) has depth \(\texttt{CV_8S}\)}\]See alsoconvertScaleAbs, Mat::convertTo",関数 LUT は，ルックアップテーブルの値で出力配列を埋めます．エントリのインデックスは，入力配列から取得されます．つまり，この関数は src の各要素を以下のように処理します．\d = ˶ˆ꒳ˆ˵ ) if ˶ˆ꒳ˆ˵ )Mat::convertTo
Calculates the sum of array elements.,配列の要素の和を求めます．
src : input array that must have from 1 to 4 channels.,src : 1 から 4 までのチャンネルを持つ入力配列．
"The function cv::sum calculates and returns the sum of array elements, independently for each channel.See alsocountNonZero, mean, meanStdDev, norm, minMaxLoc, reduce","関数 cv::sum は，各チャンネル毎に配列要素の総和を計算して返します． 参照：ocountNonZero, mean, meanStdDev, norm, minMaxLoc, reduce"
Counts non-zero array elements.,ゼロではない配列要素を数えます．
src : single-channel array.,src : シングルチャンネルの配列．
"The function returns the number of non-zero elements in src :\[\sum _{I: \; \texttt{src} (I) \ne0 } 1\]See alsomean, meanStdDev, norm, minMaxLoc, calcCovarMatrix",この関数は， src に含まれる 0 ではない要素の数を返します :⋈⋈◍＞◡̈*)
Returns the list of locations of non-zero pixels.,0 ではないピクセルの位置のリストを返します．
src : single-channel array,src : シングルチャンネル配列．
"idx : the output array, type of cv::Mat or std::vector<Point>, corresponding to non-zero indices in the input",idx : cv::Mat または std::vector<Point> 型の出力配列で，入力の 0 ではない添字に対応します．
"Given a binary matrix (likely returned from an operation such as threshold(), compare(), >, ==, etc, return all of the non-zero indices as a cv::Mat or std::vector<cv::Point> (x,y) For example:cv::Mat binaryImage; // input, binary imagecv::Mat locations;   // output, locations of non-zero pixelscv::findNonZero(binaryImage, locations);// access pixel coordinatesPoint pnt = locations.at<Point>(i);fragmentorcv::Mat binaryImage; // input, binary imagevector<Point> locations;   // output, locations of non-zero pixelscv::findNonZero(binaryImage, locations);// access pixel coordinatesPoint pnt = locations[i];fragment","2値行列（おそらく，threshold()，compare()，>，==などの処理から返されます）が与えられた場合，すべての非ゼロインデックスを cv::Mat または std::vector<cv::Point> (x,y) として返します．例えば， cv::Mat binaryImage; // 入力であるバイナリ画像ecv::Mat locations; // 出力である，非ゼロピクセルの位置cv::findNonZero(binaryImage, locations);// ピクセル座標へのアクセスPoint pnt = locations.at<Point>(i);fragmentorcv::Mat binaryImage; // 入力，バイナリイメージevector<Point> locations; // 出力，0ではないピクセルの位置cv::findNonZero(binaryImage, locations);// アクセスピクセル座標Point pnt = locations[i];fragment"
Calculates an average (mean) of array elements.,配列の要素の平均値を求めます．
src : input array that should have from 1 to 4 channels so that the result can be stored in Scalar_ .,src : 入力配列．結果が Scalar_ に格納されるように，1から4までのチャンネルを持ちます．
mask : optional operation mask.,mask : オプションである処理マスク．
"The function cv::mean calculates the mean value M of array elements, independently for each channel, and return it:\[\begin{array}{l} N = \sum _{I: \; \texttt{mask} (I) \ne 0} 1 \\ M_c = \left ( \sum _{I: \; \texttt{mask} (I) \ne 0}{ \texttt{mtx} (I)_c} \right )/N \end{array}\]When all the mask elements are 0's, the function returns Scalar::all(0)See alsocountNonZero, meanStdDev, norm, minMaxLocExamples: samples/dnn/classification.cpp, samples/dnn/object_detection.cpp, and samples/dnn/segmentation.cpp.","関数 cv::mean は，各チャンネル毎に配列要素の平均値 M を求め，それを返します．N = sum _{I:\♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪(1 M_c = ""N"" = ""Sum _{I:\♪♪～(I) eldest son}{ eldest son(I)_c}\end{array}\]すべてのマスク要素が0のとき，Scalar::all(0)を返します． 参照：ocountNonZero, meanStdDev, norm, minMaxLocExamples: samples/dn/classification.cpp, samples/dn/object_detection.cpp, samples/dn/segmentation.cpp."
Calculates a mean and standard deviation of array elements.,配列の要素の平均値と標準偏差を計算します．
src : input array that should have from 1 to 4 channels so that the results can be stored in Scalar_ 's.,src : 入力配列．結果が Scalar_ 's に格納されるように，1から4までのチャンネルを持ちます．
mean : output parameter: calculated mean value.,mean : 出力パラメータ：計算された平均値．
stddev : output parameter: calculated standard deviation.,stddev : 出力パラメータ：計算された標準偏差．
"The function cv::meanStdDev calculates the mean and the standard deviation M of array elements independently for each channel and returns it via the output parameters:\[\begin{array}{l} N = \sum _{I, \texttt{mask} (I) \ne 0} 1 \\ \texttt{mean} _c = \frac{\sum_{ I: \; \texttt{mask}(I) \ne 0} \texttt{src} (I)_c}{N} \\ \texttt{stddev} _c = \sqrt{\frac{\sum_{ I: \; \texttt{mask}(I) \ne 0} \left ( \texttt{src} (I)_c - \texttt{mean} _c \right )^2}{N}} \end{array}\]When all the mask elements are 0's, the function returns mean=stddev=Scalar::all(0).NoteThe calculated standard deviation is only the diagonal of the complete normalized covariance matrix. If the full matrix is needed, you can reshape the multi-channel array M x N to the single-channel array M*N x mtx.channels() (only possible when the matrix is continuous) and then pass the matrix to calcCovarMatrix .See alsocountNonZero, mean, norm, minMaxLoc, calcCovarMatrix","関数 cv::meanStdDev は，各チャンネル毎に，配列要素の平均値と標準偏差 M を計算し，それを出力パラメータとして返します．N = sum _{I, ˶ˆ꒳ˆ˵1_c = ˶ˆ꒳ˆ˵ )\I）\\ 0} ୨୧(I)_c}{N}\\ ♪♪～\\\ 0} ୨୧ ( ˶ˆ꒳ˆ˵ )(I）_c - ˶ˆ꒳ˆ˵ )_c )^2}{N}}\注意：計算された標準偏差は、完全な正規化共分散行列の対角線上にしかありません。完全な行列が必要ならば，マルチチャンネル配列 M x N をシングルチャンネル配列 M*N x mtx.channels() に変換し（行列が連続である場合のみ可能），その行列を calcCovarMatrix に渡します． 他にも，ocountNonZero, mean, norm, minMaxLoc, calcCovarMatrix を参照してください．"
Calculates the absolute norm of an array.,配列の絶対値ノルムを求めます．
normType : type of the norm (see NormTypes).,normType : ノルムのタイプ（NormTypesを参照）．
mask : optional operation mask; it must have the same size as src1 and CV_8UC1 type.,mask : オプションであるオペレーションマスク．
"This version of norm calculates the absolute norm of src1. The type of norm to calculate is specified using NormTypes.As example for one array consider the function \(r(x)= \begin{pmatrix} x \\ 1-x \end{pmatrix}, x \in [-1;1]\). The \( L_{1}, L_{2} \) and \( L_{\infty} \) norm for the sample value \(r(-1) = \begin{pmatrix} -1 \\ 2 \end{pmatrix}\) is calculated as follows\begin{align*} \| r(-1) \|_{L_1} &= |-1| + |2| = 3 \\ \| r(-1) \|_{L_2} &= \sqrt{(-1)^{2} + (2)^{2}} = \sqrt{5} \\ \| r(-1) \|_{L_\infty} &= \max(|-1|,|2|) = 2 \end{align*}and for \(r(0.5) = \begin{pmatrix} 0.5 \\ 0.5 \end{pmatrix}\) the calculation is\begin{align*} \| r(0.5) \|_{L_1} &= |0.5| + |0.5| = 1 \\ \| r(0.5) \|_{L_2} &= \sqrt{(0.5)^{2} + (0.5)^{2}} = \sqrt{0.5} \\ \| r(0.5) \|_{L_\infty} &= \max(|0.5|,|0.5|) = 0.5. \end{align*}The following graphic shows all values for the three norm functions \(\| r(x) \|_{L_1}, \| r(x) \|_{L_2}\) and \(\| r(x) \|_{L_\infty}\). It is notable that the \( L_{1} \) norm forms the upper and the \( L_{\infty} \) norm forms the lower border for the example function \( r(x) \).Graphs for the different norm functions from the above exampleWhen the mask parameter is specified and it is not empty, the norm isIf normType is not specified, NORM_L2 is used. calculated only over the region specified by the mask.Multi-channel input arrays are treated as single-channel arrays, that is, the results for all channels are combined.Hamming norms can only be calculated with CV_8U depth arrays.","このバージョンの norm は， src1 の絶対値ノルムを計算します．計算するノルムの種類は，NormTypesを用いて指定します．1つの配列の例として，関数 \(r(x)= origin{pmatrix} x ˶ˆ꒳ˆ˵, x ˶ˆ꒳ˆ˵) を考えます．サンプル値のノルムである˶( L_{1}, L_{2} ˶ )は、˶( L_{infty} ˶ )と同じです。-1 ♯2 ♯2 ♯2 ♯2 ♯2 ♯2 ♯2 ♯2 ♯2 ♯2 ♯2 ♯2 ♯2 ♯2 ♯2...\| r_{L_1} &= |-1| + |2| = 3 R_{L_2} &= |sqrt{(-1)^{2} + (2)^{2}}。+ (2)^{2}} = ˶ˆ꒳ˆ˵ )\\ R(-1)0.5\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\| 0.5+0.5+=1 ¶ r(0.5) ¶ L_2 ¶ &= ¶sqrt{(0.5)^{2}.+ (0.5)^{2}} = ″0.5″になります。\\ r(0.5) &= ~max(|0.5|,|0.5|) = 0.5.\end{align*}次の図は、3つのノルム関数\\ r(x), \ r(x), \ r(x), \ r(x), ∮ r(x), ∮ r(x), ∮ r(x), ∮ r(x), ∮ r(x), ∮ r(x), ∮ r(x), ∮ r(x), ∮ r(x), ∮ r(x), ∮ r(x).例題の関数 ˶( r(x) ˶ )の場合、上側が ˶( L_{1} ˶ )ノルム、下側が ˶( L_{infty} ˶ )ノルムになっていることがわかります。マルチチャンネル入力配列は，シングルチャンネル配列として扱われます．つまり，すべてのチャンネルの結果が組み合わされます．"
Calculates an absolute difference norm or a relative difference norm.,絶対差分ノルム，または相対差分ノルムを求めます．
This version of cv::norm calculates the absolute difference norm or the relative difference norm of arrays src1 and src2. The type of norm to calculate is specified using NormTypes.,このバージョンの cv::norm は，配列 src1 と src2 の絶対差分ノルム，あるいは相対差分ノルムを求めます．計算するノルムの種類は， NormTypes を用いて指定します．
Computes the Peak Signal-to-Noise Ratio (PSNR) image quality metric.,画質指標である PSNR（Peak Signal-to-Noise Ratio）を計算します．
src2 : second input array of the same size as src1.,src2 : src1 と同じサイズの2番目の入力配列．
R : the maximum pixel value (255 by default),R : 最大のピクセル値（デフォルトでは 255）．
"This function calculates the Peak Signal-to-Noise Ratio (PSNR) image quality metric in decibels (dB), between two input arrays src1 and src2. The arrays must have the same type.The PSNR is calculated as follows:\[ \texttt{PSNR} = 10 \cdot \log_{10}{\left( \frac{R^2}{MSE} \right) } \]where R is the maximum integer value of depth (e.g. 255 in the case of CV_8U data) and MSE is the mean squared error between the two arrays.",この関数は，2つの入力配列 src1 と src2 の間の，デシベル（dB）単位で表されるピーク信号対雑音比（PSNR）の画質を求めます．PSNR は，次のように計算されます： ˶‾᷄ -̫ ‾᷅˵ Rは深度の最大整数値（CV_8Uデータの場合は255），MSEは2つの配列間の平均2乗誤差です．
naive nearest neighbor finder,ナイーブな最近傍探索機
see http://en.wikipedia.org/wiki/Nearest_neighbor_searchTodo:document,http://en.wikipedia.org/wiki/Nearest_neighbor_searchTodo:document を参照してください．
Normalizes the norm or value range of an array.,配列のノルムや値域を正規化します．
dst : output array of the same size as src .,dst : src と同じサイズの出力配列．
alpha : norm value to normalize to or the lower range boundary in case of the range normalization.,alpha : 正規化するためのノルム値，または範囲正規化の場合は範囲の下限境界．
beta : upper range boundary in case of the range normalization; it is not used for the norm normalization.,beta : 範囲正規化を行う際の上限境界．
norm_type : normalization type (see cv::NormTypes).,norm_type : 正規化のタイプ（ cv::NormTypes を参照してください）．
"dtype : when negative, the output array has the same type as src; otherwise, it has the same number of channels as src and the depth =CV_MAT_DEPTH(dtype).",dtype : 負の値の場合，出力配列は src と同じ型になります．そうでない場合は， src と同じチャンネル数を持ち，深度は =CV_MAT_DEPTH(dtype) となります．
"The function cv::normalize normalizes scale and shift the input array elements so that\[\| \texttt{dst} \| _{L_p}= \texttt{alpha}\](where p=Inf, 1 or 2) when normType=NORM_INF, NORM_L1, or NORM_L2, respectively; or so that\[\min _I \texttt{dst} (I)= \texttt{alpha} , \, \, \max _I \texttt{dst} (I)= \texttt{beta}\]when normType=NORM_MINMAX (for dense arrays only). The optional mask specifies a sub-array to be normalized. This means that the norm or min-n-max are calculated over the sub-array, and then this sub-array is modified to be normalized. If you want to only use the mask to calculate the norm or min-max but modify the whole array, you can use norm and Mat::convertTo.In case of sparse matrices, only the non-zero values are analyzed and transformed. Because of this, the range transformation for sparse matrices is not allowed since it can shift the zero level.Possible usage with some positive example data:vector<double> positiveData = { 2.0, 8.0, 10.0 };vector<double> normalizedData_l1, normalizedData_l2, normalizedData_inf, normalizedData_minmax;// Norm to probability (total count)// sum(numbers) = 20.0// 2.0      0.1     (2.0/20.0)// 8.0      0.4     (8.0/20.0)// 10.0     0.5     (10.0/20.0)normalize(positiveData, normalizedData_l1, 1.0, 0.0, NORM_L1);// Norm to unit vector: ||positiveData|| = 1.0// 2.0      0.15// 8.0      0.62// 10.0     0.77normalize(positiveData, normalizedData_l2, 1.0, 0.0, NORM_L2);// Norm to max element// 2.0      0.2     (2.0/10.0)// 8.0      0.8     (8.0/10.0)// 10.0     1.0     (10.0/10.0)normalize(positiveData, normalizedData_inf, 1.0, 0.0, NORM_INF);// Norm to range [0.0;1.0]// 2.0      0.0     (shift to left border)// 8.0      0.75    (6.0/8.0)// 10.0     1.0     (shift to right border)normalize(positiveData, normalizedData_minmax, 1.0, 0.0, NORM_MINMAX);fragmentSee alsonorm, Mat::convertTo, SparseMat::convertTo","関数 cv::normalize は，normType=NORM_INF, NORM_L1, NORM_L2 の場合に，それぞれ以下のように入力配列の要素をスケーリングしたりシフトしたりして正規化します．(I)= ˶ˆ꒳ˆ˵ ) , ˶ˆ꒳ˆ˵ ) , ˶ˆ꒳ˆ˵ )(normType=NORM_MINMAX（密な配列の場合のみ）の場合は， [I (I)= ˶‾᷄ -̫ ‾᷅˵]となります．）オプションのマスクは，正規化されるサブアレイを指定します．これは，副配列に対してノルムやmin-n-maxが計算され，その後，この副配列が正規化されるように修正されることを意味します。ノルムやミニマムを計算するためにマスクを利用するだけで，配列全体を修正したい場合は，norm と Mat::convertTo を利用できます．疎な行列の場合は，ゼロではない値のみが分析・変換されます．このため，ゼロレベルを移動させることができるので，疎な行列に対する範囲変換はできません．いくつかの正の例示データを用いた可能な利用法： vector<double> positiveData = { 2.0, 8.0, 10.0 };vector<double> normalizedData_l1, normalizedData_l2, normalizedData_inf, normalizedData_minmax;// Norm to probability (total count)// sum(numbers) = 20.0// 2.0.1 0.1 (2.0/20.0)// 8.0 0.4 (8.0/20.0)// 10.0 0.5 (10.0/20.0)normalize(positiveData, normalizedData_l1, 1.0, 0.0, NORM_L1);// 単位ベクトルへのノーム。||positiveData|= 1.0//2.0 0.15//8.0 0.62//10.0 0.77normalize(positiveData, normalizedData_l2, 1.0, 0.0, NORM_L2);// 最大要素へのノルム//2.0.2 0.2 (2.0/10.0)// 8.0 0.8 (8.0/10.0)// 10.0 1.0 (10.0/10.0)normalize(positiveData, normalizedData_inf, 1.0, 0.0, NORM_INF);// 範囲に合わせたノーム [0.0;1.0]// 2.0 0.0 (左境界への移動)// 8.0 0.75 (6.0/8.0)// 10.0 1.0 (右境界への移動)normalize(positiveData, normalizedData_minmax, 1.0, 0.0, NORM_MINMAX);fragmentSee alsonorm, Mat::convertTo, SparseMat::convertTo"
Finds the global minimum and maximum in an array.,配列のグローバルな最小値と最大値を求めます．
src : input single-channel array.,src : 入力されるシングルチャンネル配列．
minVal : pointer to the returned minimum value; NULL is used if not required.,minVal : 返される最小値へのポインタ．必要なければ，NULLが用いられます．
maxVal : pointer to the returned maximum value; NULL is used if not required.,maxVal : 返される最大値へのポインタ，必要なければ NULL が用いられます．
minLoc : pointer to the returned minimum location (in 2D case); NULL is used if not required.,minLoc : 返される最小値の位置へのポインタ（2Dの場合）; 必要なければ，NULLが用いられます．
maxLoc : pointer to the returned maximum location (in 2D case); NULL is used if not required.,maxLoc : (2Dの場合)最大値の位置を示すポインタ．
mask : optional mask used to select a sub-array.,mask : 部分配列を選択するためのオプションマスク．
"The function cv::minMaxLoc finds the minimum and maximum element values and their positions. The extremums are searched across the whole array or, if mask is not an empty array, in the specified array region.The function do not work with multi-channel arrays. If you need to find minimum or maximum elements across all the channels, use Mat::reshape first to reinterpret the array as single-channel. Or you may extract the particular channel using either extractImageCOI , or mixChannels , or split .See alsomax, min, compare, inRange, extractImageCOI, mixChannels, split, Mat::reshapeExamples: samples/cpp/image_alignment.cpp, samples/cpp/tutorial_code/Histograms_Matching/MatchTemplate_Demo.cpp, samples/dnn/classification.cpp, samples/dnn/object_detection.cpp, and samples/dnn/openpose.cpp.","関数 cv::minMaxLoc は，要素の最小値と最大値，そしてそれらの位置を求めます．極値は，配列全体，あるいは mask が空の配列ではない場合は，指定された配列領域で探索されます．この関数は，マルチチャンネル配列では動作しません．すべてのチャンネルで最小や最大の要素を見つける必要がある場合は，まず Mat::reshape を利用して配列をシングルチャンネルとして再解釈します．あるいは， extractImageCOI ， mixChannels ， split のいずれかを用いて特定のチャンネルを抽出します． 関連項目：omax, min, compare, inRange, extractImageCOI, mixChannels, split, Mat::reshapeExamples: samples/cpp/image_alignment.cpp, samples/cpp/tutorial_code/Histograms_Matching/MatchTemplate_Demo.cpp, samples/dnn/classification.cpp, samples/dnn/object_detection.cpp, and samples/dnn/openpose.cpp."
"minIdx : pointer to the returned minimum location (in nD case); NULL is used if not required; Otherwise, it must point to an array of src.dims elements, the coordinates of the minimum element in each dimension are stored there sequentially.",minIdx : 返される最小値の位置へのポインタ（nDの場合）．不要な場合は，NULLが用いられます．それ以外の場合は， src.dims 要素の配列を指し，各次元の最小要素の座標が順次格納されます．
maxIdx : pointer to the returned maximum location (in nD case). NULL is used if not required.,maxIdx : 返される最大値の座標へのポインタ（nDの場合）．必要ない場合は，NULLが用いられます．
mask : specified array region,mask : 指定された配列領域．
"The function cv::minMaxIdx finds the minimum and maximum element values and their positions. The extremums are searched across the whole array or, if mask is not an empty array, in the specified array region. The function does not work with multi-channel arrays. If you need to find minimum or maximum elements across all the channels, use Mat::reshape first to reinterpret the array as single-channel. Or you may extract the particular channel using either extractImageCOI , or mixChannels , or split . In case of a sparse matrix, the minimum is found among non-zero elements only.NoteWhen minIdx is not NULL, it must have at least 2 elements (as well as maxIdx), even if src is a single-row or single-column matrix. In OpenCV (following MATLAB) each array has at least 2 dimensions, i.e. single-column matrix is Mx1 matrix (and therefore minIdx/maxIdx will be (i1,0)/(i2,0)) and single-row matrix is 1xN matrix (and therefore minIdx/maxIdx will be (0,j1)/(0,j2)).","関数 cv::minMaxIdx は，最小と最大の要素の値とその位置を求めます．極値は，配列全体，あるいは mask が空の配列ではない場合には，指定された配列領域内で探索されます．この関数は，マルチチャンネル配列では動作しません．すべてのチャンネルで最小や最大の要素を見つける必要がある場合は，最初に Mat::reshape を利用して配列をシングルチャンネルとして再解釈します．あるいは， extractImageCOI や mixChannels ，split などを用いて，特定のチャンネルを抽出しても構いません．疎な行列の場合，最小値は 0 ではない要素の中でのみ求められます．注意 minIdx が NULL ではない場合，たとえ src が 1 行や 1 列の行列であっても，（ maxIdx と同様に）少なくとも 2 つの要素を持たなければいけません．OpenCVでは（MATLABに倣って），各配列は少なくとも2次元です．つまり，1列の行列は Mx1 行列（したがって，minIdx/maxIdx は (i1,0)/(i2,0) となります），1列の行列は 1xN 行列（したがって，minIdx/maxIdx は (0,j1)/(0,j2) となります）．"
Reduces a matrix to a vector.,行列をベクトルに変換します。
src : input 2D matrix.,src : 入力 2 次元行列．
dst : output vector. Its size and type is defined by dim and dtype parameters.,dst : 出力ベクトル．そのサイズと型は， dim と dtype パラメータで定義されます．
dim : dimension index along which the matrix is reduced. 0 means that the matrix is reduced to a single row. 1 means that the matrix is reduced to a single column.,dim : 行列が縮小される次元のインデックス．0 は，行列が 1 行に縮小されることを意味します．1 は，行列が 1 列に縮小されることを意味します．
rtype : reduction operation that could be one of ReduceTypes,rtype : ReduceTypesの1つである縮小操作。
"dtype : when negative, the output vector will have the same type as the input matrix, otherwise, its type will be CV_MAKE_TYPE(CV_MAT_DEPTH(dtype), src.channels()).","dtype : 負の値の場合，出力ベクトルは入力行列と同じ型になり，そうでない場合は CV_MAKE_TYPE(CV_MAT_DEPTH(dtype), src.channels()) になります．"
"The function reduce reduces the matrix to a vector by treating the matrix rows/columns as a set of 1D vectors and performing the specified operation on the vectors until a single row/column is obtained. For example, the function can be used to compute horizontal and vertical projections of a raster image. In case of REDUCE_MAX and REDUCE_MIN , the output image should have the same type as the source one. In case of REDUCE_SUM and REDUCE_AVG , the output may have a larger element bit-depth to preserve accuracy. And multi-channel arrays are also supported in these two reduction modes.The following code demonstrates its usage for a single channel matrix.Mat m = (Mat_<uchar>(3,2) << 1,2,3,4,5,6);        Mat col_sum, row_sum;        reduce(m, col_sum, 0, REDUCE_SUM, CV_32F);        reduce(m, row_sum, 1, REDUCE_SUM, CV_32F);        /*        m =        [  1,   2;           3,   4;           5,   6]        col_sum =        [9, 12]        row_sum =        [3;         7;         11]         */fragmentAnd the following code demonstrates its usage for a two-channel matrix.// two channels        char d[] = {1,2,3,4,5,6};        Mat m(3, 1, CV_8UC2, d);        Mat col_sum_per_channel;        reduce(m, col_sum_per_channel, 0, REDUCE_SUM, CV_32F);        /*        col_sum_per_channel =        [9, 12]        */fragmentSee alsorepeat","関数 reduce は，行列の行/列を 1 次元ベクトルの集合として扱い，単一の行/列が得られるまで，そのベクトルに対して指定された処理を行うことで，行列をベクトルに変換します．例えば，この関数は，ラスター画像の水平・垂直方向の投影図を求めるために利用できます．REDUCE_MAX および REDUCE_MIN の場合，出力画像は入力画像と同じ型でなければいけません．REDUCE_SUM や REDUCE_AVG の場合，精度を保つために，出力の要素のビット深度を大きくすることができます．また，この2つのリダクションモードでは，マルチチャンネル配列もサポートされています．以下のコードは，シングルチャンネルの行列に対する使い方を示しています．Mat m = (Mat_<uchar>(3,2) << 1,2,3,4,5,6); Mat col_sum, row_sum; reduce(m, col_sum, 0, REDUCE_SUM, CV_32F); reduce(m, row_sum, 1, REDUCE_SUM, CV_32F); /* m = [ 1, 2;           3, 4; 5, 6] col_sum = [9, 12] row_sum = [3; 7; 11] */fragmentまた，次のコードは，2チャンネルの行列に対する使い方を示しています．// 2 チャンネル char d[] = {1,2,3,4,5,6}; Mat m(3, 1, CV_8UC2, d); Mat col_sum_per_channel; reduce(m, col_sum_per_channel, 0, REDUCE_SUM, CV_32F); /* col_sum_per_channel = [9, 12] */fragment他の項目も参照してください．"
mv : input vector of matrices to be merged; all the matrices in mv must have the same size and the same depth.,mv : 融合される行列の入力ベクトル，mv に含まれるすべての行列は，同じサイズ，同じビット深度でなければいけません．
dst : output array of the same size and the same depth as mv[0]; The number of channels will be the total number of channels in the matrix array.,dst : mv[0] と同じサイズ，同じビット深度の出力配列．チャンネル数は，行列配列のチャンネル数の合計になります．
Divides a multi-channel array into several single-channel arrays.,マルチチャンネル配列を，複数のシングルチャンネル配列に分割します．
src : input multi-channel array.,src : 入力マルチチャンネル配列．
"mvbegin : output array; the number of arrays must match src.channels(); the arrays themselves are reallocated, if needed.",mvbegin : 出力配列．配列の個数は src.channels() と一致しなければいけません．必要ならば，配列自体が再割り当てされます．
"The function cv::split splits a multi-channel array into separate single-channel arrays:\[\texttt{mv} [c](I) = \texttt{src} (I)_c\]If you need to extract a single channel or do some other sophisticated channel permutation, use mixChannels .The following example demonstrates how to split a 3-channel matrix into 3 single channel matrices.char d[] = {1,2,3,4,5,6,7,8,9,10,11,12};    Mat m(2, 2, CV_8UC3, d);    Mat channels[3];    split(m, channels);    /*    channels[0] =    [  1,   4;       7,  10]    channels[1] =    [  2,   5;       8,  11]    channels[2] =    [  3,   6;       9,  12]    */fragmentSee alsomerge, mixChannels, cvtColorExamples: samples/cpp/tutorial_code/videoio/video-write/video-write.cpp.","関数 cv::split は，マルチチャンネル配列を別々のシングルチャンネル配列に分割します．(以下の例では，3 チャンネルの行列を 3 つのシングルチャンネルの行列に分割しています．char d[] = {1,2,3,4,5,6,7,8,9,10,11,12}; Mat m(2, 2, CV_8UC3, d); Mat channels[3]; split(m, channels); /* channels[0] = [ 1, 4; 7, 10] channels[1] = [ 2, 5; 8, 11] channels[2] = [ 3, 6; 9, 12] */fragment 他にも，somerge, mixChannels, cvtColorExamples を参照してください．samples/cpp/tutorial_code/videoio/video-write/video-write.cpp を参照してください．"
src : input array or vector of matrices; all of the matrices must have the same size and the same depth.,src : 行列の入力配列またはベクトル．すべての行列は，同じサイズ，同じビット深度でなければいけません．
dst : output array or vector of matrices; all the matrices must be allocated; their size and depth must be the same as in src[0].,dst : 行列の出力配列またはベクトル．すべての行列が確保されている必要があり，そのサイズと深さは src[0] と同じでなければいけません．
"fromTo : array of index pairs specifying which channels are copied and where; fromTo[k*2] is a 0-based index of the input channel in src, fromTo[k*2+1] is an index of the output channel in dst; the continuous channel numbering is used: the first input image channels are indexed from 0 to src[0].channels()-1, the second input image channels are indexed from src[0].channels() to src[0].channels() + src[1].channels()-1, and so on, the same scheme is used for the output image channels; as a special case, when fromTo[k*2] is negative, the corresponding output channel is filled with zero .",fromTo : どのチャンネルをどこにコピーするかを指定するインデックスペアの配列； fromTo[k*2]は，src の入力チャンネルを示す0基準のインデックス， fromTo[k*2+1]は，dst の出力チャンネルを示すインデックス；連続したチャンネルナンバリングが用いられます： 1 番目の入力画像チャンネルは，0 から src[0] までのインデックスで表されます．channels()-1 で表されます．2番目の入力画像チャンネルは， src[0].channels() から src[0].channels() + src[1].channels()-1 までのインデックスで表され，以降，出力画像チャンネルも同様に表されます．
npairs : number of index pairs in fromTo.,npairs : fromTo のインデックスペアの数．
Extracts a single channel from src (coi is 0-based index),src から 1 つのチャンネルを抽出します（ coi は 0 ベースのインデックスです）．
src : input array,src : 入力配列
dst : output array,dst : 出力配列
coi : index of channel to extract,coi : 抽出するチャンネルのインデックス
"See alsomixChannels, splitExamples: samples/dnn/colorization.cpp.","alsomixChannels, splitExamples: samples/dnn/colorization.cpp を参照してください．"
Inserts a single channel to dst (coi is 0-based index),dst に 1 つのチャンネルを挿入します（coi は 0 ベースのインデックス）．
coi : index of channel for insertion,coi : 挿入するチャンネルのインデックス
"See alsomixChannels, merge","See alsomixChannels, merge"
"Flips a 2D array around vertical, horizontal, or both axes.",2 次元配列を，垂直，水平，または両軸で反転させます．
dst : output array of the same size and type as src.,dst : src と同じサイズ・タイプの出力配列．
"flipCode : a flag to specify how to flip the array; 0 means flipping around the x-axis and positive value (for example, 1) means flipping around y-axis. Negative value (for example, -1) means flipping around both axes.",flipCode : 配列の反転方法を指定するフラグ．0 は，X軸周りの反転を意味し，正の値（例えば，1）は，Y軸周りの反転を意味します．負の値（例えば，-1）は，両方の軸を中心に反転することを意味します．
"The function cv::flip flips the array in one of three different ways (row and column indices are 0-based):\[\texttt{dst} _{ij} = \left\{ \begin{array}{l l} \texttt{src} _{\texttt{src.rows}-i-1,j} & if\; \texttt{flipCode} = 0 \\ \texttt{src} _{i, \texttt{src.cols} -j-1} & if\; \texttt{flipCode} > 0 \\ \texttt{src} _{ \texttt{src.rows} -i-1, \texttt{src.cols} -j-1} & if\; \texttt{flipCode} < 0 \\ \end{array} \right.\]The example scenarios of using the function are the following: Vertical flipping of the image (flipCode == 0) to switch between top-left and bottom-left image origin. This is a typical operation in video processing on Microsoft Windows* OS. Horizontal flipping of the image with the subsequent horizontal shift and absolute difference calculation to check for a vertical-axis symmetry (flipCode > 0). Simultaneous horizontal and vertical flipping of the image with the subsequent shift and absolute difference calculation to check for a central symmetry (flipCode < 0). Reversing the order of point arrays (flipCode > 0 or flipCode == 0).See alsotranspose , repeat , completeSymmExamples: samples/cpp/facedetect.cpp, and samples/cpp/train_HOG.cpp.","関数 cv::flip は，3つの異なる方法のうちの1つで配列を反転させます（行と列のインデックスは 0 ベースです）．_{ij} = ˶ˆ꒳ˆ˵ )\♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪_{texttttt{src.rows}-i-1,j} & if\; ˶‾᷄ -̫ ‾᷅˵ ˶‾᷅˵ ˶‾᷄ -̫ ‾᷅˵_{i, ˶ˆ꒳ˆ˵ )-j-1} & if; ˶ˆ꒳ˆ˵ )> 0以上_{ ˶ˆ꒳ˆ˵ )-j-1} & if\; ˶ˆ꒳ˆ˵< 0 }\\\\この関数の使用例は以下の通りです。画像を垂直方向に反転させ（flipCode == 0），画像の原点を左上と左下に切り替える．Microsoft Windows*での動画処理でよく見られる操作です。垂直軸方向の対称性を確認するために、水平方向に反転させた後、水平方向にシフトさせ、差分の絶対値を計算する（flipCode > 0）。水平・垂直方向に同時に反転させ、その後シフトと絶対値の差を計算して中心軸の対称性を確認する（flipCode < 0）。点配列の順序を反転させる (flipCode > 0 or flipCode == 0).alsootranspose , repeat , completeSymmExamples: samples/cpp/facedetect.cpp, and samples/cpp/train_HOG.cppを参照してください。"
Rotates a 2D array in multiples of 90 degrees. The function cv::rotate rotates the array in one of three different ways: Rotate by 90 degrees clockwise (rotateCode = ROTATE_90_CLOCKWISE). Rotate by 180 degrees clockwise (rotateCode = ROTATE_180). Rotate by 270 degrees clockwise (rotateCode = ROTATE_90_COUNTERCLOCKWISE).,2 次元配列を 90 度の倍数で回転させます．関数 cv::rotate は，3つの異なる方法のうちの1つで配列を回転させます．時計回りに90度回転させます（rotateCode = ROTATE_90_CLOCKWISE）．時計回りに180度回転させます（rotateCode = ROTATE_180）．時計回りに270度回転させる(rotateCode = ROTATE_90_COUNTERCLOCKWISE)。
"dst : output array of the same type as src. The size is the same with ROTATE_180, and the rows and cols are switched for ROTATE_90_CLOCKWISE and ROTATE_90_COUNTERCLOCKWISE.",dst : src と同じ型の出力配列。サイズはROTATE_180と同じで、ROTATE_90_CLOCKWISEとROTATE_90_COUNTERCLOCKWISEでは、行と列が入れ替わります。
rotateCode : an enum to specify how to rotate the array; see the enum RotateFlags,rotateCode : 配列の回転方法を指定するための列挙型で， RotateFlags 列挙型を参照してください．
"See alsotranspose , repeat , completeSymm, flip, RotateFlags","関連項目：otranspose, repeat, completeSymm, flip, RotateFlags"
Fills the output array with repeated copies of the input array.,入力配列を繰り返しコピーして、出力配列を埋めます。
src : input array to replicate.,src : 複製される入力配列．
ny : Flag to specify how many times the src is repeated along the vertical axis.,ny : src を縦軸方向に何回繰り返すかを指定するフラグ．
nx : Flag to specify how many times the src is repeated along the horizontal axis.,nx : src が横軸に沿って何回繰り返されるかを表すフラグ．
dst : output array of the same type as src.,dst : src と同じ型の出力配列．
"The function cv::repeat duplicates the input array one or more times along each of the two axes:\[\texttt{dst} _{ij}= \texttt{src} _{i\mod src.rows, \; j\mod src.cols }\]The second variant of the function is more convenient to use with MatrixExpressions.See alsocv::reduce",関数 cv::repeat は，入力配列を2つの軸それぞれに沿って1回または複数回複製します：\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\⁾⁾。
Applies horizontal concatenation to given matrices.,与えられた行列に対して，水平方向の連結処理を行います．
src : input array or vector of matrices. all of the matrices must have the same number of rows and the same depth.,src : 行列の入力配列またはベクトル．すべての行列は，同じ行数，同じ深さでなければいけません．
nsrc : number of matrices in src.,nsrc : src に含まれる行列の数．
"dst : output array. It has the same number of rows and depth as the src, and the sum of cols of the src.",dst : 出力配列．src と同じ行数，同じビット深度，そして src の col の合計値を持ちます．
"The function horizontally concatenates two or more cv::Mat matrices (with the same number of rows).cv::Mat matArray[] = { cv::Mat(4, 1, CV_8UC1, cv::Scalar(1)),                       cv::Mat(4, 1, CV_8UC1, cv::Scalar(2)),                       cv::Mat(4, 1, CV_8UC1, cv::Scalar(3)),};cv::Mat out;cv::hconcat( matArray, 3, out );//out://[1, 2, 3;// 1, 2, 3;// 1, 2, 3;// 1, 2, 3]fragmentSee alsocv::vconcat(const Mat*, size_t, OutputArray), ","この関数は，（同じ行数の）2つ以上の cv::Mat 行列を水平方向に連結します．cv::Mat matArray[] = { cv::Mat(4, 1, CV_8UC1, cv::Scalar(1)), cv::Mat(4, 1, CV_8UC1, cv::Scalar(2)), cv::Mat(4, 1, CV_8UC1, cv::Scalar(3)),};cv::Mat out;cv::hconcat( matArray, 3, out );//out://[1, 2, 3;// 1, 2, 3;// 1, 2, 3]fragment また，socv::vconcat(const Mat*, size_t, OutputArray) も参照してください．"
"cv::vconcat(InputArrayOfArrays, OutputArray) and ","cv::vconcat(InputArrayOfArrays, OutputArray), および"
"cv::vconcat(InputArray, InputArray, OutputArray)Examples: samples/cpp/tutorial_code/features2D/Homography/homography_from_camera_displacement.cpp.","cv::vconcat(InputArray, InputArray, OutputArray)例: samples/cpp/tutorial_code/features2D/Homography/homography_from_camera_displacement.cpp."
Applies vertical concatenation to given matrices.,与えられた行列に対して、垂直方向の連結処理を行います。
src : input array or vector of matrices. all of the matrices must have the same number of cols and the same depth.,src : 行列の入力配列またはベクトル．すべての行列は，同じ行数，同じ深さでなければいけません．
"dst : output array. It has the same number of cols and depth as the src, and the sum of rows of the src.",dst : 出力配列．src と同じ数の列と深さを持ち， src の行数の合計が出力されます．
"The function vertically concatenates two or more cv::Mat matrices (with the same number of cols).cv::Mat matArray[] = { cv::Mat(1, 4, CV_8UC1, cv::Scalar(1)),                       cv::Mat(1, 4, CV_8UC1, cv::Scalar(2)),                       cv::Mat(1, 4, CV_8UC1, cv::Scalar(3)),};cv::Mat out;cv::vconcat( matArray, 3, out );//out://[1,   1,   1,   1;// 2,   2,   2,   2;// 3,   3,   3,   3]fragmentSee alsocv::hconcat(const Mat*, size_t, OutputArray), ","この関数は，2つ以上の cv::Mat 行列（同じ数の cols を持つ）を垂直方向に連結します．cv::Mat matArray[] = { cv::Mat(1, 4, CV_8UC1, cv::Scalar(1)), cv::Mat(1, 4, CV_8UC1, cv::Scalar(2)), cv::Mat(1, 4, CV_8UC1, cv::Scalar(3)),};cv::Mat out;cv::vconcat( matArray, 3, out );//out://[1, 1, 1, 1;// 2, 2, 2;// 3, 3, 3]fragment また，ocv::hconcat(const Mat*, size_t, OutputArray) も参照してください．"
"cv::hconcat(InputArrayOfArrays, OutputArray) and ","cv::hconcat(InputArrayOfArrays, OutputArray), および"
"cv::hconcat(InputArray, InputArray, OutputArray)","cv::hconcat(InputArray, InputArray, OutputArray)"
computes bitwise conjunction of the two arrays (dst = src1 & src2) Calculates the per-element bit-wise conjunction of two arrays or an array and a scalar.,2 つの配列のビット単位の論理和を求めます （dst = src1 & src2） 2 つの配列，あるいは配列とスカラの要素毎のビット単位の論理和を求めます．
dst : output array that has the same size and type as the input arrays.,dst : 入力配列と同じサイズ，同じ型である出力配列．
"mask : optional operation mask, 8-bit single channel array, that specifies elements of the output array to be changed.",mask : 8ビットシングルチャンネル配列で，変更される出力配列の要素を指定するオプションの演算マスク．
"The function cv::bitwise_and calculates the per-element bit-wise logical conjunction for: Two arrays when src1 and src2 have the same size:\[\texttt{dst} (I) = \texttt{src1} (I) \wedge \texttt{src2} (I) \quad \texttt{if mask} (I) \ne0\]An array and a scalar when src2 is constructed from Scalar or has the same number of elements as src1.channels():\[\texttt{dst} (I) = \texttt{src1} (I) \wedge \texttt{src2} \quad \texttt{if mask} (I) \ne0\]A scalar and an array when src1 is constructed from Scalar or has the same number of elements as src2.channels():\[\texttt{dst} (I) = \texttt{src1} \wedge \texttt{src2} (I) \quad \texttt{if mask} (I) \ne0\]In case of floating-point arrays, their machine-specific bit representations (usually IEEE754-compliant) are used for the operation. In case of multi-channel arrays, each channel is processed independently. In the second and third cases above, the scalar is first converted to the array type.Examples: samples/cpp/create_mask.cpp.",関数 cv::bitwise_and は，各要素毎にビット単位の論理積を計算します．src1 と src2 が同じサイズの場合の 2 つの配列．(I) = ˶ˆ꒳ˆ˵ )(I） ˶ˆ꒳ˆ˵(I） ＃＃quad ＃＃if mask(I) \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\(I) = ˶ˆ꒳ˆ˵ )(I） ¶ wedge ¶ src2 ¶ wedge ¶ src2 ¶ wedge ¶ wedge\♪♪「if mask(I) \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\(I) = ″src1″ (I) = ″src1″ (I)\˶ˆ꒳ˆ˵ ˶ˆ꒳ˆ˵(I） ♯♯♯♯♯♯♯♯♯(I) ˶‾᷄ -̫ ‾᷅˵ 浮動小数点型配列の場合は，マシン固有のビット表現（通常，IEEE754準拠）を用いて演算します．また，マルチチャンネル配列の場合は，各チャンネルが独立して処理されます．例： samples/cpp/create_mask.cpp.
Calculates the per-element bit-wise disjunction of two arrays or an array and a scalar.,2 つの配列，あるいは配列とスカラの要素毎のビット単位の論理和を求めます．
"The function cv::bitwise_or calculates the per-element bit-wise logical disjunction for: Two arrays when src1 and src2 have the same size:\[\texttt{dst} (I) = \texttt{src1} (I) \vee \texttt{src2} (I) \quad \texttt{if mask} (I) \ne0\]An array and a scalar when src2 is constructed from Scalar or has the same number of elements as src1.channels():\[\texttt{dst} (I) = \texttt{src1} (I) \vee \texttt{src2} \quad \texttt{if mask} (I) \ne0\]A scalar and an array when src1 is constructed from Scalar or has the same number of elements as src2.channels():\[\texttt{dst} (I) = \texttt{src1} \vee \texttt{src2} (I) \quad \texttt{if mask} (I) \ne0\]In case of floating-point arrays, their machine-specific bit representations (usually IEEE754-compliant) are used for the operation. In case of multi-channel arrays, each channel is processed independently. In the second and third cases above, the scalar is first converted to the array type.","関数 cv::bitwise_or は，要素毎のビット単位の論理和を求めます．src1 と src2 が同じサイズの場合の 2 つの配列： ˶˙º̬˙˶(I) = ˶‾᷄ -̫ ‾᷅˵ ""src1(I）\\\\(I）\\\\(I) \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\(I) = ˶ˆ꒳ˆ˵ )(I) ˶‾᷄ -̫ ‾᷅˵ ˶‾᷅˵\♪♪「if mask(I) \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\(I) = ″src1″ (I) = ″src1″ (I)\src2} (I)(I） ¶quad ¶if mask(I) ˶‾᷄ -̫ ‾᷅˵ 浮動小数点型配列の場合は，マシン固有のビット表現（通常，IEEE754準拠）を用いて演算します．また，マルチチャンネル配列の場合は，各チャンネルが独立して処理されます．上記の2番目と3番目のケースでは，まずスカラが配列型に変換されます．"
"Calculates the per-element bit-wise ""exclusive or"" operation on two arrays or an array and a scalar.",2つの配列，あるいは配列とスカラの要素毎に，ビット単位の排他的論理和を計算します．
"The function cv::bitwise_xor calculates the per-element bit-wise logical ""exclusive-or"" operation for: Two arrays when src1 and src2 have the same size:\[\texttt{dst} (I) = \texttt{src1} (I) \oplus \texttt{src2} (I) \quad \texttt{if mask} (I) \ne0\]An array and a scalar when src2 is constructed from Scalar or has the same number of elements as src1.channels():\[\texttt{dst} (I) = \texttt{src1} (I) \oplus \texttt{src2} \quad \texttt{if mask} (I) \ne0\]A scalar and an array when src1 is constructed from Scalar or has the same number of elements as src2.channels():\[\texttt{dst} (I) = \texttt{src1} \oplus \texttt{src2} (I) \quad \texttt{if mask} (I) \ne0\]In case of floating-point arrays, their machine-specific bit representations (usually IEEE754-compliant) are used for the operation. In case of multi-channel arrays, each channel is processed independently. In the 2nd and 3rd cases above, the scalar is first converted to the array type.",関数 cv::bitwise_xor は，各要素に対するビット単位の論理的な「排他的論理和」演算を求めます．src1 と src2 が同じサイズの場合の 2 つの配列： ˶˙º̬˙˶(I) = ˶ˆ꒳ˆ˵ )(I) ¶oplus ¶src2}(I）\\\\(I) \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\(I) = ˶˙º̬˙˶(I) ¶oplus ¶src2}\♪♪～(I) \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\(I) = ″src1″ (I) = ″src1″ (I)\Plus(I） ˶‾᷄ -̫ ‾᷅˵(I) ˶‾᷄ -̫ ‾᷅˵ 浮動小数点型配列の場合は，マシン固有のビット表現（通常，IEEE754準拠）を用いて演算します．また，マルチチャンネル配列の場合は，各チャンネルが独立して処理されます．上記の2番目と3番目のケースでは，まずスカラが配列型に変換されます．
Inverts every bit of an array.,配列の各ビットを反転させます。
dst : output array that has the same size and type as the input array.,dst : 入力配列と同じサイズ，同じ型の出力配列．
"The function cv::bitwise_not calculates per-element bit-wise inversion of the input array:\[\texttt{dst} (I) = \neg \texttt{src} (I)\]In case of a floating-point input array, its machine-specific bit representation (usually IEEE754-compliant) is used for the operation. In case of multi-channel arrays, each channel is processed independently.Examples: samples/cpp/camshiftdemo.cpp.",関数 cv::bitwise_not は，入力配列の要素毎に，ビット単位の反転を計算します： ˶‾᷄ -̫ ‾᷅˵ 浮動小数点型の入力配列の場合は，マシン固有のビット表現（通常は IEEE754 準拠）が用いられます．例：samples/cpp/camshiftdemo.cpp.
Calculates the per-element absolute difference between two arrays or between an array and a scalar.,2 つの配列同士，あるいは配列とスカラの間の要素毎の絶対値の差を求めます．
dst : output array that has the same size and type as input arrays.,dst : 入力配列と同じサイズ，同じ型である出力配列．
"The function cv::absdiff calculates: Absolute difference between two arrays when they have the same size and type:\[\texttt{dst}(I) = \texttt{saturate} (| \texttt{src1}(I) - \texttt{src2}(I)|)\]Absolute difference between an array and a scalar when the second array is constructed from Scalar or has as many elements as the number of channels in src1:\[\texttt{dst}(I) = \texttt{saturate} (| \texttt{src1}(I) - \texttt{src2} |)\]Absolute difference between a scalar and an array when the first array is constructed from Scalar or has as many elements as the number of channels in src2:\[\texttt{dst}(I) = \texttt{saturate} (| \texttt{src1} - \texttt{src2}(I) |)\]where I is a multi-dimensional index of array elements. In case of multi-channel arrays, each channel is processed independently.NoteSaturation is not applied when the arrays have the depth CV_32S. You may even get a negative value in the case of overflow.See alsocv::abs(const Mat&)",関数 cv::absdiff は，次のように計算します．2つの配列が同じサイズ，同じ型である場合の，配列とスカラの絶対値の差：\texttt{dst}(I) = ˶˙º̬˙˶ (| ˶˙º̬˙˶) (| ˶˙º̬˙˶)\1番目の配列がスカラから構築されるか，または src2 のチャンネル数と同じ数の要素を持つ場合の，スカラと配列の絶対値の差を表します．\ここで，I は，配列要素の多次元インデックスです．マルチチャンネル配列の場合，各チャンネルは独立して処理されます． 注意点配列のビット深度が CV_32S の場合，彩度は適用されません．また，オーバーフロー時には負の値を得ることもあります．ocv::abs(const Mat&) も参照してください．
"This is an overloaded member function, provided for convenience (python) Copies the matrix to another one. When the operation mask is specified, if the Mat::create call shown above reallocates the matrix, the newly allocated matrix is initialized with all zeros before copying the data.",これは，利便性のために提供されるオーバーロードされたメンバ関数です（python） 行列を別の行列にコピーします．操作マスクが指定されている場合，上述の Mat::create 呼び出しが行列を再割り当てすると，データをコピーする前に，新しく割り当てられた行列がすべて 0 で初期化されます．
src : source matrix.,src : コピー元の行列．
"dst : Destination matrix. If it does not have a proper size or type before the operation, it is reallocated.",dst : コピー先の行列．演算前に適切なサイズや型を持っていない場合は，再割り当てされます．
mask : Operation mask of the same size as *this. Its non-zero elements indicate which matrix elements need to be copied. The mask has to be of type CV_8U and can have 1 or multiple channels.,mask : *this と同じサイズのオペレーションマスク．その非0の要素は，どの行列要素をコピーする必要があるかを示します．このマスクは CV_8U 型でなければならず，1 つまたは複数のチャンネルを持つことができます．
Checks if array elements lie between the elements of two other arrays.,配列の要素が，他の2つの配列の要素の間にあるかどうかをチェックします．
src : first input array.,src : 最初の入力配列．
lowerb : inclusive lower boundary array or a scalar.,lowerb : 下界を含む配列またはスカラ．
upperb : inclusive upper boundary array or a scalar.,upperb : 上界を含む配列，またはスカラ．
dst : output array of the same size as src and CV_8U type.,dst : src と同じサイズで，CV_8U 型の出力配列．
The function checks the range as follows:For every element of a single-channel input array: ,この関数は，シングルチャンネルの入力配列の各要素に対して，以下のように範囲をチェックします．
\[\texttt{dst} (I)= \texttt{lowerb} (I)_0 \leq \texttt{src} (I)_0 \leq \texttt{upperb} (I)_0\],\dst(I)= ୨୧-͈ᴗ-͈)(I)_0 ˶‾᷄д‾᷅˵ ˶‾᷅˵ ˶‾᷅˵ ˶‾᷄д‾᷅˵(I)_0 \\\\(I)_0\]
For two-channel arrays: ,2チャンネルアレイの場合
\[\texttt{dst} (I)= \texttt{lowerb} (I)_0 \leq \texttt{src} (I)_0 \leq \texttt{upperb} (I)_0 \land \texttt{lowerb} (I)_1 \leq \texttt{src} (I)_1 \leq \texttt{upperb} (I)_1\],\\\\(I)= \\\\(I)_0 ˶‾᷄ -̫ ‾᷅˵˵(I)_0 \\\\(I）_0 I'm _0 eldest person(I)_1 wrestler's younger brother(I)_1 ˶‾᷄ -̫ ‾᷅˵ ˶‾᷅˵(I)_1\]
"and so forth.That is, dst (I) is set to 255 (all 1 -bits) if src (I) is within the specified 1D, 2D, 3D, ... box and 0 otherwise.When the lower and/or upper boundary parameters are scalars, the indexes (I) at lowerb and upperb in the above formulas should be omitted.Examples: samples/cpp/camshiftdemo.cpp.",つまり，src（I）が指定された1D，2D，3D，...のボックス内にあれば，dst（I）は255（すべて1ビット）に，そうでなければ0に設定されます．下界や上界のパラメータがスカラーの場合は，上の式のlowerbやupperbのインデックス（I）を省略します．
Performs the per-element comparison of two arrays or an array and scalar value.,2つの配列，あるいは配列とスカラ値の要素毎の比較を行います．
"src1 : first input array or a scalar; when it is an array, it must have a single channel.",src1 : 1 番目の入力配列，またはスカラ値．これが配列の場合は，シングルチャンネルでなければいけません．
"src2 : second input array or a scalar; when it is an array, it must have a single channel.",src2 : 2 番目の入力配列，またはスカラ値，配列の場合はシングルチャンネルでなければいけません．
dst : output array of type ref CV_8U that has the same size and the same number of channels as the input arrays.,dst : 入力配列と同じサイズ，同じチャンネル数を持つ，ref CV_8U 型の出力配列．
"cmpop : a flag, that specifies correspondence between the arrays (cv::CmpTypes)",cmpop : 配列同士の対応関係を示すフラグ（ cv::CmpTypes ）．
"The function compares: Elements of two arrays when src1 and src2 have the same size:\[\texttt{dst} (I) = \texttt{src1} (I) \,\texttt{cmpop}\, \texttt{src2} (I)\]Elements of src1 with a scalar src2 when src2 is constructed from Scalar or has a single element:\[\texttt{dst} (I) = \texttt{src1}(I) \,\texttt{cmpop}\, \texttt{src2}\]src1 with elements of src2 when src1 is constructed from Scalar or has a single element:\[\texttt{dst} (I) = \texttt{src1} \,\texttt{cmpop}\, \texttt{src2} (I)\]When the comparison result is true, the corresponding element of output array is set to 255. The comparison operations can be replaced with the equivalent matrix expressions:Mat dst1 = src1 >= src2;Mat dst2 = src1 < 8;...fragmentSee alsocheckRange, min, max, threshold","この関数は，比較を行います．src1 と src2 が同じサイズの場合，2つの配列の要素を比較します．\src1がスカラから構成される場合や、単一の要素を持つ場合は、src2の要素を持つsrc1となります。\比較結果が True の場合、出力配列の対応する要素は 255 に設定されます。比較演算は，等価な行列表現に置き換えることができます： Mat dst1 = src1 >= src2;Mat dst2 = src1 < 8;...fragmentSee alsocheckRange, min, max, threshold"
Calculates per-element minimum of two arrays or an array and a scalar.,2つの配列，あるいは配列とスカラの要素毎の最小値を求めます．
"The function cv::min calculates the per-element minimum of two arrays:\[\texttt{dst} (I)= \min ( \texttt{src1} (I), \texttt{src2} (I))\]or array and a scalar:\[\texttt{dst} (I)= \min ( \texttt{src1} (I), \texttt{value} )\]See alsomax, compare, inRange, minMaxLoc","関数 cv::min は，2つの配列の要素毎の最小値を求めます： ˶ˆ꒳ˆ˵ ) または，配列とスカラの組み合わせ．\See alsomax, compare, inRange, minMaxLoc"
Calculates per-element maximum of two arrays or an array and a scalar.,2つの配列，あるいは，配列とスカラの 要素毎の最大値を求めます．
src2 : second input array of the same size and type as src1 .,src2 : src1 と同じサイズ，同じ型の 2 番目の入力配列．
"The function cv::max calculates the per-element maximum of two arrays:\[\texttt{dst} (I)= \max ( \texttt{src1} (I), \texttt{src2} (I))\]or array and a scalar:\[\texttt{dst} (I)= \max ( \texttt{src1} (I), \texttt{value} )\]See alsomin, compare, inRange, minMaxLoc, MatrixExpressions","関数 cv::max は，2つの配列の要素毎の最大値を求めます： ˶ˆ꒳ˆ˵ ) または，配列とスカラの組み合わせ．\See alsomin, compare, inRange, minMaxLoc, MatrixExpressions"
Calculates a square root of array elements.,配列の要素の平方根を計算します．
src : input floating-point array.,src : 入力される浮動小数点型配列．
"The function cv::sqrt calculates a square root of each input array element. In case of multi-channel arrays, each channel is processed independently. The accuracy is approximately the same as of the built-in std::sqrt .",関数 cv::sqrt は，入力配列の各要素の平方根を求めます．マルチチャンネル配列の場合，各チャンネルは独立して処理されます．その精度は，組み込みの std::sqrt とほぼ同じです．
Raises every array element to a power.,各配列要素をべき乗します．
power : exponent of power.,power : べき乗の指数．
"The function cv::pow raises every element of the input array to power :\[\texttt{dst} (I) = \fork{\texttt{src}(I)^{power}}{if \(\texttt{power}\) is integer}{|\texttt{src}(I)|^{power}}{otherwise}\]So, for a non-integer power exponent, the absolute values of input array elements are used. However, it is possible to get true values for negative values using some extra operations. In the example below, computing the 5th root of array src shows:Mat mask = src < 0;pow(src, 1./5, dst);subtract(Scalar::all(0), dst, dst, mask);fragmentFor some values of power, such as integer values, 0.5 and -0.5, specialized faster algorithms are used.Special values (NaN, Inf) are not handled.See alsosqrt, exp, log, cartToPolar, polarToCart","関数 cv::pow は，入力配列の各要素をべき乗にします．(I) = \\\\\\\\\\\\\\\しかし，いくつかの追加演算を行うことで，負の値に対する真の値を得ることができます．以下の例では，配列 src の 5 次根を計算すると，次のようになります： Mat mask = src < 0;pow(src, 1./5, dst);subtract(Scalar::all(0), dst, dst, mask);fragment整数値である 0.5 や -0.5 などの一部の累乗値に対しては，特殊な高速アルゴリズムが用いられます．特殊な値（NaN, Inf）は扱われません．"
Calculates the exponent of every array element.,各配列要素の指数を求めます．
"The function cv::exp calculates the exponent of every element of the input array:\[\texttt{dst} [I] = e^{ src(I) }\]The maximum relative error is about 7e-6 for single-precision input and less than 1e-10 for double-precision input. Currently, the function converts denormalized values to zeros on output. Special values (NaN, Inf) are not handled.See alsolog , cartToPolar , polarToCart , phase , pow , sqrt , magnitude","関数 cv::exp は，入力配列の各要素の指数を求めます．[I] = e^{ src(I) }\]最大の相対誤差は，単精度入力では約 7e-6，倍精度入力では 1e-10 未満です．現在，この関数は，非正規化された値をゼロに変換して出力します．関連項目：olog , cartToPolar , polarToCart , phase , pow , sqrt , magnitude"
For example,例
q : a quaternion.,q : クォータニオン（四元演算）。
"assumeUnit : if QUAT_ASSUME_UNIT, q assume to be a unit quaternion and this function will save some computations.",assumeUnit : QUAT_ASSUME_UNITの場合、qはユニットクォータニオンであると仮定し、この関数はいくつかの計算を節約します。
"Quatd q1{1,2,3,4};cout << log(q1) << endl;fragmentExamples: samples/cpp/polar_transforms.cpp, and samples/cpp/stitching_detailed.cpp.","quatd q1{1,2,3,4};cout << log(q1) << endl;fragmentExamples: samples/cpp/polar_transforms.cpp, and samples/cpp/stitching_detailed.cpp."
Calculates x and y coordinates of 2D vectors from their magnitude and angle.,"2次元ベクトルの大きさと角度からx,y座標を計算します。"
"magnitude : input floating-point array of magnitudes of 2D vectors; it can be an empty matrix (=Mat()), in this case, the function assumes that all the magnitudes are =1; if it is not empty, it must have the same size and type as angle.",magnitude : 2次元ベクトルの大きさを表す，浮動小数点型の入力配列．これは，空の行列（=Mat()）でもよく，その場合，この関数はすべての大きさが =1 であると仮定します．空でない場合は， angle と同じサイズ，同じ型でなければいけません．
angle : input floating-point array of angles of 2D vectors.,angle : 2次元ベクトルの角度を表す，浮動小数点型の入力配列．
x : output array of x-coordinates of 2D vectors; it has the same size and type as angle.,x : 2次元ベクトルのx座標の出力配列；angleと同じサイズと型を持ちます．
y : output array of y-coordinates of 2D vectors; it has the same size and type as angle.,y : 2次元ベクトルのy座標の出力配列；angleと同じサイズと型です．
"angleInDegrees : when true, the input angles are measured in degrees, otherwise, they are measured in radians.",angleInDegrees : これが真の場合，入力された角度は度単位で表され，そうでない場合はラジアン単位で表されます．
"The function cv::polarToCart calculates the Cartesian coordinates of each 2D vector represented by the corresponding elements of magnitude and angle:\[\begin{array}{l} \texttt{x} (I) = \texttt{magnitude} (I) \cos ( \texttt{angle} (I)) \\ \texttt{y} (I) = \texttt{magnitude} (I) \sin ( \texttt{angle} (I)) \\ \end{array}\]The relative accuracy of the estimated coordinates is about 1e-6.See alsocartToPolar, magnitude, phase, exp, log, pow, sqrt","関数 cv::polarToCart は，magnitude と angle の対応する要素で表される各 2 次元ベクトルのデカルト座標を求めます．\x(I) = ୨୧-͈ᴗ-͈ˋ(I） cos ( ˶ˆ꒳ˆ˵ )(I))\\ ♪♪♪♪♪♪♪～(I) = ˶ˆ꒳ˆ˵ )(sin ( ˶ˆ꒳ˆ˵ )(I))\\ 参照：socartToPolar, magnitude, phase, exp, log, pow, sqrt"
Calculates the magnitude and angle of 2D vectors.,2D ベクトルの大きさと角度を計算します。
x : array of x-coordinates; this must be a single-precision or double-precision floating-point array.,x : x座標の配列，単精度または倍精度の浮動小数点型配列でなければいけません．
"y : array of y-coordinates, that must have the same size and same type as x.",y : y座標の配列，xと同じサイズ，同じ型である必要があります．
magnitude : output array of magnitudes of the same size and type as x.,magnitude : x と同じサイズ，同じ型の大きさの出力配列．
angle : output array of angles that has the same size and type as x; the angles are measured in radians (from 0 to 2*Pi) or in degrees (0 to 360 degrees).,angle : x と同じサイズ，同じ型の角度の出力配列．角度はラジアン（0から2*Piまで）または度（0から360度）で表されます．
"angleInDegrees : a flag, indicating whether the angles are measured in radians (which is by default), or in degrees.",angleInDegrees : フラグで，角度をラジアン（デフォルト）で測るか，あるいは度数で測るかを指定します．
"The function cv::cartToPolar calculates either the magnitude, angle, or both for every 2D vector (x(I),y(I)):\[\begin{array}{l} \texttt{magnitude} (I)= \sqrt{\texttt{x}(I)^2+\texttt{y}(I)^2} , \\ \texttt{angle} (I)= \texttt{atan2} ( \texttt{y} (I), \texttt{x} (I))[ \cdot180 / \pi ] \end{array}\]The angles are calculated with accuracy about 0.3 degrees. For the point (0,0), the angle is set to 0.See alsoSobel, Scharr","関数 cv::cartToPolar は，すべての 2 次元ベクトル (x(I),y(I)) に対して，大きさ，角度，またはその両方を計算します．|magnitude}。(I)= \sqrt{\texttt{x}(I)^2+\texttt{y}(I)^2}♪♪♪♪～(I）＝ ˶‾᷄ -̫ ‾᷅˵(I), ˶ˆ꒳ˆ˵ )(I))[ ˶‾᷄ -̫ ‾᷅˵ ]角度は約0.3度の精度で計算されています。点(0,0)の場合、角度は0になります。"
Calculates the rotation angle of 2D vectors.,2次元ベクトルの回転角度を計算します。
x : input floating-point array of x-coordinates of 2D vectors.,x : 2次元ベクトルのx座標を表す，浮動小数点型の入力配列．
y : input array of y-coordinates of 2D vectors; it must have the same size and the same type as x.,y : 2次元ベクトルのy座標の入力配列；xと同じサイズ，同じ型でなければいけません．
angle : output array of vector angles; it has the same size and same type as x .,angle : ベクトルの角度を表す出力配列；x と同じサイズ，同じ型です．
"angleInDegrees : when true, the function calculates the angle in degrees, otherwise, they are measured in radians.",angleInDegrees : これが真の場合，この関数は角度を度単位で計算し，そうでない場合はラジアン単位で計算します．
"The function cv::phase calculates the rotation angle of each 2D vector that is formed from the corresponding elements of x and y :\[\texttt{angle} (I) = \texttt{atan2} ( \texttt{y} (I), \texttt{x} (I))\]The angle estimation accuracy is about 0.3 degrees. When x(I)=y(I)=0 , the corresponding angle(I) is set to 0.",関数 cv::phase は，x と y の対応する要素で構成される各 2 次元ベクトルの回転角を求めます．x(I)=y(I)=0のとき，対応する角度(I)は0になります。
Calculates the magnitude of 2D vectors.,2次元ベクトルの大きさを計算します。
x : floating-point array of x-coordinates of the vectors.,x : ベクトルのx座標の浮動小数点配列．
y : floating-point array of y-coordinates of the vectors; it must have the same size as x.,y : ベクトルの y 座標を表す浮動小数点型配列，x と同じサイズでなければいけません．
magnitude : output array of the same size and type as x.,magnitude : x と同じサイズ，同じ型の出力配列．
"The function cv::magnitude calculates the magnitude of 2D vectors formed from the corresponding elements of x and y arrays:\[\texttt{dst} (I) = \sqrt{\texttt{x}(I)^2 + \texttt{y}(I)^2}\]See alsocartToPolar, polarToCart, phase, sqrtExamples: samples/cpp/polar_transforms.cpp.",関数 cv::magnitude は，x と y の配列の対応する要素から生成される 2 次元ベクトルの大きさを求めます：\\\ (I) = sqrt{\\ (I)^2 + ୨୧┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈
Checks every element of an input array for invalid values.,入力配列の各要素に無効な値がないかチェックします。
a : input array.,a : 入力配列．
"quiet : a flag, indicating whether the functions quietly return false when the array elements are out of range or they throw an exception.",quiet : フラグ．配列の要素が範囲外の場合に，関数が静かに false を返すか，あるいは例外を発生させるかを示します．
"pos : optional output parameter, when not NULL, must be a pointer to array of src.dims elements.",pos : オプションの出力パラメータ．NULLでない場合は， src.dims 要素の配列へのポインタでなければいけません．
minVal : inclusive lower boundary of valid values range.,minVal : 有効な値の範囲の下界を含みます。
maxVal : exclusive upper boundary of valid values range.,maxVal : 有効な値の範囲の上側の境界を含みます．
"The function cv::checkRange checks that every array element is neither NaN nor infinite. When minVal > -DBL_MAX and maxVal < DBL_MAX, the function also checks that each value is between minVal and maxVal. In case of multi-channel arrays, each channel is processed independently. If some values are out of range, position of the first outlier is stored in pos (when pos != NULL). Then, the function either returns false (when quiet=true) or throws an exception.",関数 cv::checkRange は，各配列要素が NaN でも無限でもないことをチェックします．また，minVal > -DBL_MAX かつ maxVal < DBL_MAX の場合は，各値が minVal と maxVal の間にあるかどうかをチェックします．マルチチャンネル配列の場合，各チャンネルは独立して処理されます．いくつかの値が範囲外である場合，最初に外れた値の位置が pos に格納されます（ pos != NULL の場合）．そして，この関数は，（ quiet=true の場合）偽を返すか，例外を発生させます．
converts NaNs to the given number,NaNを指定された数値に変換します．
a : input/output matrix (CV_32F type).,a : 入出力行列（CV_32F型）．
val : value to convert the NaNs,val : NaNsを変換する値．
Performs generalized matrix multiplication.,一般化された行列の乗算を行います．
"src1 : first multiplied input matrix that could be real(CV_32FC1, CV_64FC1) or complex(CV_32FC2, CV_64FC2).","src1 : 1 番目に乗算される入力行列．実数（CV_32FC1, CV_64FC1）または複素数（CV_32FC2, CV_64FC2）の可能性があります．"
src2 : second multiplied input matrix of the same type as src1.,src2 : src1 と同じ型の，2 番目に乗算される入力行列．
alpha : weight of the matrix product.,alpha : 行列の積の重み．
src3 : third optional delta matrix added to the matrix product; it should have the same type as src1 and src2.,src3 : オプションで，行列の積に追加される3番目のデルタ行列．
beta : weight of src3.,beta : src3 の重み．
dst : output matrix; it has the proper size and the same type as input matrices.,dst : 出力行列．入力行列と同じ型で，適切なサイズの行列です．
flags : operation flags (cv::GemmFlags),flags : 操作フラグ（cv::GemmFlags）．
"The function cv::gemm performs generalized matrix multiplication similar to the gemm functions in BLAS level 3. For example, gemm(src1, src2, alpha, src3, beta, dst, GEMM_1_T + GEMM_3_T) corresponds to\[\texttt{dst} = \texttt{alpha} \cdot \texttt{src1} ^T \cdot \texttt{src2} + \texttt{beta} \cdot \texttt{src3} ^T\]In case of complex (two-channel) data, performed a complex matrix multiplication.The function can be replaced with a matrix expression. For example, the above call can be replaced with:dst = alpha*src1.t()*src2 + beta*src3.t();fragmentSee alsomulTransposed , transformExamples: samples/cpp/image_alignment.cpp.","関数 cv::gemm は，BLAS レベル 3 の gemm 関数と同様に，一般化された行列の乗算を行います．例えば， gemm(src1, src2, alpha, src3, beta, dst, GEMM_1_T + GEMM_3_T) は，次のように対応します：[˶ˆ꒳ˆ˵] = ˶ˆ꒳ˆ˵^T ˶ˆ꒳ˆ˵ )+ ˶ˆ꒳ˆ˵ )^T**]複素数（2チャンネル）のデータの場合は、複素数行列の乗算を行います。この関数は、行列式に置き換えることができます。例えば，上記の呼び出しは次のように置き換えることができます：dst = alpha*src1.t()*src2 + beta*src3.t();fragmentSee alsomulTransposed , transformExamples: samples/cpp/image_alignment.cpp."
Calculates the product of a matrix and its transposition.,行列とその転置行列の積を求めます．
"src : input single-channel matrix. Note that unlike gemm, the function can multiply not only floating-point matrices.",src : シングルチャンネルの入力行列．gemm とは異なり，この関数は浮動小数点型の行列だけでなく，積むこともできることに注意してください．
dst : output square matrix.,dst : 出力される正方行列．
aTa : Flag specifying the multiplication ordering. See the description below.,aTa : 乗算の順序を指定するフラグ．以下の説明を参照してください．
"delta : Optional delta matrix subtracted from src before the multiplication. When the matrix is empty ( delta=noArray() ), it is assumed to be zero, that is, nothing is subtracted. If it has the same size as src , it is simply subtracted. Otherwise, it is ""repeated"" (see repeat ) to cover the full src and then subtracted. Type of the delta matrix, when it is not empty, must be the same as the type of created output matrix. See the dtype parameter description below.",delta :オプションで，乗算の前に src から減算されるデルタ行列．この行列が空の場合（ delta=noArray() ），ゼロであると見なされ，何も引かれません．また， src と同じサイズの場合は，単に減算されます．そうでない場合は，src 全体を覆うように「繰り返され」（ repeat を参照してください），その後に減算されます．デルタ行列が空ではない場合，そのタイプは，作成される出力行列のタイプと同じでなければいけません．以下の dtype パラメータの説明を参照してください．
scale : Optional scale factor for the matrix product.,scale : オプションである，行列の積のスケールファクタ．
"dtype : Optional type of the output matrix. When it is negative, the output matrix will have the same type as src . Otherwise, it will be type=CV_MAT_DEPTH(dtype) that should be either CV_32F or CV_64F .",dtype :オプションである，出力行列の型．これが負の値の場合，出力行列は src と同じ型になります．そうでない場合は， type=CV_MAT_DEPTH(dtype) となり，CV_32F または CV_64F のどちらかになります．
"The function cv::mulTransposed calculates the product of src and its transposition:\[\texttt{dst} = \texttt{scale} ( \texttt{src} - \texttt{delta} )^T ( \texttt{src} - \texttt{delta} )\]if aTa=true , and\[\texttt{dst} = \texttt{scale} ( \texttt{src} - \texttt{delta} ) ( \texttt{src} - \texttt{delta} )^T\]otherwise. The function is used to calculate the covariance matrix. With zero delta, it can be used as a faster substitute for general matrix product A*B when B=A'See alsocalcCovarMatrix, gemm, repeat, reduce","関数 cv::mulTransposed は， src とその転置の積を計算します： ˶‾᷄ -̫ ‾᷅˵ ( ˶‾᷅˵ )( ˶ˆ꒳ˆ˵ ) ^T ( ˶ˆ꒳ˆ˵ ) ATa=true , and\[˶ˆ꒳ˆ˵ )( ˶‾᷄ -̫ ‾᷅˵ ) ( ˶‾᷅⚰︎˵ )この関数は，共分散行列を計算するために使われます。デルタが 0 の場合は，一般的な行列積 A*B when B=A の高速な代替手段として利用できます．"
Transposes a matrix.,行列を転置します．
"The function cv::transpose transposes the matrix src :\[\texttt{dst} (i,j) = \texttt{src} (j,i)\]NoteNo complex conjugation is done in case of a complex matrix. It should be done separately if needed.Examples: samples/cpp/train_HOG.cpp.",関数 cv::transpose は，行列 src を転置します．例：samples/cpp/train_HOG.cpp.
Performs the matrix transformation of every array element.,配列の各要素に対して，行列変換を行います．
src : input array that must have as many channels (1 to 4) as m.cols or m.cols-1.,src : m.cols または m.cols-1 と同数のチャンネル（1 から 4）を持つ入力配列．
dst : output array of the same size and depth as src; it has as many channels as m.rows.,dst : src と同じサイズ・深さの出力配列．m.rows と同数のチャンネルを持ちます．
m : transformation 2x2 or 2x3 floating-point matrix.,m : 2x2 または 2x3 の浮動小数点型変換行列．
"The function cv::transform performs the matrix transformation of every element of the array src and stores the results in dst :\[\texttt{dst} (I) = \texttt{m} \cdot \texttt{src} (I)\](when m.cols=src.channels() ), or\[\texttt{dst} (I) = \texttt{m} \cdot [ \texttt{src} (I); 1]\](when m.cols=src.channels()+1 )Every element of the N -channel array src is interpreted as N -element vector that is transformed using the M x N or M x (N+1) matrix m to M-element vector - the corresponding element of the output array dst .The function may be used for geometrical transformation of N -dimensional points, arbitrary linear color space transformation (such as various kinds of RGB to YUV transforms), shuffling the image channels, and so forth.See alsoperspectiveTransform, getAffineTransform, estimateAffine2D, warpAffine, warpPerspective","関数 cv::transform は，配列 src の各要素に対して行列変換を行い，その結果を dst に格納します :˶‾᷄ -̫ ‾᷅˵ ˶‾᷅˵ (I) = ˶‾᷄ -̫ ‾᷅˵ (I) ˶‾᷅˵ (m.cols=src.channels()), or\[˶‾᷅˵ (I) = ˶‾᷄ -̫ ‾᷄˵ (I) = ˶‾᷄ -̫ ‾᷄˵ (I)(I) = \\\\\Nチャンネル配列 src の各要素は， M x N または M x (N+1) の行列 m を用いて変換された N 個の要素のベクトルとして解釈され，出力配列 dst の対応する要素となります．この関数は，N - 次元の点の幾何学的変換，任意の線形色空間変換（様々な RGB から YUV への変換など），画像チャンネルの入れ替えなどに利用できます． 関連項目：operspectiveTransform, getAffineTransform, estimateAffine2D, warpAffine, warpPerspective"
Performs the perspective matrix transformation of vectors.,ベクトルの透視行列変換を行います．
src : input two-channel or three-channel floating-point array; each element is a 2D/3D vector to be transformed.,src : 2チャンネルまたは3チャンネルの浮動小数点型の入力配列，各要素は変換される2次元/3次元ベクトル．
m : 3x3 or 4x4 floating-point transformation matrix.,m : 3x3 または 4x4 の浮動小数点型変換行列．
"The function cv::perspectiveTransform transforms every element of src by treating it as a 2D or 3D vector, in the following way:\[(x, y, z) \rightarrow (x'/w, y'/w, z'/w)\]where\[(x', y', z', w') = \texttt{mat} \cdot \begin{bmatrix} x & y & z & 1 \end{bmatrix}\]and\[w = \fork{w'}{if \(w' \ne 0\)}{\infty}{otherwise}\]Here a 3D vector transformation is shown. In case of a 2D vector transformation, the z component is omitted.NoteThe function transforms a sparse set of 2D or 3D vectors. If you want to transform an image using perspective transformation, use warpPerspective . If you have an inverse problem, that is, you want to compute the most probable perspective transformation out of several pairs of corresponding points, you can use getPerspectiveTransform or findHomography .See alsotransform, warpPerspective, getPerspectiveTransform, findHomography","関数 cv::parkentTransform は， src の各要素を 2 次元または 3 次元のベクトルとして扱い，次のように変換します：˶[(x, y, z)] ˶[Ictions], [Ictions], [Ictions], [Ictions], [Ictions], [Ictions], [Ictions], [Ictions], [Ictions], [Ictions], [Ictions], [Ictions].\ここでは、3次元ベクトル変換を示しています。2次元ベクトル変換の場合は，z成分が省略されています．注：この関数は，2次元または3次元ベクトルの疎な集合を変換します．透視変換を用いて画像を変換したい場合は， warpPerspective を利用してください．逆問題がある場合，つまり，対応する複数の点の組から最も確率の高い透視変換を計算したい場合は， getPerspectiveTransform や findHomography を利用できます．"
Copies the lower or the upper half of a square matrix to its another half.,正方行列の下半分または上半分を，もう一方の半分にコピーします．
m : input-output floating-point square matrix.,m : 入出力可能な浮動小数点型正方行列．
"lowerToUpper : operation flag; if true, the lower half is copied to the upper half. Otherwise, the upper half is copied to the lower half.",trueの場合，下半分が上半分にコピーされます．そうでなければ，上半分が下半分にコピーされます．
The function cv::completeSymm copies the lower or the upper half of a square matrix to its another half. The matrix diagonal remains unchanged:\(\texttt{m}_{ij}=\texttt{m}_{ji}\) for \(i > j\) if lowerToUpper=false,関数 cv::completeSymm は，正方行列の下半分または上半分を，もう片方の半分にコピーします．行列の対角線は変更されません： ˶(˶‾᷄ -̫ ‾᷅˵) for ˶(˶‾᷄ -̫ ‾᷄˵) if lowerToUpper=false
"\(\texttt{m}_{ij}=\texttt{m}_{ji}\) for \(i < j\) if lowerToUpper=trueSee alsoflip, transpose","\for elderToUpper=trueSee alsoflip, transpose"
Initializes a scaled identity matrix.,スケーリングされた単位行列を初期化します．
mtx : matrix to initialize (not necessarily square).,mtx : 初期化される行列（正方でなくても構いません）．
s : value to assign to diagonal elements.,s : 対角線上の要素に割り当てられる値．
"The function cv::setIdentity initializes a scaled identity matrix:\[\texttt{mtx} (i,j)= \fork{\texttt{value}}{ if \(i=j\)}{0}{otherwise}\]The function can also be emulated using the matrix initializers and the matrix expressions:Mat A = Mat::eye(4, 3, CV_32F)*5;// A will be set to [[5, 0, 0], [0, 5, 0], [0, 0, 5], [0, 0, 0]]fragmentSee alsoMat::zeros, Mat::ones, Mat::setTo, Mat::operator=Examples: samples/cpp/kalman.cpp.","関数 cv::setIdentity は，拡大された単位行列を初期化します．この関数は，行列の初期化子と行列表現を用いて，模倣することもできます．Mat A = Mat::eye(4, 3, CV_32F)*5;// A は，[[5, 0, 0], [0, 5, 0], [0, 0, 5], [0, 0, 0]]fragmentSee alsoMat::zeros, Mat::ones, Mat::setTo, Mat::operator=Examples: samples/cpp/kalman.cpp."
Returns the determinant of a square floating-point matrix.,浮動小数点型正方行列の行列式を返します．
mtx : input matrix that must have CV_32FC1 or CV_64FC1 type and square size.,mtx : CV_32FC1 または CV_64FC1 型で，サイズが正方である入力行列．
"The function cv::determinant calculates and returns the determinant of the specified matrix. For small matrices ( mtx.cols=mtx.rows<=3 ), the direct method is used. For larger matrices, the function uses LU factorization with partial pivoting.For symmetric positively-determined matrices, it is also possible to use eigen decomposition to calculate the determinant.See alsotrace, invert, solve, eigen, MatrixExpressions",関数 cv::determinant は，指定された行列の行列式を計算し，それを返します．小さな行列 ( mtx.cols=mtx.rows<=3 ) に対しては，直接法が用いられます．対称的な正定値行列に対しては，行列式を計算するために固有値分解を利用することも可能です． 関連項目：トレース，反転，解，固有値，MatrixExpressions
Returns the trace of a matrix.,行列のトレースを返します。
mtx : input matrix.,mtx : 入力行列．
"The function cv::trace returns the sum of the diagonal elements of the matrix mtx .\[\mathrm{tr} ( \texttt{mtx} ) = \sum _i \texttt{mtx} (i,i)\]",関数 cv::trace は，行列 mtx の対角要素の和を返します．
Finds the inverse or pseudo-inverse of a matrix.,逆行列（擬似逆行列）を求めます。
src : input floating-point M x N matrix.,src : 入力となる浮動小数点型の M x N の行列．
dst : output matrix of N x M size and the same type as src.,dst : src と同じ型の，N x M サイズの出力行列．
flags : inversion method (cv::DecompTypes),flags : 反転法（cv::DecompTypes）．
"The function cv::invert inverts the matrix src and stores the result in dst . When the matrix src is singular or non-square, the function calculates the pseudo-inverse matrix (the dst matrix) so that norm(src*dst - I) is minimal, where I is an identity matrix.In case of the DECOMP_LU method, the function returns non-zero value if the inverse has been successfully calculated and 0 if src is singular.In case of the DECOMP_SVD method, the function returns the inverse condition number of src (the ratio of the smallest singular value to the largest singular value) and 0 if src is singular. The SVD method calculates a pseudo-inverse matrix if src is singular.Similarly to DECOMP_LU, the method DECOMP_CHOLESKY works only with non-singular square matrices that should also be symmetrical and positively defined. In this case, the function stores the inverted matrix in dst and returns non-zero. Otherwise, it returns 0.See alsosolve, SVD","関数 cv::invert は，行列 src を反転し，その結果を dst に格納します．行列 src が特異または非正方である場合，この関数は，ノルム(src*dst - I) が最小となるような擬似逆行列（dst 行列）を求めます（ここで I は単位行列）．DECOMP_SVD メソッドの場合，この関数は src の逆数（最小の特異値と最大の特異値の比）を返し， src が特異な場合は 0 を返します．DECOMP_LU と同様に，DECOMP_CHOLESKY メソッドも，非特異な正方行列（対称で正定値である必要があります）に対してのみ動作します．この場合，この関数は，反転した行列を dst に格納し，0 以外の値を返します。他にも，solve, SVD を参照してください。"
Solves one or more linear systems or least-squares problems.,1 つまたは複数の連立方程式や最小二乗問題を解きます．
src1 : input matrix on the left-hand side of the system.,src1 : 連立方程式の左辺の入力行列．
src2 : input matrix on the right-hand side of the system.,src2 : 連立方程式の右辺にある入力行列．
dst : output solution.,dst : 出力される解．
flags : solution (matrix inversion) method (DecompTypes),flags : 解法（行列反転）メソッド（DecompTypes）．
"The function cv::solve solves a linear system or least-squares problem (the latter is possible with SVD or QR methods, or by specifying the flag DECOMP_NORMAL ):\[\texttt{dst} = \arg \min _X \| \texttt{src1} \cdot \texttt{X} - \texttt{src2} \|\]If DECOMP_LU or DECOMP_CHOLESKY method is used, the function returns 1 if src1 (or \(\texttt{src1}^T\texttt{src1}\) ) is non-singular. Otherwise, it returns 0. In the latter case, dst is not valid. Other methods find a pseudo-solution in case of a singular left-hand side part.NoteIf you want to find a unity-norm solution of an under-defined singular system \(\texttt{src1}\cdot\texttt{dst}=0\) , the function solve will not do the work. Use SVD::solveZ instead.See alsoinvert, SVD, eigen","関数 cv::solve は，連立方程式や最小二乗問題を解きます（後者は，SVD や QR などの手法を用いるか，DECOMP_NORMAL フラグを指定することで可能になります）： ˶‾᷄ -̫ ‾᷅˵˵ ˶‾᷅˵ ˶‾᷅˵ ˶‾᷅˵ ˶‾᷅˵ ˶‾᷅˵ ˶‾᷅˵ ˶‾᷅˵ ˶‾᷅˵ ˶‾᷅˵ ˶‾᷅˵ ˶‾᷄ -̫ ‾᷅˵\\\\\\\\\\\\\\\\\\\\\\\\\\\|\DECOMP_LU または DECOMP_CHOLESKY メソッドが使われた場合， src1 (または ˶‾᷄ -̫ ‾᷅˵) が非特異点であれば，1を返します．また，後者の場合，dst は無効です．注意定義されていない特異系\(˶ˆ꒳ˆ˵ ) のユニティノルム解を求めたい場合，関数solveでは解が得られません．SVD::solveZを使ってください．参照：invert, SVD, eigen"
Solve given (non-integer) linear programming problem using the Simplex Algorithm (Simplex Method).,与えられた（非整数）線形計画問題を Simplex Algorithm (Simplex Method) を用いて解きます。
"Func : This row-vector corresponds to \(c\) in the LP problem formulation (see above). It should contain 32- or 64-bit floating point numbers. As a convenience, column-vector may be also submitted, in the latter case it is understood to correspond to \(c^T\).",func :この行ベクトルは，LP問題の定式化における ˶(c˶)に相当します（上述）．32ビットまたは64ビットの浮動小数点数を含むべきである．便宜上，列ベクトルも提出することができるが，その場合は， \(c^T\)に対応するものとする．
"Constr : m-by-n+1 matrix, whose rightmost column corresponds to \(b\) in formulation above and the remaining to \(A\). It should contain 32- or 64-bit floating point numbers.",Constr : m×n+1 の行列で、右端の列は \(b\)、残りの列は \(A\)に対応する。32ビットまたは64ビットの浮動小数点数である．
z : The solution will be returned here as a column-vector - it corresponds to \(c\) in the formulation above. It will contain 64-bit floating point numbers.,z : 解答は，列ベクトルとして返され，上の式の ˶(c˶)に相当する．これは，64ビットの浮動小数点数を含みます．
"What we mean here by ""linear programming problem"" (or LP problem, for short) can be formulated as:\[\mbox{Maximize } c\cdot x\\ \mbox{Subject to:}\\ Ax\leq b\\ x\geq 0\]Where \(c\) is fixed 1-by-n row-vector, \(A\) is fixed m-by-n matrix, \(b\) is fixed m-by-1 column vector and \(x\) is an arbitrary n-by-1 column vector, which satisfies the constraints.Simplex algorithm is one of many algorithms that are designed to handle this sort of problems efficiently. Although it is not optimal in theoretical sense (there exist algorithms that can solve any problem written as above in polynomial time, while simplex method degenerates to exponential time for some special cases), it is well-studied, easy to implement and is shown to work well for real-life purposes.The particular implementation is taken almost verbatim from Introduction to Algorithms, third edition by T. H. Cormen, C. E. Leiserson, R. L. Rivest and Clifford Stein. In particular, the Bland's rule http://en.wikipedia.org/wiki/Bland%27s_rule is used to prevent cycling.","ここでいう「線形計画問題」（略してLP問題）は、次のように定式化できます。ここで、\\は1×nの行ベクトル、A\はm×nの行列、b☞はm×1の列ベクトル、x☞はn×1の任意の列ベクトルであり、制約条件を満たしている。Simplexアルゴリズムは、この種の問題を効率的に処理するために設計された数多くのアルゴリズムの1つです。理論的には最適ではありませんが（上のように書かれた問題を多項式時間で解くことができるアルゴリズムは存在しますが、シンプレックス法はいくつかの特別なケースでは指数時間に縮退します）、よく研究されており、実装も簡単で、実際の目的にもよく合うことが示されています。特定の実装は、T. H. Cormen, C. E. Leiserson, R. L. Rivest and Clifford Stein著のIntroduction to Algorithms, third editionからほぼそのまま引用しています。特に、循環を防ぐために、Blandのルールhttp://en.wikipedia.org/wiki/Bland%27s_rule を使用しています。"
Sorts each row or each column of a matrix.,行列の各行または各列をソートします。
"flags : operation flags, a combination of SortFlags",flags : 操作フラグ，SortFlags の組み合わせ．
"The function cv::sort sorts each matrix row or each matrix column in ascending or descending order. So you should pass two operation flags to get desired behaviour. If you want to sort matrix rows or columns lexicographically, you can use STL std::sort generic function with the proper comparison predicate.See alsosortIdx, randShuffleExamples: samples/cpp/stitching_detailed.cpp.","関数 cv::sort は，行列の各行または各列を，昇順または降順にソートします．したがって，目的の動作を得るためには，2つの操作フラグを渡す必要があります．行列の行や列を辞書的にソートしたい場合は，STL の汎用関数 std::sort と適切な比較述語を利用することができます．他にも，sortIdx, randShuffleExamples: samples/cpp/stitching_detailed.cpp を参照してください．"
dst : output integer array of the same size as src.,dst : src と同じサイズの，出力される整数配列．
flags : operation flags that could be a combination of cv::SortFlags,flags : cv::SortFlags を組み合わせた操作フラグ．
"The function cv::sortIdx sorts each matrix row or each matrix column in the ascending or descending order. So you should pass two operation flags to get desired behaviour. Instead of reordering the elements themselves, it stores the indices of sorted elements in the output array. For example:Mat A = Mat::eye(3,3,CV_32F), B;sortIdx(A, B, SORT_EVERY_ROW + SORT_ASCENDING);// B will probably contain// (because of equal elements in A some permutations are possible):// [[1, 2, 0], [0, 2, 1], [0, 1, 2]]fragmentSee alsosort, randShuffle","関数 cv::sortIdx は，行列の各行または各列を，昇順または降順にソートします．したがって，望ましい動作を得るためには，2つの操作フラグを渡す必要があります．この関数は，要素自体を並べ替えるのではなく，ソートされた要素のインデックスを出力配列に格納します．例： Mat A = Mat::eye(3,3,CV_32F), B;sortIdx(A, B, SORT_EVERY_ROW + SORT_ASCENDING);// B はおそらく，// （A の要素が等しいので，いくつかの順列が可能です）：// [[1, 2, 0], [0, 2, 1], [0, 1, 2]]fragment 他にも，sort, randShuffle を参照してください．"
Finds the real roots of a cubic equation.,三次方程式の実根を求めます。
"coeffs : equation coefficients, an array of 3 or 4 elements.",coeffs : 方程式の係数、3要素または4要素の配列。
roots : output array of real roots that has 1 or 3 elements.,roots : 実根の出力配列で，1 個または 3 個の要素を持つ．
The function solveCubic finds the real roots of a cubic equation:if coeffs is a 4-element vector: ,関数 solveCubic は， coeffs が 4 要素のベクトルである場合に，3 次方程式の実根を求めます．
\[\texttt{coeffs} [0] x^3 + \texttt{coeffs} [1] x^2 + \texttt{coeffs} [2] x + \texttt{coeffs} [3] = 0\],\coeffs を 4 要素のベクトルとすると[0] x^3 + ˶ˆ꒳ˆ˵[1] x^2 + \\[2] x + ˶ˆ꒳ˆ˵ )[3] = 0\]
if coeffs is a 3-element vector: ,coeffsが3要素のベクトルの場合。
\[x^3 + \texttt{coeffs} [0] x^2 + \texttt{coeffs} [1] x + \texttt{coeffs} [2] = 0\]The roots are stored in the roots array.,\x^3 + ˶ˆ꒳ˆ˵ )[0] x^2 + ˶‾᷄д‾᷅˵ [1] x + ˶‾᷅˵ [2] x^2 + ˶‾᷅˵ [3] = 0[1] x + ˶ˆ꒳ˆ˵ [2] = 0[2] = 0\]根は roots配列に格納されます。
Finds the real or complex roots of a polynomial equation.,多項式の実根または複素根を求めます。
coeffs : array of polynomial coefficients.,coeffs : 多項式の係数の配列．
roots : output (complex) array of roots.,roots : ルーツの出力（複素数）配列．
maxIters : maximum number of iterations the algorithm does.,maxIters : アルゴリズムが行う反復の最大数．
The function cv::solvePoly finds real and complex roots of a polynomial equation:\[\texttt{coeffs} [n] x^{n} + \texttt{coeffs} [n-1] x^{n-1} + ... + \texttt{coeffs} [1] x + \texttt{coeffs} [0] = 0\],関数 cv::solvePoly は，多項式方程式の実根と複素根を求めます：˶ˆ꒳ˆ˵ )[n] x^{n}.+ ˶ˆ꒳ˆ˵ )[n-1] x^{n-1} + ...+ ˶ˆ꒳ˆ˵[1] x + ˶ˆ꒳ˆ˵[0] = 0\]
Calculates eigenvalues and eigenvectors of a symmetric matrix.,対称行列の固有値と固有ベクトルを求めます．
"src : input matrix that must have CV_32FC1 or CV_64FC1 type, square size and be symmetrical (src ^T^ == src).",src : CV_32FC1 または CV_64FC1 型の入力行列．サイズは正方で，対称でなければいけません（src ^T^ == src）．
eigenvalues : output vector of eigenvalues of the same type as src; the eigenvalues are stored in the descending order.,eigenvalues : src と同じ型の固有値の出力ベクトル．
"eigenvectors : output matrix of eigenvectors; it has the same size and type as src; the eigenvectors are stored as subsequent matrix rows, in the same order as the corresponding eigenvalues.",src と同じサイズ，同じ型の固有ベクトルの出力行列．固有ベクトルは，対応する固有値と同じ順番で，行列の行として保存されます．
"The function cv::eigen calculates just eigenvalues, or eigenvalues and eigenvectors of the symmetric matrix src:src*eigenvectors.row(i).t() = eigenvalues.at<srcType>(i)*eigenvectors.row(i).t()fragmentNoteUse cv::eigenNonSymmetric for calculation of real eigenvalues and eigenvectors of non-symmetric matrix.See alsoeigenNonSymmetric, completeSymm , PCA","関数 cv::eigen は，対称行列 src の固有値のみ，あるいは固有値と固有ベクトルを計算します： src*eigenvectors.row(i).t() = eigenvalues.at<srcType>(i)*eigenvectors.row(i).t()fragmentNote 非対称行列の実在の固有値と固有ベクトルを計算するには， cv::eigenNonSymmetric を利用します．eigenNonSymmetric, completeSymm , PCA も参照してください．"
Calculates eigenvalues and eigenvectors of a non-symmetric matrix (real eigenvalues only).,非対称な行列の固有値と固有ベクトルを求めます（実数の固有値のみ）．
src : input matrix (CV_32FC1 or CV_64FC1 type).,src : 入力行列（CV_32FC1 または CV_64FC1 型）．
eigenvalues : output vector of eigenvalues (type is the same type as src).,eigenvalues : 固有値を表す出力ベクトル（type は src と同じです）．
"eigenvectors : output matrix of eigenvectors (type is the same type as src). The eigenvectors are stored as subsequent matrix rows, in the same order as the corresponding eigenvalues.",eigenvectors : 固有ベクトルの出力行列（タイプは src と同じ）．固有ベクトルは，対応する固有値と同じ順番で，後続の行列の行として保存されます．
NoteAssumes real eigenvalues.The function calculates eigenvalues and eigenvectors (optional) of the square matrix src:src*eigenvectors.row(i).t() = eigenvalues.at<srcType>(i)*eigenvectors.row(i).t()fragmentSee alsoeigen,注意実数の固有値を仮定しています．この関数は，正方行列 src の固有値と固有ベクトル（オプション）を計算します： src*eigenvectors.row(i).t() = eigenvalues.at<srcType>(i)*eigenvectors.row(i).t()fragment See alsoeigen
Calculates the covariance matrix of a set of vectors.,ベクトルの集合の共分散行列を計算します。
samples : samples stored as separate matrices,samples : 別々の行列として格納されたサンプル
nsamples : number of samples,nsamples : サンプルの数
covar : output covariance matrix of the type ctype and square size.,covar : 型が ctype でサイズが square の共分散行列を出力．
mean : input or output (depending on the flags) array as the average value of the input vectors.,mean : 入力ベクトルの平均値としての入力または出力（flags に依存）配列．
flags : operation flags as a combination of CovarFlags,flags : CovarFlagsの組み合わせによる操作フラグ．
ctype : type of the matrixl; it equals 'CV_64F' by default.,ctype : 行列の型．デフォルトでは，'CV_64F'になります．
"The function cv::calcCovarMatrix calculates the covariance matrix and, optionally, the mean vector of the set of input vectors.See alsoPCA, mulTransposed, MahalanobisTodo:InputArrayOfArrays",関数 cv::calcCovarMatrix は，入力ベクトル集合の共分散行列と，オプションとして平均ベクトルを求めます．
samples : samples stored as rows/columns of a single matrix.,samples : 1つの行列の行/列として格納されたサンプル．
Noteuse COVAR_ROWS or COVAR_COLS flag,COVAR_ROWS または COVAR_COLS フラグを利用してください．
wrap PCA::operator(),ラップ PCA::operator()
wrap PCA::project,ラップ PCA::プロジェクト
wrap PCA::backProject,ラップ PCA::バックプロジェクト
wrap SVD::compute,wrap SVD::compute
Examples: samples/cpp/tutorial_code/features2D/Homography/pose_from_homography.cpp.,例: samples/cpp/tutorial_code/features2D/Homography/pose_from_homography.cpp.
wrap SVD::backSubst,ラップ SVD::BackSubst
Calculates the Mahalanobis distance between two vectors.,2 つのベクトル間のマハラノビス距離を計算します。
v1 : first 1D input vector.,v1 : 1番目の1次元入力ベクトル。
v2 : second 1D input vector.,v2 : 2 番目の 1 次元入力ベクトル。
icovar : inverse covariance matrix.,icovar : 共分散行列の逆行列．
"The function cv::Mahalanobis calculates and returns the weighted distance between two vectors:\[d( \texttt{vec1} , \texttt{vec2} )= \sqrt{\sum_{i,j}{\texttt{icovar(i,j)}\cdot(\texttt{vec1}(I)-\texttt{vec2}(I))\cdot(\texttt{vec1(j)}-\texttt{vec2(j)})} }\]The covariance matrix may be calculated using the calcCovarMatrix function and then inverted using the invert function (preferably using the DECOMP_SVD method, as the most accurate).",関数 cv::Mahalanobis は，2 つのベクトル間の重み付き距離を計算し，それを返します．\d( ˶ˆ꒳ˆ˵ ) = ˶ˆ꒳ˆ˵ (˶ˆ꒳ˆ˵)CalcCovarMatrix関数を用いて共分散行列を計算し，invert関数を用いて逆行列を求めることができます（最も精度の高いDECOMP_SVD法を用いるのが望ましい）。
Performs a forward or inverse Discrete Fourier transform of a 1D or 2D floating-point array.,1 次元または 2 次元の浮動小数点型配列に対して，離散フーリエ変換または逆変換を行います．
src : input array that could be real or complex.,src : 実数または複素数の入力配列．
dst : output array whose size and type depends on the flags .,dst : 出力配列．そのサイズと型は flags に依存します．
"flags : transformation flags, representing a combination of the DftFlags",flags : DftFlags の組み合わせで表される変換フラグ．
"nonzeroRows : when the parameter is not zero, the function assumes that only the first nonzeroRows rows of the input array (DFT_INVERSE is not set) or only the first nonzeroRows of the output array (DFT_INVERSE is set) contain non-zeros, thus, the function can handle the rest of the rows more efficiently and save some time; this technique is very useful for calculating array cross-correlation or convolution using DFT.",nonzeroRows : このパラメータが 0 ではない場合，この関数は，入力配列の最初の nonzeroRows 行のみ（DFT_INVERSE がセットされていない），あるいは出力配列の最初の nonzeroRows 行のみ（DFT_INVERSE がセットされている）に非ゼロが含まれていると仮定します．
The function cv::dft performs one of the following:Forward the Fourier transform of a 1D vector of N elements: ,関数 cv::dft は，以下のいずれかの処理を行います： N 個の要素を持つ 1 次元ベクトルのフーリエ変換を行います．
"\[Y = F^{(N)} \cdot X,\]","\Y = F^{(N)} ˶cdot X,˶]となります．"
 where \(F^{(N)}_{jk}=\exp(-2\pi i j k/N)\) and \(i=\sqrt{-1}\), ここで、\(F^{(N)}_{jk}=\exp(-2pi i j k/N)\)、\(i=sqrt{-1}\)とします。
Inverse the Fourier transform of a 1D vector of N elements: ,N個の要素を持つ1次元ベクトルのフーリエ変換を逆変換します。
"\[\begin{array}{l} X'= \left (F^{(N)} \right )^{-1} \cdot Y = \left (F^{(N)} \right )^* \cdot y \\ X = (1/N) \cdot X, \end{array}\]",\♪♪♪♪♪～X'= ୨୧ (F^{(N)})\(1/N) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ )
 where \(F^*=\left(\textrm{Re}(F^{(N)})-\textrm{Im}(F^{(N)})\right)^T\), ここで、F^*=\left(F^{(N)})-\textrm{Im}(F^{(N)})\right)^T\)
Forward the 2D Fourier transform of a M x N matrix: ,M x N の行列の 2 次元フーリエ変換を進めます。
\[Y = F^{(M)} \cdot X \cdot F^{(N)}\],Y = F^{(M)} \cdot X \cdot F^{(N)}\[Y = F^{(M)} \cdot X ½cdot F^{(N)}\]となります。
Inverse the 2D Fourier transform of a M x N matrix: ,M×Nの行列の2次元フーリエ変換を逆にします。
"\[\begin{array}{l} X'= \left (F^{(M)} \right )^* \cdot Y \cdot \left (F^{(N)} \right )^* \\ X = \frac{1}{M \cdot N} \cdot X' \end{array}\]In case of real (single-channel) data, the output spectrum of the forward Fourier transform or input spectrum of the inverse Fourier transform can be represented in a packed format called CCS (complex-conjugate-symmetrical). It was borrowed from IPL (Intel* Image Processing Library). Here is how 2D CCS spectrum looks:\[\begin{bmatrix} Re Y_{0,0} & Re Y_{0,1} & Im Y_{0,1} & Re Y_{0,2} & Im Y_{0,2} & \cdots & Re Y_{0,N/2-1} & Im Y_{0,N/2-1} & Re Y_{0,N/2} \\ Re Y_{1,0} & Re Y_{1,1} & Im Y_{1,1} & Re Y_{1,2} & Im Y_{1,2} & \cdots & Re Y_{1,N/2-1} & Im Y_{1,N/2-1} & Re Y_{1,N/2} \\ Im Y_{1,0} & Re Y_{2,1} & Im Y_{2,1} & Re Y_{2,2} & Im Y_{2,2} & \cdots & Re Y_{2,N/2-1} & Im Y_{2,N/2-1} & Im Y_{1,N/2} \\ \hdotsfor{9} \\ Re Y_{M/2-1,0} & Re Y_{M-3,1} & Im Y_{M-3,1} & \hdotsfor{3} & Re Y_{M-3,N/2-1} & Im Y_{M-3,N/2-1}& Re Y_{M/2-1,N/2} \\ Im Y_{M/2-1,0} & Re Y_{M-2,1} & Im Y_{M-2,1} & \hdotsfor{3} & Re Y_{M-2,N/2-1} & Im Y_{M-2,N/2-1}& Im Y_{M/2-1,N/2} \\ Re Y_{M/2,0} & Re Y_{M-1,1} & Im Y_{M-1,1} & \hdotsfor{3} & Re Y_{M-1,N/2-1} & Im Y_{M-1,N/2-1}& Re Y_{M/2,N/2} \end{bmatrix}\]In case of 1D transform of a real vector, the output looks like the first row of the matrix above.So, the function chooses an operation mode depending on the flags and size of the input array:If DFT_ROWS is set or the input array has a single row or single column, the function performs a 1D forward or inverse transform of each row of a matrix when DFT_ROWS is set. Otherwise, it performs a 2D transform.","\♪♪♪♪♪♪～♪ X'= ♪ ♪ left (F^{(M)})Y\\\\\♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪\実データ（シングルチャンネル）の場合、フーリエ変換の出力スペクトルや逆フーリエ変換の入力スペクトルは、CCS（complex-conjugate-symmetrical）というパック形式で表現できます。これはIPL(Intel* Image Processing Library)から拝借したものです。2次元CCSスペクトルはこんな感じです。Re Y_{0,0} & Re Y_{0,1} & Im Y_{0,1} & Re Y_{0,2} & Im Y_{0,2} & ˶ˆ꒳ˆ˵ ) Re Y_{0,N/2-1} & Im Y_{0,N/2-1} & Re Y_{0,N/2} ˶ˆ꒳ˆ˵ ) Re Y_{1,0} & Re Y_{1,1} & Im Y_{1,1} & Re Y_{1,2} & Im Y_{1,2} & ˶ˆ꒳ˆ˵ & Re Y_{1,N/2-1} & Im Y_{1,N/2-1} & Re Y_{1,N/2} ˶ˆ꒳ˆ˵ Im Y_{1,0} & Re Y_{2,1} & Im Y_{2,1} & Re Y_{2,2} & Im Y_{2,2} & \\ & Re Y_{2,N/2-1} & Im Y_{2,N/2-1} & Im Y_{1,N/2} \\\Re Y_{M/2-1,0} & Re Y_{M-3,1} & Im Y_{M-3,1} & \\\\N/2} ¶ Im Y_{M/2-1,0} & Re Y_{M-2,1} & Im Y_{M-2,1} & ¶hdotsfor{3} & Re Y_{M-2,N/2-1} & Im Y_{M-2,N/2-1}& Im Y_{M/2-1,N/2} \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\そこで，この関数は，フラグと入力配列のサイズに応じて処理モードを選択します： DFT_ROWS がセットされている場合，あるいは入力配列が1行あるいは1列の場合，この関数は，行列の各行に対して1次元の順変換あるいは逆変換を行います．それ以外の場合は，2次元変換を行います．"
"If the input array is real and DFT_INVERSE is not set, the function performs a forward 1D or 2D transform:",入力配列が実数で，DFT_INVERSE がセットされていない場合，この関数は 1 次元または 2 次元の順変換を行います。
"When DFT_COMPLEX_OUTPUT is set, the output is a complex matrix of the same size as input.",DFT_COMPLEX_OUTPUT がセットされている場合，入力と同じサイズの複素行列が出力されます．
"When DFT_COMPLEX_OUTPUT is not set, the output is a real matrix of the same size as input. In case of 2D transform, it uses the packed format as shown above. In case of a single 1D transform, it looks like the first row of the matrix above. In case of multiple 1D transforms (when using the DFT_ROWS flag), each row of the output matrix looks like the first row of the matrix above.",DFT_COMPLEX_OUTPUT がセットされていない場合，出力は入力と同じサイズの実数行列です．2次元変換の場合は，上記のようなパック形式を使用します。1つの1次元変換の場合，上述の行列の最初の行のようになります。複数の1次元変換の場合（DFT_ROWS フラグを利用した場合），出力行列の各行は，上述の行列の最初の行のようになります．
"If the input array is complex and either DFT_INVERSE or DFT_REAL_OUTPUT are not set, the output is a complex array of the same size as input. The function performs a forward or inverse 1D or 2D transform of the whole input array or each row of the input array independently, depending on the flags DFT_INVERSE and DFT_ROWS.",入力配列が複素数で，かつ DFT_INVERSE や DFT_REAL_OUTPUT がセットされていない場合，出力は入力と同じサイズの複素数配列になります．この関数は，フラグ DFT_INVERSE と DFT_ROWS に応じて，入力配列全体，あるいは入力配列の各行に対して，個別に 1 次元または 2 次元の順変換，逆変換を行います。
"When DFT_INVERSE is set and the input array is real, or it is complex but DFT_REAL_OUTPUT is set, the output is a real array of the same size as input. The function performs a 1D or 2D inverse transformation of the whole input array or each individual row, depending on the flags DFT_INVERSE and DFT_ROWS.If DFT_SCALE is set, the scaling is done after the transformation.Unlike dct , the function supports arrays of arbitrary size. But only those arrays are processed efficiently, whose sizes can be factorized in a product of small prime numbers (2, 3, and 5 in the current implementation). Such an efficient DFT size can be calculated using the getOptimalDFTSize method.The sample below illustrates how to calculate a DFT-based convolution of two 2D real arrays:void convolveDFT(InputArray A, InputArray B, OutputArray C){    // reallocate the output array if needed    C.create(abs(A.rows - B.rows)+1, abs(A.cols - B.cols)+1, A.type());    Size dftSize;    // calculate the size of DFT transform    dftSize.width = getOptimalDFTSize(A.cols + B.cols - 1);    dftSize.height = getOptimalDFTSize(A.rows + B.rows - 1);    // allocate temporary buffers and initialize them with 0's    Mat tempA(dftSize, A.type(), Scalar::all(0));    Mat tempB(dftSize, B.type(), Scalar::all(0));    // copy A and B to the top-left corners of tempA and tempB, respectively    Mat roiA(tempA, Rect(0,0,A.cols,A.rows));    A.copyTo(roiA);    Mat roiB(tempB, Rect(0,0,B.cols,B.rows));    B.copyTo(roiB);    // now transform the padded A & B in-place;    // use ""nonzeroRows"" hint for faster processing    dft(tempA, tempA, 0, A.rows);    dft(tempB, tempB, 0, B.rows);    // multiply the spectrums;    // the function handles packed spectrum representations well    mulSpectrums(tempA, tempB, tempA);    // transform the product back from the frequency domain.    // Even though all the result rows will be non-zero,    // you need only the first C.rows of them, and thus you    // pass nonzeroRows == C.rows    dft(tempA, tempA, DFT_INVERSE + DFT_SCALE, C.rows);    // now copy the result back to C.    tempA(Rect(0, 0, C.cols, C.rows)).copyTo(C);    // all the temporary buffers will be deallocated automatically}fragmentTo optimize this sample, consider the following approaches:Since nonzeroRows != 0 is passed to the forward transform calls and since A and B are copied to the top-left corners of tempA and tempB, respectively, it is not necessary to clear the whole tempA and tempB. It is only necessary to clear the tempA.cols - A.cols ( tempB.cols - B.cols) rightmost columns of the matrices.","DFT_INVERSEがセットされていて，入力配列が実数の場合，あるいは複素数だがDFT_REAL_OUTPUTがセットされている場合，出力は入力と同じサイズの実数配列になります。この関数は，フラグ DFT_INVERSE と DFT_ROWS に応じて，入力配列全体，あるいは各行に対して，1次元あるいは2次元の逆変換を行います．DFT_SCALE がセットされている場合は，変換後にスケーリングが行われます．しかし，効率的に処理されるのは，そのサイズが小さな素数（現在の実装では，2，3，5）の積で因数分解できる配列だけです．このような効率的なDFTサイズは，getOptimalDFTSizeメソッドを用いて計算することができます．以下のサンプルは，2つの2次元実数配列のDFTベースの畳み込みを計算する方法を示しています： void convolveDFT(InputArray A, InputArray B, OutputArray C){ // 必要に応じて出力配列を再配置する C.create(abs(A.rows - B.rows)+1, abs(A.cols - B.cols)+1, A.type()); Size dftSize; // DFT 変換のサイズを計算する dftSize.width = getOptimalDFTSize(A.cols + B.cols - 1); dftSize.height = getOptimalDFTSize(A.rows + B.rows - 1); // 一時的なバッファを確保し，0 で初期化する Mat tempA(dftSize, A.type(), Scalar::all(0)); Mat tempB(dftSize, B.type(), Scalar::all(0)); // A と B をそれぞれ tempA と tempB の左上隅にコピーします． Mat roiA(tempA, Rect(0,0,A.cols,A.rows)); A.copyTo(roiA); Mat roiB(tempB, Rect(0,0,B.cols,B.rows)); B.copyTo(roiB); // パディングされた A & B をインプレースで変換します． // 高速処理のために ""nonzeroRows"" ヒントを利用します dft(tempA, tempA, 0, A.rows); dft(tempB, tempB, 0, B.rows); // スペクトルを乗算します．    // すべての結果行が非ゼロになるとしても， // 最初の C.rows だけが必要なので， // nonzeroRows == C.rows を渡します． dft(tempA, tempA, DFT_INVERSE + DFT_SCALE, C.rows); // ここで，結果を C にコピーします．copyTo(C); // すべての一時バッファは自動的に解放されます}fragmentこのサンプルを最適化するために，以下の方法を考えます： nonzeroRows != 0 が順変換の呼び出しに渡され，A と B はそれぞれ tempA と tempB の左上隅にコピーされるので，tempA と tempB 全体をクリアする必要はありません．tempA.cols - A.cols ( tempB.cols - B.cols ) の右端の列をクリアするだけでよいのです。"
"This DFT-based convolution does not have to be applied to the whole big arrays, especially if B is significantly smaller than A or vice versa. Instead, you can calculate convolution by parts. To do this, you need to split the output array C into multiple tiles. For each tile, estimate which parts of A and B are required to calculate convolution in this tile. If the tiles in C are too small, the speed will decrease a lot because of repeated work. In the ultimate case, when each tile in C is a single pixel, the algorithm becomes equivalent to the naive convolution algorithm. If the tiles are too big, the temporary arrays tempA and tempB become too big and there is also a slowdown because of bad cache locality. So, there is an optimal tile size somewhere in the middle.",このDFTベースの畳み込みは，特にBがAよりもかなり小さい場合やその逆の場合には，大きな配列全体に適用する必要はありません．その代わりに，部分的に畳み込みを計算することができます．そのためには，出力配列Cを複数のタイルに分割する必要があります．各タイルについて、そのタイルでコンボリューションを計算するために、AとBのどの部分が必要かを推定します。Cのタイルが小さすぎると、繰り返し作業が発生するため、速度が大きく低下します。究極的には、Cの各タイルが1ピクセルの場合、このアルゴリズムは、ナイーブコンボリューションアルゴリズムと同等になる。タイルが大きすぎると、一時的な配列であるtempAとtempBが大きくなりすぎ、また、キャッシュの局所性が悪くなるため、速度が低下してしまいます。つまり、中間に最適なタイルサイズがあるのです。
"If different tiles in C can be calculated in parallel and, thus, the convolution is done by parts, the loop can be threaded.All of the above improvements have been implemented in matchTemplate and filter2D . Therefore, by using them, you can get the performance even better than with the above theoretically optimal implementation. Though, those two functions actually calculate cross-correlation, not convolution, so you need to ""flip"" the second convolution operand B vertically and horizontally using flip .Note",C の異なるタイルが並行して計算され，その結果，畳み込みが部分的に行われるのであれば，ループをスレッド化することができます．上記の改良点はすべて matchTemplate と filter2D に実装されています．以上の改良点はすべてmatchTemplateとfilter2Dに実装されていますので，これらを使用すれば，上記の理論的に最適な実装よりもさらに優れた性能を得ることができます．ただし，この2つの関数は，実際には畳み込みではなく相互相関を計算しているので，flip.Noteを使って，2番目の畳み込みオペランドBを縦横に反転させる必要があります．
An example using the discrete fourier transform can be found at opencv_source_code/samples/cpp/dft.cpp,離散フーリエ変換を用いた例は、opencv_source_code/samples/cpp/dft.cppにあります。
(Python) An example using the dft functionality to perform Wiener deconvolution can be found at opencv_source/samples/python/deconvolution.py,(Python) dft 機能を使って Wiener デコンボリューションを行う例は opencv_source/samples/python/deconvolution.py にあります。
"(Python) An example rearranging the quadrants of a Fourier image can be found at opencv_source/samples/python/dft.pySee alsodct , getOptimalDFTSize , mulSpectrums, filter2D , matchTemplate , flip , cartToPolar , magnitude , phase",(Python) フーリエ画像の象限を並べ替える例は、opencv_source/samples/python/dft.pyにあります。
Calculates the inverse Discrete Fourier Transform of a 1D or 2D array.,1次元または2次元配列の離散フーリエ変換の逆変換を求めます。
src : input floating-point real or complex array.,src : 浮動小数点型の実数または複素数の入力配列．
dst : output array whose size and type depend on the flags.,dst : 出力配列．サイズと型は flags に依存します．
flags : operation flags (see dft and DftFlags).,flags : 処理フラグ（ dft および DftFlags 参照）．
nonzeroRows : number of dst rows to process; the rest of the rows have undefined content (see the convolution sample in dft description.,nonzeroRows : 処理する dst 行の数．残りの行は，未定義の内容を持ちます（ dft の説明にある畳み込みサンプルを参照してください）．
"idft(src, dst, flags) is equivalent to dft(src, dst, flags | DFT_INVERSE) .NoteNone of dft and idft scales the result by default. So, you should pass DFT_SCALE to one of dft or idft explicitly to make these transforms mutually inverse.See alsodft, dct, idct, mulSpectrums, getOptimalDFTSize","idft(src, dst, flags) は， dft(src, dst, flags | DFT_INVERSE) と同等です．注意 dft と idft のいずれも，デフォルトでは結果をスケーリングしません．参照：odft, dct, idct, mulSpectrums, getOptimalDFTSize"
Performs a forward or inverse discrete Cosine transform of 1D or 2D array.,1次元あるいは2次元の配列に対して，離散コサイン変換（順変換，逆変換）を行います．
dst : output array of the same size and type as src .,dst : src と同じサイズ，同じ型の出力配列．
flags : transformation flags as a combination of cv::DftFlags (DCT_*),flags : cv::DftFlags (DCT_*) の組み合わせで表される変換フラグ．
The function cv::dct performs a forward or inverse discrete Cosine transform (DCT) of a 1D or 2D floating-point array:Forward Cosine transform of a 1D vector of N elements: ,関数 cv::dct は，1次元あるいは2次元の浮動小数点型配列に対して，離散コサイン変換（DCT）を行います： N 個の要素を持つ 1 次元ベクトルに対するコサイン変換．
\[Y = C^{(N)} \cdot X\],\Y = C^{(N)} \cdot X\] となります．
 where , ここで
\[C^{(N)}_{jk}= \sqrt{\alpha_j/N} \cos \left ( \frac{\pi(2k+1)j}{2N} \right )\],\[C^{(N)}_{jk}= \sqrt{\alpha_j/N}\C^{(N)}_{jk}= C^{(N)}_{jk}= C^{(N)}_{jk}= C^{(N)}_{k+1)j}{2N}\右）になります。］
" and \(\alpha_0=1\), \(\alpha_j=2\) for j > 0.", となり、j > 0では、\\0=1\、\\j=2✝となります。
Inverse Cosine transform of a 1D vector of N elements: ,N個の要素を持つ1次元ベクトルの逆コサイン変換です。
\[X = \left (C^{(N)} \right )^{-1} \cdot Y = \left (C^{(N)} \right )^T \cdot Y\],\♪ X = ♪ left (C^{(N)})\¶X = ¶left (C^{(N)} ¶right )^{-1} ¶cdot Y = ¶left (C^{(N)} ¶right )^T ¶C\♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪］
" (since \(C^{(N)}\) is an orthogonal matrix, \(C^{(N)} \cdot \left(C^{(N)}\right)^T = I\) )", (C^{(N)}\)は直交行列なので、\(C^{(N)})(C^{(N)}\\left(C^{(N)}\right)^T = I\) )
Forward 2D Cosine transform of M x N matrix: ,M x N 行列の2次元コサイン変換を行います。
\[Y = C^{(N)} \cdot X \cdot \left (C^{(N)} \right )^T\],\Y = C^{(N)} \cdot X \cdot left (C^{(N)} \right )^T\]となります。
Inverse 2D Cosine transform of M x N matrix: ,M×N行列の逆2次元コサイン変換です。
"\[X = \left (C^{(N)} \right )^T \cdot X \cdot C^{(N)}\]The function chooses the mode of operation by looking at the flags and size of the input array:If (flags & DCT_INVERSE) == 0 , the function does a forward 1D or 2D transform. Otherwise, it is an inverse 1D or 2D transform.",\この関数は，入力配列のフラグとサイズを考慮して処理モードを決定します： (flags & DCT_INVERSE) == 0 の場合，この関数は 1 次元または 2 次元の順変換を行います。それ以外の場合は，1次元または2次元の逆変換を行います。
"If (flags & DCT_ROWS) != 0 , the function performs a 1D transform of each row.",(flags & DCT_ROWS) != 0 の場合，この関数は各行に対して1次元変換を行います。
"If the array is a single column or a single row, the function performs a 1D transform.",配列が1つの列，あるいは1つの行である場合，この関数は1次元の変換を行います。
"If none of the above is true, the function performs a 2D transform.NoteCurrently dct supports even-size arrays (2, 4, 6 ...). For data analysis and approximation, you can pad the array when necessary. Also, the function performance depends very much, and not monotonically, on the array size (see getOptimalDFTSize ). In the current implementation DCT of a vector of size N is calculated via DFT of a vector of size N/2 . Thus, the optimal DCT size N1 >= N can be calculated as: size_t getOptimalDCTSize(size_t N) { return 2*getOptimalDFTSize((N+1)/2); }N1 = getOptimalDCTSize(N);See alsodft , getOptimalDFTSize , idct","注意現在のdctは，偶数サイズの配列（2, 4, 6 ...）をサポートしています．データ解析や近似のために，必要に応じて配列をパディングすることができます．また，関数の性能は，配列のサイズに大きく依存し，単調ではありません（ getOptimalDFTSize を参照してください）．現在の実装では，サイズ N のベクトルに対する DCT は，サイズ N/2 のベクトルに対する DFT を介して計算されます．したがって，最適な DCT サイズ N1 >= N は次のように計算されます： size_t getOptimalDCTSize(size_t N) { return 2*getOptimalDFTSize((N+1)/2); }N1 = getOptimalDCTSize(N);See alsodft , getOptimalDFTSize , idct"
Calculates the inverse Discrete Cosine Transform of a 1D or 2D array.,1次元あるいは2次元配列の離散コサイン変換の逆変換を求めます．
src : input floating-point single-channel array.,src : 入力となる浮動小数点型シングルチャンネル配列．
flags : operation flags.,flags : 処理フラグ．
"idct(src, dst, flags) is equivalent to dct(src, dst, flags | DCT_INVERSE).See alsodct, dft, idft, getOptimalDFTSize","idct(src, dst, flags) は， dct(src, dst, flags | DCT_INVERSE) と同等です． 以下もご参照ください：odct, dft, idft, getOptimalDFTSize"
Performs the per-element multiplication of two Fourier spectrums.,2つのフーリエ・スペクトルの要素毎の乗算を行います．
a : first input array.,a : 1番目の入力配列．
b : second input array of the same size and type as src1 .,b : src1 と同じサイズ，同じ型の 2 番目の入力配列．
c : output array of the same size and type as src1 .,c : src1 と同じサイズ，同じ型の出力配列．
"flags : operation flags; currently, the only supported flag is cv::DFT_ROWS, which indicates that each row of src1 and src2 is an independent 1D Fourier spectrum. If you do not want to use this flag, then simply add a 0 as value.",flags : 操作フラグ．現在サポートされている唯一のフラグは cv::DFT_ROWS で，これは src1 と src2 の各行が独立した1次元フーリエ変換であることを示します．このフラグを利用したくない場合は，単に 0 を値として追加してください．
conjB : optional flag that conjugates the second input array before the multiplication (true) or not (false).,conjB : オプションフラグ．乗算の前に 2 番目の入力配列を共役にする（true），しない（false）を指定します．
"The function cv::mulSpectrums performs the per-element multiplication of the two CCS-packed or complex matrices that are results of a real or complex Fourier transform.The function, together with dft and idft , may be used to calculate convolution (pass conjB=false ) or correlation (pass conjB=true ) of two arrays rapidly. When the arrays are complex, they are simply multiplied (per element) with an optional conjugation of the second-array elements. When the arrays are real, they are assumed to be CCS-packed (see dft for details).",関数 cv::mulSpectrums は，実数または複素数のフーリエ変換の結果である，CCSパックされた2つの行列または複素数行列の要素毎の乗算を行います．この関数は， dft や idft と共に，2つの配列の畳み込み（ conjB=false を渡す）や相関（ conjB=true を渡す）を高速に計算するために利用されます．配列が複素数の場合，単純に（要素毎に）2番目の配列要素の共役を掛け合わせます．配列が実数の場合は，CCSパックされていると仮定されます（詳細は dft を参照してください）．
Returns the optimal DFT size for a given vector size.,与えられたベクトルサイズに対する最適な DFT サイズを返します．
vecsize : vector size.,vecsize : ベクトルサイズ．
"DFT performance is not a monotonic function of a vector size. Therefore, when you calculate convolution of two arrays or perform the spectral analysis of an array, it usually makes sense to pad the input data with zeros to get a bit larger array that can be transformed much faster than the original one. Arrays whose size is a power-of-two (2, 4, 8, 16, 32, ...) are the fastest to process. Though, the arrays whose size is a product of 2's, 3's, and 5's (for example, 300 = 5*5*3*2*2) are also processed quite efficiently.The function cv::getOptimalDFTSize returns the minimum number N that is greater than or equal to vecsize so that the DFT of a vector of size N can be processed efficiently. In the current implementation N = 2 ^p^ * 3 ^q^ * 5 ^r^ for some integer p, q, r.The function returns a negative number if vecsize is too large (very close to INT_MAX ).While the function cannot be used directly to estimate the optimal vector size for DCT transform (since the current DCT implementation supports only even-size vectors), it can be easily processed as getOptimalDFTSize((vecsize+1)/2)*2.See alsodft , dct , idft , idct , mulSpectrums","DFTの性能は，ベクトルサイズの単調な関数ではありません．そのため，2つの配列の畳み込みを計算したり，配列のスペクトル分析を行ったりする場合は，通常，入力データをゼロで埋めて，元の配列よりも高速に変換できる少し大きな配列を得ることが有効です．サイズが2の累乗（2，4，8，16，32，...）である配列は，最も高速に処理できます．関数 cv::getOptimalDFTSize は，サイズ N のベクトルの DFT を効率的に処理できるような， vecsize 以上の最小の数 N を返します．現在の実装では，N = 2 ^p^ * 3 ^q^ * 5 ^r^ for some integer p, q, r.この関数は， vecsize が大きすぎる（INT_MAX に非常に近い）場合，負の値を返します．この関数は，DCT変換のための最適なベクトルサイズを推定するために直接利用することはできませんが（現在のDCT実装は，偶数サイズのベクトルしかサポートしていないので）， getOptimalDFTSize((vecsize+1)/2)*2 のように簡単に処理することができます．"
Generates a single uniformly-distributed random number or an array of random numbers.,一様分散された単一の乱数または乱数配列を生成します。
dst : output array of random numbers; the array must be pre-allocated.,dst : 出力される乱数の配列，配列はあらかじめ確保されていなければならない．
low : inclusive lower boundary of the generated random numbers.,low : 生成される乱数の下界を含みます．
high : exclusive upper boundary of the generated random numbers.,high : 生成される乱数の排他的上限値．
"Non-template variant of the function fills the matrix dst with uniformly-distributed random numbers from the specified range:\[\texttt{low} _c \leq \texttt{dst} (I)_c < \texttt{high} _c\]See alsoRNG, randn, theRNGExamples: samples/cpp/cout_mat.cpp, and samples/cpp/falsecolor.cpp.","この関数の非テンプレート版は，指定された範囲の一様分布乱数で行列 dst を埋めます：˶‾᷄ -̫ ‾᷅˵˵RNG, randn, theRNGExamples: samples/cpp/cout_mat.cpp, samples/cpp/falecolor.cpp."
Fills the array with normally distributed random numbers.,配列を正規分布した乱数で埋めます．
dst : output array of random numbers; the array must be pre-allocated and have 1 to 4 channels.,dst : 乱数を出力する配列．この配列は，あらかじめ割り当てられており，1から4チャンネルを持つ必要があります．
mean : mean value (expectation) of the generated random numbers.,mean : 生成された乱数の平均値（期待値）を表します．
stddev : standard deviation of the generated random numbers; it can be either a vector (in which case a diagonal standard deviation matrix is assumed) or a square matrix.,stddev : 生成された乱数の標準偏差．これは，ベクトル（この場合，標準偏差の対角行列が仮定されます）か，正方行列のどちらかです．
"The function cv::randn fills the matrix dst with normally distributed random numbers with the specified mean vector and the standard deviation matrix. The generated random numbers are clipped to fit the value range of the output array data type.See alsoRNG, randuExamples: samples/cpp/kalman.cpp.","関数 cv::randn は，指定された平均ベクトルと標準偏差行列を持つ正規分布乱数で行列 dst を埋めます．生成された乱数は，出力配列のデータ型の値域に合うようにクリップされます．RNG, randuExamples: samples/cpp/kalman.cpp も参照してください．"
Shuffles the array elements randomly.,配列の要素をランダムにシャッフルします。
dst : input/output numerical 1D array.,dst : 入力/出力の数値1次元配列．
iterFactor : scale factor that determines the number of random swap operations (see the details below).,iterFactor : ランダムな入れ替え操作の回数を決定するスケールファクター（詳細は後述）．
"rng : optional random number generator used for shuffling; if it is zero, theRNG () is used instead.",rng : シャッフルに利用されるオプションの乱数生成器．これが0の場合，代わりにRNG () が利用されます．
"The function cv::randShuffle shuffles the specified 1D array by randomly choosing pairs of elements and swapping them. The number of such swap operations will be dst.rows*dst.cols*iterFactor .See alsoRNG, sortExamples: modules/shape/samples/shape_example.cpp, and samples/cpp/kmeans.cpp.","関数 cv::randShuffle は，指定された1次元配列に対して，ランダムに要素のペアを選び，それらを入れ替えることでシャッフルを行います．このようなスワップ操作の回数は， dst.rows*dst.cols*iterFactor となります．RNG, sortExamples: modules/shape/samples/shape_example.cpp, samples/cpp/kmeans.cpp も参照してください．"
Finds centers of clusters and groups input samples around the clusters.,クラスタの中心を見つけ、入力されたサンプルをそのクラスタの周りにグループ化します。
data : Data for clustering. An array of N-Dimensional points with float coordinates is needed. Examples of this array can be:,data :クラスタリングのためのデータ．N次元の点をfloat座標で表した配列が必要です．この配列の例は，以下の通りです．
"Mat points(count, 2, CV_32F);","Mat points(count, 2, CV_32F)．"
"Mat points(count, 1, CV_32FC2);","Mat points(count, 1, CV_32FC2);;"
"Mat points(1, count, CV_32FC2);","Mat points(1, count, CV_32FC2);"
std::vector<cv::Point2f> points(sampleCount);,std::vector<cv::Point2f> points(sampleCount);
K : Number of clusters to split the set by.,K : セットを分割するクラスタの数．
bestLabels : Input/output integer array that stores the cluster indices for every sample.,bestLabels : 各サンプルのクラスタインデックスを格納する，入出力可能な整数型配列．
"criteria : The algorithm termination criteria, that is, the maximum number of iterations and/or the desired accuracy. The accuracy is specified as criteria.epsilon. As soon as each of the cluster centers moves by less than criteria.epsilon on some iteration, the algorithm stops.",criteria :アルゴリズムの終了基準，すなわち，最大反復回数および/または希望する精度．精度はcliteria.epsilonとして指定されます．ある反復において，各クラスタ中心の移動量がcliteria.epsilon以下になった時点で，アルゴリズムは停止する．
attempts : Flag to specify the number of times the algorithm is executed using different initial labellings. The algorithm returns the labels that yield the best compactness (see the last function parameter).,attempts :異なる初期ラベリングを用いてアルゴリズムを実行する回数を指定するフラグ。このアルゴリズムは，最もコンパクトになるラベルを返す（最後の関数パラメータを参照）．
flags : Flag that can take values of cv::KmeansFlags,flags :cv::KmeansFlags の値を取ることができるフラグ．
"centers : Output matrix of the cluster centers, one row per each cluster center.",centers : クラスタ中心を表す出力行列，各クラスタ中心毎に1行です．
"The function kmeans implements a k-means algorithm that finds the centers of cluster_count clusters and groups the input samples around the clusters. As an output, \(\texttt{bestLabels}_i\) contains a 0-based cluster index for the sample stored in the \(i^{th}\) row of the samples matrix.Note",関数 kmeans は，k-means アルゴリズムを実装しています．これは， cluster_count 個のクラスタの中心を見つけ，そのクラスタの周りに入力サンプルをグループ化するものです．出力として，Samples行列のSamples(i^{th}\)行に格納されているサンプルの0ベースのクラスタインデックスが含まれています．
(Python) An example on K-means clustering can be found at opencv_source_code/samples/python/kmeans.pyExamples: samples/cpp/kmeans.cpp.,(Python) K-means クラスタリングの例は、opencv_source_code/samples/python/kmeans.pyExamples: samples/cpp/kmeans.cpp にあります。
Computes the cube root of an argument.,引数の立方根を計算します。
val : A function argument.,val : 関数の引数です。
The function cubeRoot computes \(\sqrt[3]{\texttt{val}}\). Negative arguments are handled correctly. NaN and Inf are not handled. The accuracy approaches the maximum possible accuracy for single-precision data.,cubeRoot関数は，\(\sqrt[3]{\texttt{val}}\)を計算します。負の引数は正しく処理されます。NaN，Infは処理されません。精度は、単精度データの最大可能精度に近づきます。
Calculates the angle of a 2D vector in degrees.,2次元ベクトルの角度を度単位で計算します。
x : x-coordinate of the vector.,x : ベクトルのx座標．
y : y-coordinate of the vector.,y : ベクトルのy座標
The function fastAtan2 calculates the full-range angle of an input 2D vector. The angle is measured in degrees and varies from 0 to 360 degrees. The accuracy is about 0.3 degrees.,関数 fastAtan2 は，入力された 2 次元ベクトルの角度を全範囲にわたって計算します．角度の単位は度で，0度から360度の範囲で変化します．精度は約0.3度です。
Stores algorithm parameters in a file storage.,アルゴリズムのパラメータを，ファイルストレージに保存します．
"Reimplemented in cv::FlannBasedMatcher, cv::DescriptorMatcher, cv::face::FaceRecognizer, cv::optflow::GPCForest< T >, cv::bioinspired::Retina, cv::line_descriptor::BinaryDescriptor, cv::Feature2D, cv::optflow::GPCTree, cv::bioinspired::TransientAreasSegmentationModule, cv::legacy::Tracker, cv::saliency::StaticSaliencySpectralResidual, and cv::face::BasicFaceRecognizer.","cv::FlannBasedMatcher, cv::DescriptorMatcher, cv::face::FaceRecognizer, cv::optflow::GPCForest< T >, cv::bioinspired::Retina, cv::line_descriptor::BinaryDescriptor, cv...で再実装されています．:Feature2D, cv::optflow::GPCTree, cv::bioinspired::TransientAreasSegmentationModule, cv::legacy::Tracker, cv::saliency::StaticSaliencySpectralResidual, and cv::face::BasicFaceRecognizer."
Reads algorithm parameters from a file storage.,ファイルストレージからアルゴリズムパラメータを読み込みます．
"Reimplemented in cv::FlannBasedMatcher, cv::DescriptorMatcher, cv::face::FaceRecognizer, cv::optflow::GPCForest< T >, cv::line_descriptor::BinaryDescriptor, cv::Feature2D, cv::optflow::GPCTree, cv::legacy::Tracker, cv::saliency::StaticSaliencySpectralResidual, and cv::face::BasicFaceRecognizer.","cv::FlannBasedMatcher, cv::DescriptorMatcher, cv::face::FaceRecognizer, cv::optflow::GPCForest< T >, cv::line_descriptor::BinaryDescriptor, cv::Feature2D, cv::optflow::GPCTree, cv::legacy::Tracker, cv::saliency::StaticSaliencySpectralResidual, and cv::face::BasicFaceRecognizer."
Returns true if the Algorithm is empty (e.g. in the very beginning or after unsuccessful read.,アルゴリズムが空の場合（例えば，最初の頃や読み込みに失敗した後など）は，真を返します．
"Reimplemented in cv::DescriptorMatcher, cv::face::FaceRecognizer, cv::ml::StatModel, cv::Feature2D, cv::BaseCascadeClassifier, cv::cuda::DescriptorMatcher, cv::quality::QualityPSNR, cv::quality::QualityBase, cv::face::BasicFaceRecognizer, cv::quality::QualityGMSD, cv::quality::QualitySSIM, and cv::quality::QualityMSE.","cv::DescriptorMatcher, cv::face::FaceRecognizer, cv::ml::StatModel, cv::Feature2D, cv::BaseCascadeClassifier, cv::cuda::DescriptorMatcher, cv. で再実装されました．:quality::QualityPSNR, cv::quality::QualityBase, cv::face::BasicFaceRecognizer, cv::quality::QualityGMSD, cv::quality::QualitySSIM, そして cv::quality::QualityMSE."
"Saves the algorithm to a file. In order to make this method work, the derived class must implement Algorithm::write(FileStorage& fs).",アルゴリズムをファイルに保存します．このメソッドを動作させるためには，派生クラスが Algorithm::write(FileStorage& fs) を実装する必要があります．
Principal Component Analysis. ,主成分分析を行います。
"The class is used to calculate a special basis for a set of vectors. The basis will consist of eigenvectors of the covariance matrix calculated from the input set of vectors. The class PCA can also transform vectors to/from the new coordinate space defined by the basis. Usually, in this new coordinate system, each vector from the original set (and any linear combination of such vectors) can be quite accurately approximated by taking its first few components, corresponding to the eigenvectors of the largest eigenvalues of the covariance matrix. Geometrically it means that you calculate a projection of the vector to a subspace formed by a few eigenvectors corresponding to the dominant eigenvalues of the covariance matrix. And usually such a projection is very close to the original vector. So, you can represent the original vector from a high-dimensional space with a much shorter vector consisting of the projected vector's coordinates in the subspace. Such a transformation is also known as Karhunen-Loeve Transform, or KLT. See http://en.wikipedia.org/wiki/Principal_component_analysis",このクラスは、ベクトルの集合に対する特別な基底を計算するために使用されます。基底は、入力されたベクトルの集合から計算された共分散行列の固有ベクトルで構成されます。PCA クラスは，基底によって定義された新しい座標空間との間で，ベクトルを変換することもできます．通常，この新しい座標系では，元の集合からの各ベクトル（およびそのようなベクトルの任意の線形結合）は，共分散行列の最大の固有値の固有ベクトルに対応する最初の数個の成分を取ることによって，非常に正確に近似することができます．幾何学的には、共分散行列の支配的な固有値に対応するいくつかの固有ベクトルによって形成される部分空間へのベクトルの投影を計算することを意味します。そして、通常、このような投影は、元のベクトルに非常に近いものになります。つまり、高次元空間の元のベクトルを、部分空間に投影されたベクトルの座標からなるずっと短いベクトルで表すことができるのです。このような変換は、Karhunen-Loeve変換（KLT）とも呼ばれます。http://en.wikipedia.org/wiki/Principal_component_analysis 参照
"The sample below is the function that takes two matrices. The first function stores a set of vectors (a row per vector) that is used to calculate PCA. The second function stores another ""test"" set of vectors (a row per vector). First, these vectors are compressed with PCA, then reconstructed back, and then the reconstruction error norm is computed and printed for each vector. :",下のサンプルは，2つの行列を受け取る関数です．1つ目の関数は，PCAの計算に使用されるベクトルの集合（1つのベクトルにつき1行）を格納します．2つ目の関数は，もう1つの「テスト」用のベクトルセット（1つのベクトルにつき1行）を格納します．まず，これらのベクトルは PCA によって圧縮され，次に再構成されます．そして，再構成誤差ノルムが計算され，各ベクトルに対して出力されます．
"using namespace cv;PCA compressPCA(const Mat& pcaset, int maxComponents,                const Mat& testset, Mat& compressed){    PCA pca(pcaset, // pass the data            Mat(), // we do not have a pre-computed mean vector,                   // so let the PCA engine to compute it            PCA::DATA_AS_ROW, // indicate that the vectors                                // are stored as matrix rows                                // (use PCA::DATA_AS_COL if the vectors are                                // the matrix columns)            maxComponents // specify, how many principal components to retain            );    // if there is no test data, just return the computed basis, ready-to-use    if( !testset.data )        return pca;    CV_Assert( testset.cols == pcaset.cols );    compressed.create(testset.rows, maxComponents, testset.type());    Mat reconstructed;    for( int i = 0; i < testset.rows; i++ )    {        Mat vec = testset.row(i), coeffs = compressed.row(i), reconstructed;        // compress the vector, the result will be stored        // in the i-th row of the output matrix        pca.project(vec, coeffs);        // and then reconstruct it        pca.backProject(coeffs, reconstructed);        // and measure the error        printf(""%d. diff = %g\n"", i, norm(vec, reconstructed, NORM_L2));    }    return pca;} See alsocalcCovarMatrix, mulTransposed, SVD, dft, dct ","using namespace cv;PCA compressPCA(const Mat& pcaset, int maxComponents, const Mat& testset, Mat& compressed){ PCA pca(pcaset, // データを渡す Mat(), // 事前に計算された平均ベクトルを持っていないので， // PCA エンジンに計算させます PCA::DATA_AS_ROW, // ベクトルが // 行列の行として格納されていることを示します // （ベクトルが // 行列の列である場合は PCA::DATA_AS_COL を利用してください） maxComponents // 保持する主成分の数を指定します ); // テストデータがない場合は，計算された基底を返すだけで，すぐに利用できます if( !testset.data ) return pca; CV_Assert( testset.cols == pcaset.cols ); compressed.create(testset.rows, maxComponents, testset.type()); Mat reconstructed; for( int i = 0; i < testset.rows; i++ ) { Mat vec = testset.row(i), coeffs = compressed.row(i), reconstructed; // ベクトルを圧縮し，その結果を // 出力行列の i 番目の行に格納します． pca.project(vec, coeffs); // そして，それを再構成します． pca.backProject(coeffs, reconstructed); // そして，その誤差を測定します．関連項目：ocalcCovarMatrix, mulTransposed, SVD, dft, dct"
"Examples: samples/cpp/pca.cpp, and samples/cpp/tutorial_code/ml/introduction_to_pca/introduction_to_pca.cpp.","例: samples/cpp/pca.cpp, samples/cpp/tutorial_code/ml/introduction_to_pca/introduction_to_pca.cpp."
Projects vector(s) to the principal component subspace.,ベクトルを主成分部分空間に投射します．
"vec : input vector(s); must have the same dimensionality and the same layout as the input data used at PCA phase, that is, if DATA_AS_ROW are specified, then vec.cols==data.cols (vector dimensionality) and vec.rows is the number of vectors to project, and the same is true for the PCA::DATA_AS_COL case.",vec : 入力ベクトル（複数可）; PCA フェーズで使用される入力データと同じ次元，同じレイアウトでなければなりません．つまり，DATA_AS_ROW が指定された場合， vec.cols==data.cols（ベクトルの次元）， vec.rows は投影するベクトルの数であり，PCA::DATA_AS_COL の場合も同様になります．
"The methods project one or more vectors to the principal component subspace, where each vector projection is represented by coefficients in the principal component basis. The first form of the method returns the matrix that the second form writes to the result. So the first form can be used as a part of expression while the second form can be more efficient in a processing loop.Examples: samples/cpp/pca.cpp.",これらのメソッドは，1 つ以上のベクトルを主成分部分空間に射影します．各ベクトルの射影は，主成分基底の係数で表されます．このメソッドの第 1 の形式は，第 2 の形式が結果に書き込む行列を返します．したがって，第1の形式は式の一部として使用でき，第2の形式は処理ループの中でより効率的に使用できます．例：samples/cpp/pca.cpp.
"result : output vectors; in case of PCA::DATA_AS_COL, the output matrix has as many columns as the number of input vectors, this means that result.cols==vec.cols and the number of rows match the number of principal components (for example, maxComponents parameter passed to the constructor).",result : 出力ベクトル．PCA::DATA_AS_COL の場合，出力行列は入力ベクトルの数と同じ数の列を持ちます．つまり， result.cols==vec.cols であり，行数は主成分の数（例えば，コンストラクタに渡されるパラメータ maxComponents）と一致します．
Reconstructs vectors from their PC projections.,ベクトルをそのPC投影から再構成します．
"vec : coordinates of the vectors in the principal component subspace, the layout and size are the same as of PCA::project output vectors.",vec : 主成分部分空間におけるベクトルの座標，レイアウトとサイズは，PCA::project の出力ベクトルと同じです．
"The methods are inverse operations to PCA::project. They take PC coordinates of projected vectors and reconstruct the original vectors. Unless all the principal components have been retained, the reconstructed vectors are different from the originals. But typically, the difference is small if the number of components is large enough (but still much smaller than the original vector dimensionality). As a result, PCA is used.Examples: samples/cpp/pca.cpp.",これらのメソッドは，PCA::project の逆の操作です．これらのメソッドは，投影されたベクトルのPC座標を取り，元のベクトルを再構成します．すべての主成分が保持されていない限り，再構成されたベクトルは元のベクトルとは異なります．しかし，成分数が十分に多ければ，その差は小さくなります（それでも，元のベクトルの次元数よりはずっと小さいです）．例：samples/cpp/pca.cpp.PCAを使用しています．
result : reconstructed vectors; the layout and size are the same as of PCA::project input vectors.,result : 再構成されたベクトル．レイアウトとサイズは，PCA::project の入力ベクトルと同じです．
eigenvectors of the covariation matrix,共分散行列の固有ベクトル
eigenvalues of the covariation matrix,共分散行列の固有値
mean value subtracted before the projection and added after the back projection,投影前に減算され，バックプロジェクション後に加算される平均値．
write PCA objects,PCA オブジェクトの書き込み
Writes eigenvalues eigenvectors and mean to specified FileStorage,固有値 固有ベクトルと平均値を指定された FileStorage に書き込みます．
load PCA objects,PCA オブジェクトの読み込み
Loads eigenvalues eigenvectors and mean from specified FileNode,固有値，固有ベクトル，平均値を指定した FileNode から読み込みます．
Fills arrays with random numbers.,配列を乱数で埋めます．
"mat : 2D or N-dimensional matrix; currently matrices with more than 4 channels are not supported by the methods, use Mat::reshape as a possible workaround.",mat : 2次元またはN次元の行列．現在のところ，4チャンネル以上の行列はこのメソッドではサポートされていませんが，回避策として Mat::reshape を利用してください．
"distType : distribution type, RNG::UNIFORM or RNG::NORMAL.",distType : 分布の種類，RNG::UNIFORM または RNG::NORMAL．
"a : first distribution parameter; in case of the uniform distribution, this is an inclusive lower boundary, in case of the normal distribution, this is a mean value.",a : 1番目の分布パラメータ．一様分布の場合は，包含的な下限値，正規分布の場合は，平均値になります．
"b : second distribution parameter; in case of the uniform distribution, this is a non-inclusive upper boundary, in case of the normal distribution, this is a standard deviation (diagonal of the standard deviation matrix or the full standard deviation matrix).",b : 2番目の分布パラメータ．一様分布の場合は，非包含的な上限値，正規分布の場合は，標準偏差（標準偏差行列の対角線，または完全な標準偏差行列）を表します．
"saturateRange : pre-saturation flag; for uniform distribution only; if true, the method will first convert a and b to the acceptable value range (according to the mat datatype) and then will generate uniformly distributed random numbers within the range [saturate(a), saturate(b)), if saturateRange=false, the method will generate uniformly distributed random numbers in the original range [a, b) and then will saturate them, it means, for example, that theRNG().fill(mat_8u, RNG::UNIFORM, -DBL_MAX, DBL_MAX) will likely produce array mostly filled with 0's and 255's, since the range (0, 255) is significantly smaller than [-DBL_MAX, DBL_MAX).","saturateRange : 事前飽和フラグ．一様分布のみを対象とします．真の場合，このメソッドは，まず a と b を（mat データ型に応じた）許容可能な値の範囲に変換してから，[saturate(a), saturate(b)]の範囲内で一様に分布する乱数を生成します．saturateRange=false の場合，このメソッドは，元の範囲 [a, b] 内で一様に分布する乱数を生成してから，それらを飽和させます．これは，例えば，RNG().fill(mat_8u, RNG::UNIFORM, -DBL_MAX, DBL_MAX) は，範囲 (0, 255) が [-DBL_MAX, DBL_MAX] よりもかなり小さいので，ほとんどが 0 と 255 で埋め尽くされた配列を生成する可能性が高いです．"
"Each of the methods fills the matrix with the random values from the specified distribution. As the new numbers are generated, the RNG state is updated accordingly. In case of multiple-channel images, every channel is filled independently, which means that RNG cannot generate samples from the multi-dimensional Gaussian distribution with non-diagonal covariance matrix directly. To do that, the method generates samples from multi-dimensional standard Gaussian distribution with zero mean and identity covariation matrix, and then transforms them using transform to get samples from the specified Gaussian distribution.Examples: samples/cpp/kmeans.cpp.",それぞれのメソッドは，指定された分布からのランダムな値で行列を埋めます．新しい数値が生成されると，それに応じて RNG の状態が更新されます．マルチチャンネル画像の場合，各チャンネルは独立に埋められます．つまり，RNGは，非対角共分散行列を持つ多次元ガウス分布から直接サンプルを生成することはできません．そのため，このメソッドは，平均が0で共分散行列が等比級数である多次元標準ガウス分布からサンプルを生成し，それをtransformを用いて変換することで，指定したガウス分布からサンプルを得ることができます．
Returns the next random number sampled from the Gaussian distribution.,ガウス分布からサンプリングされた次の乱数を返します。
sigma : standard deviation of the distribution.,sigma : 分布の標準偏差を指定します．
"The method transforms the state using the MWC algorithm and returns the next random number from the Gaussian distribution N(0,sigma) . That is, the mean value of the returned random numbers is zero and the standard deviation is the specified sigma .","このメソッドは，MWCアルゴリズムを用いて状態を変換し，ガウス分布N(0,sigma)から次の乱数を返します．つまり，返される乱数の平均値は0で，標準偏差は指定されたsigmaとなります．"
Singular Value Decomposition. ,特異値分解。
"Class for computing Singular Value Decomposition of a floating-point matrix. The Singular Value Decomposition is used to solve least-square problems, under-determined linear systems, invert matrices, compute condition numbers, and so on.",浮動小数点型の行列の特異値分解を計算するためのクラスです．特異値分解は，最小二乗問題，過小決定された連立方程式，行列の反転，条件数の計算などに利用されます．
"If you want to compute a condition number of a matrix or an absolute value of its determinant, you do not need u and vt. You can pass flags=SVD::NO_UV|... . Another flag SVD::FULL_UV indicates that full-size u and vt must be computed, which is not necessary most of the time.",行列の条件数や行列式の絶対値を計算したい場合は，u と vt は必要ありません．flags=SVD::NO_UV|...を渡すことができます．もう1つのフラグ SVD::FULL_UV は，フルサイズの u と vt を計算しなければならないことを示しますが，ほとんどの場合は必要ありません．
"See alsoinvert, solve, eigen, determinant ","関連項目： 反転, 解決, 固有値, 行列式"
performs a singular value back substitution.,特異値後退代入を行います。
"rhs : right-hand side of a linear system (u*w*v')*dst = rhs to be solved, where A has been previously decomposed.",rhs : 解くべき連立方程式の右辺 (u*w*v')*dst = rhs (Aは事前に分解されている).
dst : found solution of the system.,dst : 連立方程式の求解。
"The method calculates a back substitution for the specified right-hand side:\[\texttt{x} = \texttt{vt} ^T \cdot diag( \texttt{w} )^{-1} \cdot \texttt{u} ^T \cdot \texttt{rhs} \sim \texttt{A} ^{-1} \cdot \texttt{rhs}\]Using this technique you can either get a very accurate solution of the convenient linear system, or the best (in the least-squares terms) pseudo-solution of an overdetermined linear system.NoteExplicit SVD with the further back substitution only makes sense if you need to solve many linear systems with the same left-hand side (for example, src ). If all you need is to solve a single system (possibly with multiple rhs immediately available), simply call solve add pass DECOMP_SVD there. It does absolutely the same thing.","このメソッドは，指定された右辺の後退代入を計算します：\[texttt{x} = \tt{vt}.^T Đag( ˶ˆ꒳ˆ˵ )^{-1} 덴덴덴덴덴덴덴덴↩♪♪♪♪♪～\Smilax china (species of sarsaparilla)NoteExplicit SVD with further back substitution is only meaning that you need to solve many linear systems with the same left-hand side (for example, src ).1つの連立方程式（複数の rhs がすぐに得られる可能性がある）を解くだけならば，単に solve を呼び出して，DECOMP_SVD を渡してください．これは，まったく同じことを行います．"
decomposes matrix and stores the results to user-provided matrices,行列を分解し，その結果をユーザが提供する行列に格納します
src : decomposed matrix. The depth has to be CV_32F or CV_64F.,src : 分解された行列．深さは CV_32F または CV_64F でなければいけません．
w : calculated singular values,w : 計算された特異値
u : calculated left singular vectors,u : 求められた左特異点ベクトル
vt : transposed matrix of right singular vectors,vt : 右特異点ベクトルの転置行列．
flags : operation flags - see SVD::Flags.,flags : 処理フラグ - SVD::Flags を参照してください．
"The methods/functions perform SVD of matrix. Unlike SVD::SVD constructor and SVD::operator(), they store the results to the user-provided matrices:Mat A, w, u, vt;SVD::compute(A, w, u, vt);fragment",これらのメソッド/関数は，行列の SVD を行います．SVD::SVD コンストラクタや SVD::operator() とは異なり，ユーザが提供する行列に結果が格納されます．
"This is an overloaded member function, provided for convenience. It differs from the above function only in what argument(s) it accepts. computes singular values of a matrix",これは，利便性のために提供されるオーバーロードされたメンバ関数です．上の関数との違いは，どのような引数を受け取るかだけです． 行列の特異値を計算します．
performs back substitution,後方置換の実行
solves an under-determined singular linear system,決定されていない特異な連立方程式を解く
src : left-hand-side matrix.,src : 左辺の行列．
dst : found solution.,dst : 求められた解．
"The method finds a unit-length solution x of a singular linear system A*x = 0. Depending on the rank of A, there can be no solutions, a single solution or an infinite number of solutions. In general, the algorithm solves the following problem:\[dst = \arg \min _{x: \| x \| =1} \| src \cdot x \|\]","このメソッドは，特異連立方程式 A*x = 0 の単位長さの解 x を求めます．A の階数に応じて，解が存在しない場合，1つの解が存在する場合，無限の解が存在する場合があります．一般的に、このアルゴリズムは次のような問題を解きます：˶‾᷄ -̫ ‾᷅˵ ""dst = ˶‾᷅"" ""x: ˶‾᷄ -̫ ‾᷅˵"" ""src: ˶‾᷅"" ""x: ˶‾᷄ -̫ ‾᷄˵"
Linear Discriminant Analysis. ,Linear Discriminant Analysisの略。
Todo:document this class ,Todo:document this class
Serializes this object to a given filename.,このオブジェクトを指定されたファイル名にシリアル化します。
Deserializes this object from a given filename.,与えられたファイル名からこのオブジェクトをデシリアライズします。
Compute the discriminants for data in src (row aligned) and labels.,src (row aligned) と labels のデータの判別式を計算します．
Projects samples into the LDA subspace. src may be one or more row aligned samples.,src は，1つまたは複数の行整列されたサンプルです．
Reconstructs projections from the LDA subspace. src may be one or more row aligned projections.,LDA 部分空間から投影データを再構成します． src は，1つまたは複数の行がアラインメントされた投影データです．
Returns the eigenvectors of this LDA.,この LDA の固有ベクトルを返します．
Returns the eigenvalues of this LDA.,LDA の固有値を返します。
File Storage Node class. ,ファイルストレージノードクラス．
"The node is used to store each and every element of the file storage opened for reading. When XML/YAML file is read, it is first parsed and stored in the memory as a hierarchical collection of nodes. Each node can be a ""leaf"" that is contain a single number or a string, or be a collection of other nodes. There can be named collections (mappings) where each element has a name and it is accessed by a name, and ordered collections (sequences) where elements do not have names but rather accessed by index. Type of the file node can be determined using FileNode::type method.",このノードは，読み込みのために開かれたファイルストレージの各要素を格納するために使用されます．XML/YAMLファイルが読み込まれると，まずそれが解析され，ノードの階層的なコレクションとしてメモリに格納されます．各ノードは、1つの数値や文字列を含む「リーフ」であったり、他のノードのコレクションであったりします。各要素が名前を持ち、名前でアクセスされる名前付きコレクション（マッピング）と、要素が名前を持たずインデックスでアクセスされる順序付きコレクション（シーケンス）があります。ファイルノードのタイプは、FileNode::type メソッドを使用して決定できます。
"Note that file nodes are only used for navigating file storages opened for reading. When a file storage is opened for writing, no data is stored in memory after it is written. ",ファイルノードは、読み取り用にオープンされたファイルストレージをナビゲートするためにのみ使用されることに注意してください。ファイルノードは、読み込み用にオープンされたファイルストレージをナビゲートするためにのみ使用され、書き込み用にオープンされたファイルストレージでは、書き込んだ後にデータはメモリに保存されません。
Examples: samples/cpp/filestorage.cpp.,例：samples/cpp/filestorage.cpp.
Returns type of the node.,ノードのタイプを返します。
returns true if the node is empty,ノードが空の場合はtrueを返します。
"returns true if the node is a ""none"" object","ノードが ""none ""オブジェクトの場合はtrueを返します。"
returns true if the node is a sequence,ノードがシーケンスの場合はtrueを返します。
returns true if the node is a mapping,ノードがマッピングの場合はtrueを返します。
returns true if the node is an integer,ノードが整数の場合はtrueを返します。
returns true if the node is a floating-point number,ノードが浮動小数点数の場合はtrueを返します。
returns true if the node is a text string,ノードがテキスト文字列の場合はtrueを返します。
returns true if the node has a name,ノードに名前がある場合はtrueを返します。
returns the node name or an empty string if the node is nameless,ノードの名前を返します。ノードに名前がない場合は、空の文字列を返します。
"returns the number of elements in the node, if it is a sequence or mapping, or 1 otherwise.",ノードがシーケンスやマッピングの場合は要素数を，そうでない場合は1を返します。
returns iterator pointing to the first node element,最初のノード要素を指すイテレータを返します。
returns iterator pointing to the element following the last node element,最後のノード要素の次の要素を指し示すイテレータを返す
Reads node elements to the buffer with the specified format.,ノードの要素を指定されたフォーマットでバッファに読み込みます。
fmt : Specification of each array element. See format specification,fmt : 各配列要素の指定．フォーマット指定参照
vec : Pointer to the destination array.,vec :読み込み先の配列へのポインタ．
len : Number of bytes to read (buffer size limit). If it is greater than number of remaining elements then all of them will be read.,len : 読み込むバイト数（バッファサイズの制限）．len が残りの要素数よりも大きければ，すべての要素が読み込まれます．
Usually it is more convenient to use operator >> instead of this method.,通常は，この方法ではなく，演算子 >> を使った方が便利です．
used to iterate through sequences and mappings. ,シーケンスやマッピングの反復処理に使用されます。
"A standard STL notation, with node.begin(), node.end() denoting the beginning and the end of a sequence, stored in node. See the data reading sample in the beginning of the section. ",標準的なSTLの記法で、node.begin()、node.end()はシーケンスの始まりと終わりを表し、nodeに格納されます。冒頭のデータ読み込みサンプルをご覧ください。
XML/YAML/JSON file storage class that encapsulates all the information necessary for writing or reading data to/from a file. ,XML/YAML/JSON形式のファイル保存クラスで、ファイルへのデータの書き込みや読み出しに必要な情報をすべてカプセル化しています。
"Examples: samples/cpp/filestorage.cpp, samples/cpp/tutorial_code/features2D/Homography/decompose_homography.cpp, samples/cpp/tutorial_code/features2D/Homography/homography_from_camera_displacement.cpp, and samples/cpp/tutorial_code/features2D/Homography/pose_from_homography.cpp.",例：samples/cpp/filestorage.cpp、samples/cpp/tutorial_code/features2D/Homography/decompose_homography.cpp、samples/cpp/tutorial_code/features2D/Homography/homography_from_camera_displacement.cpp、samples/cpp/tutorial_code/features2D/Homography/pose_from_homography.cpp。
Opens a file.,ファイルを開きます。
"filename : Name of the file to open or the text string to read the data from. Extension of the file (.xml, .yml/.yaml or .json) determines its format (XML, YAML or JSON respectively). Also you can append .gz to work with compressed files, for example myHugeMatrix.xml.gz. If both FileStorage::WRITE and FileStorage::MEMORY flags are specified, source is used just to specify the output file format (e.g. mydata.xml, .yml etc.). A file name can also contain parameters. You can use this format, ""*?base64"" (e.g. ""file.json?base64"" (case sensitive)), as an alternative to FileStorage::BASE64 flag.","filename : 開くファイルの名前，またはデータを読み込むテキスト文字列．ファイルの拡張子（.xml、.yml/.yaml、.json）によって、そのフォーマット（それぞれXML、YAML、JSON）が決まります。また、myHugeMatrix.xml.gzのように、.gzを付けると圧縮ファイルを扱うことができます。FileStorage::WRITEとFileStorage::MEMORYの両方のフラグが指定されている場合、sourceは出力ファイルのフォーマットを指定するためだけに使用されます（例：mydata.xml、.ymlなど）。ファイル名にはパラメータを含めることもできます。FileStorage::BASE64フラグの代わりに、""*?base64""（例：""file.json?base64""（大文字小文字を区別する））というフォーマットを使うことができます。"
flags : Mode of operation. One of FileStorage::Mode,フラグ :操作のモード。FileStorage::Modeの一つです。
encoding : Encoding of the file. Note that UTF-16 XML encoding is not supported currently and you should use 8-bit encoding instead of it.,encoding : ファイルのエンコーディング。なお，現在，UTF-16のXMLエンコーディングはサポートされておらず，代わりに8ビットエンコーディングを利用する必要があります。
See description of parameters in FileStorage::FileStorage. The method calls FileStorage::release before opening the file.,FileStorage::FileStorageのパラメータの説明を参照してください。このメソッドは，ファイルをオープンする前にFileStorage::releaseを呼び出します。
Checks whether the file is opened.,ファイルが開かれているかどうかをチェックします。
Closes the file and releases all the memory buffers.,ファイルを閉じ、すべてのメモリバッファを解放します。
Call this method after all I/O operations with the storage are finished. If the storage was opened for writing data and FileStorage::WRITE was specifiedExamples: samples/cpp/filestorage.cpp.,このメソッドは、ストレージに対するすべての I/O 操作が終了した後に呼び出します。ストレージがデータ書き込み用にオープンされていて、FileStorage::WRITEが指定されている場合サンプル: samples/cpp/filestorage.cpp.
Returns the first element of the top-level mapping.,トップレベルのマッピングの最初の要素を返します。
Returns the top-level mapping.,トップレベルのマッピングを返します。
"streamidx : Zero-based index of the stream. In most cases there is only one stream in the file. However, YAML supports multiple streams and so there can be several.",streamidx : ストリームの0ベースのインデックスです。ほとんどの場合、ファイル内のストリームは1つだけです。しかし、YAMLは複数のストリームをサポートしているため、複数のストリームが存在することもあります。
Writes multiple numbers.,複数の数値を書き込みます。
"fmt : Specification of each array element, see format specification",fmt : 各配列要素の指定，フォーマット指定参照
vec : Pointer to the written array.,vec :書き込まれた配列へのポインタ．
len : Number of the uchar elements to write.,len : 書き込むユーカー要素の数．
Writes one or more numbers of the specified format to the currently written structure. Usually it is more convenient to use operator << instead of this method.,現在書き込まれている構造体に，指定された形式の1つまたは複数の数値を書き込みます。通常は，この方法ではなく，演算子 << を使った方が便利です。
Writes a comment.,コメントを書き込みます。
"comment : The written comment, single-line or multi-line",comment : 書き込まれたコメント（単一行または複数行）．
"append : If true, the function tries to put the comment at the end of current line. Else if the comment is multi-line, or if it does not fit at the end of the current line, the comment starts a new line.",append : true の場合，現在の行の最後にコメントを書こうとします。そうでない場合、コメントが複数行の場合、あるいはコメントが現在の行の最後に収まらない場合、コメントは新しい行を開始します。
The function writes a comment into file storage. The comments are skipped when the storage is read.,この関数は、ファイルストレージにコメントを書き込みます。ストレージが読み込まれると、コメントはスキップされます。
Returns the normalized object name for the specified name of a file.,指定されたファイル名に対する正規化されたオブジェクト名を返します。
filename : Name of a file,filename : ファイルの名前
Starts to write a nested structure (sequence or a mapping).,入れ子構造（シーケンスやマッピング）の書き込みを開始します。
"name : name of the structure. When writing to sequences (a.k.a. ""arrays""), pass an empty string.",name : 構造体の名前．シーケンス（別名「配列」）に書き込む場合は，空の文字列を渡します．
flags : type of the structure (FileNode::MAP or FileNode::SEQ (both with optional FileNode::FLOW)).,flags : 構造体のタイプ（FileNode::MAPまたはFileNode::SEQ（いずれもオプションでFileNode::FLOWあり））．
"typeName : optional name of the type you store. The effect of setting this depends on the storage format. I.e. if the format has a specification for storing type information, this parameter is used.",typeName : 保存する型の名前（オプション）．これを設定した場合の効果は、保存形式によって異なる。つまり、フォーマットに型情報を格納する仕様がある場合は、このパラメータが使用されます。
Finishes writing nested structure (should pair startWriteStruct()),ネストされた構造体の書き込みを終了します（startWriteStruct()とペアにする必要があります）。
This is the proxy class for passing read-only input arrays into OpenCV functions. ,これは，OpenCVの関数に読み取り専用の入力配列を渡すための，プロキシクラスです．
"It is defined as: typedef const _InputArray& InputArray; where _InputArray is a class that can be constructed from Mat, Mat_<T>, Matx<T, m, n>, std::vector<T>, std::vector<std::vector<T> >, std::vector<Mat>, std::vector<Mat_<T> >, UMat, std::vector<UMat> or double. It can also be constructed from a matrix expression.","これは次のように定義されます： typedef const _InputArray& InputArray; ここで _InputArray は， Mat, Mat_<T>, Matx<T, m, n>, std::vector<T>, std::vector<std::vector<T> >, std::vector<Mat>, std::vector<Mat_<T> >, UMat, std::vector<UMat> または double から構成されるクラスです．また，行列式から構築することもできます．"
"Since this is mostly implementation-level class, and its interface may change in future versions, we do not describe it in details. There are a few key things, though, that should be kept in mind:",これはほとんどが実装レベルのクラスであり，そのインターフェースは将来のバージョンで変更される可能性があるので，ここでは詳細な説明を省略します．しかし、いくつかの重要な点を覚えておく必要があります。
"When you see in the reference manual or in OpenCV source code a function that takes InputArray, it means that you can actually pass Mat, Matx, vector<T> etc. (see above the complete list).","リファレンスマニュアルや OpenCV のソースコードで，InputArray を受け取る関数を見かけることがありますが，これは，実際に Mat, Matx, vector<T> などを渡すことができることを意味します（上記の完全なリストを参照してください）．(完全なリストは上を参照してください）．"
"Optional input arguments: If some of the input arrays may be empty, pass cv::noArray() (or simply cv::Mat() as you probably did before).",オプションの入力引数．入力配列の一部が空の場合は， cv::noArray() を渡してください（あるいは，以前のように単に cv::Mat() を渡しても構いません）．
"The class is designed solely for passing parameters. That is, normally you should not declare class members, local and global variables of this type.",このクラスは，パラメータを渡すためだけに設計されています．つまり，通常は，クラスのメンバやローカル変数，グローバル変数をこの型で宣言してはいけません．
"If you want to design your own function or a class method that can operate of arrays of multiple types, you can use InputArray (or OutputArray) for the respective parameters. Inside a function you should use _InputArray::getMat() method to construct a matrix header for the array (without copying data). _InputArray::kind() can be used to distinguish Mat from vector<> etc., but normally it is not needed.",複数の型の配列を操作できる独自の関数やクラスメソッドを設計したい場合は，それぞれのパラメータに InputArray（または OutputArray）を利用します．関数内では _InputArray::getMat() メソッドを使用して、（データをコピーせずに）配列の行列ヘッダを作成する必要があります。_InputArray::kind() は、Mat と vector<> などを区別するために利用できますが、通常は必要ありません。
"Here is how you can use a function that takes InputArray : std::vector<Point2f> vec;// points or a circlefor( int i = 0; i < 30; i++ )    vec.push_back(Point2f((float)(100 + 30*cos(i*CV_PI*2/5)),                          (float)(100 - 30*sin(i*CV_PI*2/5))));cv::transform(vec, vec, cv::Matx23f(0.707, -0.707, 10, 0.707, 0.707, 20)); That is, we form an STL vector containing points, and apply in-place affine transformation to the vector using the 2x3 matrix created inline as Matx<float, 2, 3> instance.","ここでは，InputArray : std::vector<Point2f> vec;// 点または円for( int i = 0; i < 30; i++ ) vec.push_back(Point2f((float)(100 + 30*cos(i*CV_PI*2/5)), (float)(100 - 30*sin(i*CV_PI*2/5))));cv::transform(vec, vec, cv::Matx23f(0.707, -0.707, 10, 0.707, 0.707, 20)); つまり，点を含む STL ベクトルを作成し，Matx<float, 2, 3> インスタンスとしてインラインで作成された 2x3 行列を用いて，そのベクトルにインプレースのアフィン変換を行います．"
"Here is how such a function can be implemented (for simplicity, we implement a very specific case of it, according to the assertion statement inside) : void myAffineTransform(InputArray _src, OutputArray _dst, InputArray _m){    // get Mat headers for input arrays. This is O(1) operation,    // unless _src and/or _m are matrix expressions.    Mat src = _src.getMat(), m = _m.getMat();    CV_Assert( src.type() == CV_32FC2 && m.type() == CV_32F && m.size() == Size(3, 2) );    // [re]create the output array so that it has the proper size and type.    // In case of Mat it calls Mat::create, in case of STL vector it calls vector::resize.    _dst.create(src.size(), src.type());    Mat dst = _dst.getMat();    for( int i = 0; i < src.rows; i++ )        for( int j = 0; j < src.cols; j++ )        {            Point2f pt = src.at<Point2f>(i, j);            dst.at<Point2f>(i, j) = Point2f(m.at<float>(0, 0)*pt.x +                                            m.at<float>(0, 1)*pt.y +                                            m.at<float>(0, 2),                                            m.at<float>(1, 0)*pt.x +                                            m.at<float>(1, 1)*pt.y +                                            m.at<float>(1, 2));        }} There is another related type, InputArrayOfArrays, which is currently defined as a synonym for InputArray: typedef InputArray InputArrayOfArrays; It denotes function arguments that are either vectors of vectors or vectors of matrices. A separate synonym is needed to generate Python/Java etc. wrappers properly. At the function implementation level their use is similar, but _InputArray::getMat(idx) should be used to get header for the idx-th component of the outer vector and _InputArray::size().area() should be used to find the number of components (vectors/matrices) of the outer vector.","以下は，このような関数の実装方法です（簡単にするために，内部のアサーション文に従って，非常に特殊なケースを実装しています）： void myAffineTransform(InputArray _src, OutputArray _dst, InputArray _m){ // 入力配列に対する Mat ヘッダを取得します．これは， _src や _m が行列表現でない限り， // O(1) の処理です．    Mat src = _src.getMat(), m = _m.getMat(); CV_Assert( src.type() == CV_32FC2 && m.type() == CV_32F && m.size() == Size(3, 2) ); // 出力配列を適切なサイズと型になるように [再]作成します．    // Mat の場合は Mat::create を，STL vector の場合は vector::resize を呼び出します．    _dst.create(src.size(), src.type()); Mat dst = _dst.getMat(); for( int i = 0; i < src.rows; i++ ) for( int j = 0; j < src.cols; j++ ) { Point2f pt = src.at<Point2f>(i, j); dst.at<Point2f>(i, j) = Point2f(m.at<float>(0, 0)*pt.x + m.at<float>(0, 1)*pt.y + m.at<float>(0, 2), m.at<float>(1, 0)*pt.x + m.at<float>(1, 1)*pt.y + m.at<float>(1, 2)); }}。typedef InputArray InputArrayOfArrays; これは、ベクトルのベクトル、または行列のベクトルのいずれかである関数の引数を表します。Python/Javaなどのラッパーを適切に生成するためには、別のシノニムが必要です。関数の実装レベルでは、これらの使用方法は似ていますが、_InputArray::getMat(idx)は、外側のベクトルの idx 番目のコンポーネントのヘッダを取得するために使用し、_InputArray::size().area() は、外側のベクトルのコンポーネント（ベクトル/行列）の数を求めるために使用します。"
"In general, type support is limited to cv::Mat types. Other types are forbidden. But in some cases we need to support passing of custom non-general Mat types, like arrays of cv::KeyPoint, cv::DMatch, etc. This data is not intended to be interpreted as an image data, or processed somehow like regular cv::Mat. To pass such custom type use rawIn() / rawOut() / rawInOut() wrappers. Custom type is wrapped as Mat-compatible CV_8UC<N> values (N = sizeof(T), N <= CV_CN_MAX). ","一般に，サポートされる型は cv::Mat 型に限られます．それ以外の型は禁止されています．しかし，場合によっては， cv::KeyPoint や cv::DMatch などの配列のような，一般的ではないカスタムの Mat 型の受け渡しをサポートする必要があります．このデータは，画像データとして解釈されたり，通常の cv::Mat のように何らかの処理が行われたりすることはありません．このようなカスタムタイプを渡すには， rawIn() / rawOut() / rawInOut() ラッパーを利用してください．カスタムタイプは，Mat と互換性のある CV_8UC<N> 個の値（N = sizeof(T), N <= CV_CN_MAX）としてラッピングされます．"
"Examples: samples/cpp/pca.cpp, and samples/cpp/peopledetect.cpp.",例：samples/cpp/pca.cpp，samples/cpp/peopledetect.cpp．
Examples: samples/cpp/pca.cpp.,例：samples/cpp/pca.cpp．
n-dimensional dense array class ,n-dimensional dense array クラス
" The class Mat represents an n-dimensional dense numerical single-channel or multi-channel array. It can be used to store real or complex-valued vectors and matrices, grayscale or color images, voxel volumes, vector fields, point clouds, tensors, histograms (though, very high-dimensional histograms may be better stored in a SparseMat ). The data layout of the array M is defined by the array M.step[], so that the address of element \((i_0,...,i_{M.dims-1})\), where \(0\leq i_k<M.size[k]\), is computed as: "," クラス Mat は，シングルチャンネルまたはマルチチャンネルの，n-dimensional dense 数値配列を表します．これは，実数または複素数のベクトルや行列，グレースケールやカラーの画像，ボクセルボリューム，ベクトルフィールド，点群，テンソル，ヒストグラム（ただし，非常に高次元のヒストグラムは， SparseMat に格納した方が良いかもしれません）を格納するために利用できます．配列 M のデータレイアウトは，配列 M.step[] によって定義されており，要素 ˶((i_0,...,i_{M.dims-1})˶)のアドレスは，次のように計算されます： ˶((i_k<M.size[k]˶))"
"\[addr(M_{i_0,...,i_{M.dims-1}}) = M.data + M.step[0]*i_0 + M.step[1]*i_1 + ... + M.step[M.dims-1]*i_{M.dims-1}\]","\Addr(M_{i_0,...,i_{M.dims-1}}) = M.data + M.step[0]*i_0 + M.step[1]*i_1 + ...+ M.step[M.dims-1]*i_{M.dims-1}\]"
" In case of a 2-dimensional array, the above formula is reduced to: ", 2次元配列の場合，上の式は次のようになります．
"\[addr(M_{i,j}) = M.data + M.step[0]*i + M.step[1]*j\]","\Addr(M_{i,j}) = M.data + M.step[0]*i + M.step[1]*j\]」となります。"
" Note that M.step[i] >= M.step[i+1] (in fact, M.step[i] >= M.step[i+1]*M.size[i+1] ). This means that 2-dimensional matrices are stored row-by-row, 3-dimensional matrices are stored plane-by-plane, and so on. M.step[M.dims-1] is minimal and always equal to the element size M.elemSize() .", M.step[i] >= M.step[i+1] (実際には、M.step[i] >= M.step[i+1]*M.size[i+1] ) であることに注意してください。つまり，2次元の行列は行ごとに，3次元の行列は面ごとに，それぞれ格納されているということです．M.step[M.dims-1] は最小で，常に要素サイズ M.elemSize() と等しくなります．
"So, the data layout in Mat is compatible with the majority of dense array types from the standard toolkits and SDKs, such as Numpy (ndarray), Win32 (independent device bitmaps), and others, that is, with any array that uses steps (or strides) to compute the position of a pixel. Due to this compatibility, it is possible to make a Mat header for user-allocated data and process it in-place using OpenCV functions.",つまり，Mat のデータレイアウトは，Numpy (ndarray) や Win32 (independent device bitmaps) などの標準的なツールキットや SDK で提供されている密な配列タイプの大半，つまり，ピクセルの位置を計算するのにステップ（またはストライド）を利用するあらゆる配列と互換性があります．この互換性により，ユーザが割り当てたデータの Mat ヘッダを作成し，OpenCV の関数を使ってその場で処理することができます．
There are many different ways to create a Mat object. The most popular options are listed below:,Mat オブジェクトを作成するには，様々な方法があります．最も一般的な方法を以下に示します．
"Use the create(nrows, ncols, type) method or the similar Mat(nrows, ncols, type[, fillValue]) constructor. A new array of the specified size and type is allocated. type has the same meaning as in the cvCreateMat method. For example, CV_8UC1 means a 8-bit single-channel array, CV_32FC2 means a 2-channel (complex) floating-point array, and so on. // make a 7x7 complex matrix filled with 1+3j.Mat M(7,7,CV_32FC2,Scalar(1,3));// and now turn M to a 100x60 15-channel 8-bit matrix.// The old content will be deallocatedM.create(100,60,CV_8UC(15)); As noted in the introduction to this chapter, create() allocates only a new array when the shape or type of the current array are different from the specified ones.","create(nrows, ncols, type) メソッドや，類似の Mat(nrows, ncols, type[, fillValue]) コンストラクタを利用します．type は， cvCreateMat メソッドと同じ意味を持ちます．例えば， CV_8UC1 は 8 ビットシングルチャンネル配列を意味し， CV_32FC2 は 2 チャンネル（複素数）浮動小数点型配列を意味します．// 1+3 で埋め尽くされた 7x7 の複素行列を作成します。j.Mat M(7,7,CV_32FC2,Scalar(1,3));// そして今度は，M を 100x60 の 15 チャンネル 8 ビット行列に変えます。 // 古い内容は解放されます。M.create(100,60,CV_8UC(15)); 本章のイントロダクションで述べたように， create() は，現在の配列の形状や型が指定されたものと異なる場合にのみ，新しい配列を確保します．"
"Create a multi-dimensional array: // create a 100x100x100 8-bit arrayint sz[] = {100, 100, 100};Mat bigCube(3, sz, CV_8U, Scalar::all(0)); It passes the number of dimensions =1 to the Mat constructor but the created array will be 2-dimensional with the number of columns set to 1. So, Mat::dims is always >= 2 (can also be 0 when the array is empty).","多次元の配列を作成します。// 100x100x100 の 8 ビット配列を作成しますint sz[] = {100, 100, 100};Mat bigCube(3, sz, CV_8U, Scalar::all(0)); Mat のコンストラクタに次元数 =1 を渡していますが，作成される配列は列数が 1 の 2 次元になります．"
"Use a copy constructor or assignment operator where there can be an array or expression on the right side (see below). As noted in the introduction, the array assignment is an O(1) operation because it only copies the header and increases the reference counter. The Mat::clone() method can be used to get a full (deep) copy of the array when you need it.",右辺に配列や式が存在する可能性がある場合は，コピーコンストラクタや代入演算子を利用します（後述）．導入部で述べたように，配列の代入は，ヘッダのコピーと参照カウンタの増加だけなので，O(1) の処理です．Mat::clone() メソッドを利用すれば，必要な時に配列の完全な（深い）コピーを得ることができます．
"Construct a header for a part of another array. It can be a single row, single column, several rows, several columns, rectangular region in the array (called a minor in algebra) or a diagonal. Such operations are also O(1) because the new header references the same data. You can actually modify a part of the array using this feature, for example: // add the 5-th row, multiplied by 3 to the 3rd rowM.row(3) = M.row(3) + M.row(5)*3;// now copy the 7-th column to the 1-st column// M.col(1) = M.col(7); // this will not workMat M1 = M.col(1);M.col(7).copyTo(M1);// create a new 320x240 imageMat img(Size(320,240),CV_8UC3);// select a ROIMat roi(img, Rect(10,10,100,100));// fill the ROI with (0,255,0) (which is green in RGB space);// the original 320x240 image will be modifiedroi = Scalar(0,255,0); Due to the additional datastart and dataend members, it is possible to compute a relative sub-array position in the main container array using locateROI(): Mat A = Mat::eye(10, 10, CV_32S);// extracts A columns, 1 (inclusive) to 3 (exclusive).Mat B = A(Range::all(), Range(1, 3));// extracts B rows, 5 (inclusive) to 9 (exclusive).// that is, C \~ A(Range(5, 9), Range(1, 3))Mat C = B(Range(5, 9), Range::all());Size size; Point ofs;C.locateROI(size, ofs);// size will be (width=10,height=10) and the ofs will be (x=1, y=5) As in case of whole matrices, if you need a deep copy, use the clone() method of the extracted sub-matrices.","別の配列の一部に対するヘッダを作成します．それは，1 つの行，1 つの列，複数の行，複数の列，配列中の矩形領域（代数学ではマイナーと呼ばれます），または対角線のいずれかです．このような操作も，新しいヘッダが同じデータを参照するため，O(1)となります．この機能を使って，実際に配列の一部を変更することができます．例えば// 5 番目の行に 3 を掛けたものを 3 番目の行に追加するM.row(3) = M.row(3) + M.row(5)*3;// 7 番目の列を 1 番目の列にコピーする// M.col(1) = M.col(7); // これは動作しませんMat M1 = M.col(1);M.col(7).copyTo(M1);// 320x240 の新しい imageMat img(Size(320,240),CV_8UC3);// ROIMat roi(img, Rect(10,10,100,100));// ROI を (0,255,0) (RGB 空間では緑) で埋める．// オリジナルの 320x240 の画像が変更されます． roi = Scalar(0,255,0); 追加の datastart および dataend メンバにより， locateROI() を用いて，メインコンテナ配列内の相対的なサブアレイの位置を計算することができます．Mat A = Mat::eye(10, 10, CV_32S);// A の列，1 (including) から 3 (exclusive) を抽出します．Mat B = A(Range::all(), Range(1, 3));// B の行，5 (including) から 9 (exclusive) を抽出します．locateROI(size, ofs);// size は (width=10,height=10) となり，ofs は (x=1, y=5) 行列全体の場合と同様に，深いコピーが必要な場合は，抽出された部分行列の clone() メソッドを利用します．"
Make a header for user-allocated data. It can be useful to do the following:,ユーザが割り当てたデータのヘッダを作成します。以下のようにすると便利です．
"Process ""foreign"" data using OpenCV (for example, when you implement a DirectShow* filter or a processing module for gstreamer, and so on). For example: Mat process_video_frame(const unsigned char* pixels,                        int width, int height, int step){    // wrap input buffer    Mat img(height, width, CV_8UC3, (unsigned char*)pixels, step);    Mat result;    GaussianBlur(img, result, Size(7, 7), 1.5, 1.5);    return result;}","OpenCV を用いて，「外部」のデータを処理します（例えば，DirectShow* のフィルタや，gstreamer の処理モジュールを実装する場合など）．例えば，以下のようになります．Mat process_video_frame(const unsigned char* pixels, int width, int height, int step){ // 入力バッファをラップします． Mat img(height, width, CV_8UC3, (unsigned char*)pixels, step); Mat result; GaussianBlur(img, result, Size(7, 7), 1.5, 1.5); return result;}．"
"Quickly initialize small matrices and/or get a super-fast element access. double m[3][3] = {{a, b, c}, {d, e, f}, {g, h, i}};Mat M = Mat(3, 3, CV_64F, m).inv();","double m[3][3] = {{a, b, c}, {d, e, f}, {g, h, i}};Mat M = Mat(3, 3, CV_64F, m).inv();"
"Use MATLAB-style array initializers, zeros(), ones(), eye(), for example: // create a double-precision identity matrix and add it to M.M += Mat::eye(M.rows, M.cols, CV_64F);","MATLAB スタイルの配列初期化子，zeros(), ones(), eye() などを利用します．// 倍精度の単位行列を作成し，それを M.M += Mat::eye(M.rows, M.cols, CV_64F) に追加します．"
"Use a comma-separated initializer: // create a 3x3 double-precision identity matrixMat M = (Mat_<double>(3,3) << 1, 0, 0, 0, 1, 0, 0, 0, 1); With this approach, you first call a constructor of the Mat class with the proper parameters, and then you just put << operator followed by comma-separated values that can be constants, variables, expressions, and so on. Also, note the extra parentheses required to avoid compilation errors.","コンマで区切られたイニシャライザを利用します．// 3x3 の倍精度単位行列を作成します。Mat M = (Mat_<double>(3,3) << 1, 0, 0, 1, 0, 0, 1); この方法では，まず適切なパラメータを指定して Mat クラスのコンストラクタを呼び出し，次に << 演算子の後に定数，変数，式などのコンマで区切られた値を置くだけです．また，コンパイルエラーを避けるために，余分な括弧が必要であることにも注意してください．"
"Once the array is created, it is automatically managed via a reference-counting mechanism. If the array header is built on top of user-allocated data, you should handle the data by yourself. The array data is deallocated when no one points to it. If you want to release the data pointed by a array header before the array destructor is called, use Mat::release().",配列が作成されると、参照カウントの仕組みによって自動的に管理されます。配列のヘッダがユーザが割り当てたデータの上に構築されている場合は，そのデータを自分で処理する必要があります．配列データは、誰からも指されなくなると解放されます。配列デストラクタが呼ばれる前に，配列ヘッダが指し示すデータを解放したい場合は， Mat::release() を利用してください．
"The next important thing to learn about the array class is element access. This manual already described how to compute an address of each array element. Normally, you are not required to use the formula directly in the code. If you know the array element type (which can be retrieved using the method Mat::type() ), you can access the element \(M_{ij}\) of a 2-dimensional array as: M.at<double>(i,j) += 1.f; assuming that M is a double-precision floating-point array. There are several variants of the method at for a different number of dimensions.","配列クラスについて次に学ぶべき重要なことは，要素へのアクセスです．このマニュアルでは，各配列要素のアドレスを計算する方法を既に説明しました．通常は，コード内で直接その式を使う必要はありません．配列要素の型（メソッド Mat::type() を用いて取得できます）を知っていれば，2 次元配列の要素 \(M_{ij}\) に，次のようにアクセスできます．M.at<double>(i,j) += 1.f; M は倍精度浮動小数点型配列であると仮定します．M.at<double>(i,j) += 1.f; Mは倍精度の浮動小数点配列であると仮定しています。"
"If you need to process a whole row of a 2D array, the most efficient way is to get the pointer to the row first, and then just use the plain C operator [] : // compute sum of positive matrix elements// (assuming that M is a double-precision matrix)double sum=0;for(int i = 0; i < M.rows; i++){    const double* Mi = M.ptr<double>(i);    for(int j = 0; j < M.cols; j++)        sum += std::max(Mi[j], 0.);} Some operations, like the one above, do not actually depend on the array shape. They just process elements of an array one by one (or elements from multiple arrays that have the same coordinates, for example, array addition). Such operations are called element-wise. It makes sense to check whether all the input/output arrays are continuous, namely, have no gaps at the end of each row. If yes, process them as a long single row: // compute the sum of positive matrix elements, optimized variantdouble sum=0;int cols = M.cols, rows = M.rows;if(M.isContinuous()){    cols *= rows;    rows = 1;}for(int i = 0; i < rows; i++){    const double* Mi = M.ptr<double>(i);    for(int j = 0; j < cols; j++)        sum += std::max(Mi[j], 0.);} In case of the continuous matrix, the outer loop body is executed just once. So, the overhead is smaller, which is especially noticeable in case of small matrices.","2 次元配列の行全体を処理する必要がある場合，最も効率的な方法は，最初に行へのポインタを取得し，次に C の単純な演算子 [] を使用することです： // 正の行列要素の総和を求めます // （M が倍精度の行列であると仮定して）double sum=0;for(int i = 0; i < M. .rows; i++){ const double* Mi = M.ptr<double>(i); for(int j = 0; j < M.cols; j++) sum += std::max(Mi[j], 0.);}．上の例のように，実際には配列の形状に依存しない演算もあります．このような演算は，配列の要素を1つずつ処理するだけです（あるいは，配列の加算など，同じ座標を持つ複数の配列の要素を処理することもあります）．このような操作を「要素ワイズ」と呼びます。すべての入出力配列が連続しているかどうか，つまり，各行の終わりに隙間がないかどうかをチェックすることは意味があります．もしそうであれば，それらを長い1つの行として処理します．// 最適化された正の行列要素の和を求めるtdouble sum=0;int cols = M.cols, rows = M.rows;if(M.isContinuous()){ cols *= rows; rows = 1;}for(int i = 0; i < rows; i++){ const double* Mi = M.ptr<double>(i); for(int j = 0; j < cols; j++) sum += std::max(Mi[j], 0.);}．連続した行列の場合，外側のループ本体は1回しか実行されません．そのため，オーバーヘッドが小さくなり，特に小さな行列の場合には顕著です．"
"Finally, there are STL-style iterators that are smart enough to skip gaps between successive rows: // compute sum of positive matrix elements, iterator-based variantdouble sum=0;MatConstIterator_<double> it = M.begin<double>(), it_end = M.end<double>();for(; it != it_end; ++it)    sum += std::max(*it, 0.); The matrix iterators are random-access iterators, so they can be passed to any STL algorithm, including std::sort().","最後に，連続した行の間のギャップをスキップすることができる STL スタイルのイテレータを紹介します．// 正の行列要素の総和を計算する，イテレータベースのバリアントdouble sum=0;MatConstIterator_<double> it = M.begin<double>(), it_end = M.end<double>();for(; it != it_end; ++it) sum += std::max(*it, 0.); 行列イテレータは，ランダムアクセスイテレータなので，std::sort() を含む，あらゆる STL アルゴリズムに渡すことができます．"
NoteMatrix Expressions and arithmetic see MatExpr ,注釈matrix Expressions and arithmetic see MatExpr
"Examples: fld_lines.cpp, modules/shape/samples/shape_example.cpp, samples/cpp/camshiftdemo.cpp, samples/cpp/connected_components.cpp, samples/cpp/contours2.cpp, samples/cpp/convexhull.cpp, samples/cpp/cout_mat.cpp, samples/cpp/create_mask.cpp, samples/cpp/demhist.cpp, samples/cpp/distrans.cpp, samples/cpp/edge.cpp, samples/cpp/facedetect.cpp, samples/cpp/falsecolor.cpp, samples/cpp/ffilldemo.cpp, samples/cpp/filestorage.cpp, samples/cpp/fitellipse.cpp, samples/cpp/grabcut.cpp, samples/cpp/image_alignment.cpp, samples/cpp/intersectExample.cpp, samples/cpp/kalman.cpp, samples/cpp/kmeans.cpp, samples/cpp/laplace.cpp, samples/cpp/lkdemo.cpp, samples/cpp/minarea.cpp, samples/cpp/pca.cpp, samples/cpp/peopledetect.cpp, samples/cpp/polar_transforms.cpp, samples/cpp/segment_objects.cpp, samples/cpp/squares.cpp, samples/cpp/stitching.cpp, samples/cpp/stitching_detailed.cpp, samples/cpp/train_HOG.cpp, samples/cpp/tutorial_code/features2D/Homography/decompose_homography.cpp, samples/cpp/tutorial_code/features2D/Homography/homography_from_camera_displacement.cpp, samples/cpp/tutorial_code/features2D/Homography/pose_from_homography.cpp, samples/cpp/tutorial_code/HighGUI/AddingImagesTrackbar.cpp, samples/cpp/tutorial_code/Histograms_Matching/MatchTemplate_Demo.cpp, samples/cpp/tutorial_code/ImgProc/basic_drawing/Drawing_1.cpp, samples/cpp/tutorial_code/ImgProc/basic_drawing/Drawing_2.cpp, samples/cpp/tutorial_code/ImgProc/Morphology_1.cpp, samples/cpp/tutorial_code/ImgProc/Morphology_2.cpp, samples/cpp/tutorial_code/ImgProc/Pyramids/Pyramids.cpp, samples/cpp/tutorial_code/ImgProc/Smoothing/Smoothing.cpp, samples/cpp/tutorial_code/ImgTrans/copyMakeBorder_demo.cpp, samples/cpp/tutorial_code/ImgTrans/houghcircles.cpp, samples/cpp/tutorial_code/ImgTrans/houghlines.cpp, samples/cpp/tutorial_code/ImgTrans/Sobel_Demo.cpp, samples/cpp/tutorial_code/ml/introduction_to_pca/introduction_to_pca.cpp, samples/cpp/tutorial_code/photo/non_photorealistic_rendering/npr_demo.cpp, samples/cpp/tutorial_code/photo/seamless_cloning/cloning_demo.cpp, samples/cpp/tutorial_code/videoio/video-write/video-write.cpp, samples/cpp/videowriter_basic.cpp, samples/cpp/warpPerspective_demo.cpp, samples/cpp/watershed.cpp, samples/dnn/classification.cpp, samples/dnn/colorization.cpp, samples/dnn/object_detection.cpp, samples/dnn/openpose.cpp, samples/dnn/segmentation.cpp, samples/dnn/text_detection.cpp, and samples/tapi/squares.cpp.","例： fld_lines.cpp，modules/shape/samples/shape_example.cpp，samples/cpp/camshiftdemo.cpp，samples/cpp/connected_components.cpp，samples/cpp/contours2.cpp，samples/cpp/convexhull.cpp，samples/cpp/cout_mat.cpp，samples/cpp/create_mask.cpp，samples/cpp/demhist.cpp，samples/cpp/distrans.cpp, samples/cpp/edge.cpp, samples/cpp/facedetect.cpp, samples/cpp/falecolor.cpp, samples/cpp/ffilldemo.cpp, samples/cpp/filestorage.cpp, samples/cpp/fitellipse.cpp, samples/cpp/grabcut.cpp, samples/cpp/image_alignment.cpp, samples/cpp/intersectExample.cpp, samples/cpp/kalman.cpp, samples/cpp/kmeans.cpp，samples/cpp/laplace.cpp，samples/cpp/lkdemo.cpp，samples/cpp/minarea.cpp，samples/cpp/pca.cpp，samples/cpp/peopledetect.cpp，samples/cpp/polar_transforms.cpp，samples/cpp/segment_objects.cpp，samples/cpp/squares.cpp，samples/cpp/stitching.cpp，samples/cpp/stitching_detailed.cpp，samples/cpp/train_HOG.cpp, samples/cpp/tutorial_code/features2D/Homography/decompose_homography.cpp, samples/cpp/tutorial_code/features2D/Homography/homography_from_camera_displacement.cpp, samples/cpp/tutorial_code/features2D/Homography/pose_from_homography.cpp, samples/cpp/tutorial_code/HighGUI/AddingImagesTrackbar.cpp, samples/cpp/tutorial_code/Histograms_Matching/MatchTemplate_Demo.cpp, samples/cpp/tutorial_code/ImgProc/basic_drawing/Drawing_1.cpp, samples/cpp/tutorial_code/ImgProc/basic_drawing/Drawing_2.cpp, samples/cpp/tutorial_code/ImgProc/Morphology_1.cpp, samples/cpp/tutorial_code/ImgProc/Morphology_2.cpp, samples/cpp/tutorial_code/ImgProc/Pyramids/Pyramids.cpp, samples/cpp/tutorial_code/ImgProc/Smoothing/Smoothing.cpp, samples/cpp/tutorial_code/ImgTrans/copyMakeBorder_demo.cpp, samples/cpp/tutorial_code/ImgTrans/houghcircles.cpp, samples/cpp/tutorial_code/ImgTrans/houghlines.cpp, samples/cpp/tutorial_code/ImgTrans/Sobel_Demo.cpp, samples/cpp/tutorial_code/ml/introduction_to_pca/introduction_to_pca.cpp, samples/cpp/tutorial_code/photo/non_photorealistic_rendering/npr_demo.cpp, samples/cpp/tutorial_code/photo/seamless_cloning/cloning_demo.cpp, samples/cpp/tutorial_code/videoio/video-write/video-write.cpp, samples/cpp/videowriter_basic.cpp, samples/cpp/warpPerspective_demo.cpp, samples/cpp/watershed.cpp, samples/dnn/classification.cpp, samples/dnn/colorization.cpp, samples/dnn/object_detection.cpp, samples/dnn/openpose.cpp, samples/dnn/segmentation.cpp, samples/dnn/text_detection.cpp, and samples/tapi/squares.cpp."
retrieve UMat from Mat,Mat から UMat を取得します．
Creates a matrix header for the specified matrix row.,指定された行列行に対する行列ヘッダを作成します．
y : A 0-based row index.,y :0 基準の行インデックス．
"The method makes a new header for the specified matrix row and returns it. This is an O(1) operation, regardless of the matrix size. The underlying data of the new matrix is shared with the original matrix. Here is the example of one of the classical basic matrix processing operations, axpy, used by LU and many other algorithms:inline void matrix_axpy(Mat& A, int i, int j, double alpha){    A.row(i) += A.row(j)*alpha;}fragmentNoteIn the current implementation, the following code does not work as expected: Mat A;...A.row(i) = A.row(j); // will not work This happens because A.row(i) forms a temporary header that is further assigned to another header. Remember that each of these operations is O(1), that is, no data is copied. Thus, the above assignment is not true if you may have expected the j-th row to be copied to the i-th row. To achieve that, you should either turn this simple assignment into an expression or use the Mat::copyTo method: Mat A;...// works, but looks a bit obscure.A.row(i) = A.row(j) + 0;// this is a bit longer, but the recommended method.A.row(j).copyTo(A.row(i));Examples: samples/cpp/pca.cpp, and samples/cpp/train_HOG.cpp.","このメソッドは，指定された行列の行に対する新しいヘッダを作成し，それを返します．これは，行列のサイズに関わらず，O(1) の処理です．新しい行列の基礎データは，元の行列と共有されます．以下は，LU やその他多くのアルゴリズムで利用される，古典的な基本行列処理の 1 つである axpy の例です：inline void matrix_axpy(Mat& A, int i, int j, double alpha){ A.row(i) += A.row(j)*alpha;}fragmentNote現在の実装では，以下のコードは期待通りに動作しません．Mat A;...A.row(i) = A.row(j); // うまくいきません． これは，A.row(i) が一時的なヘッダを形成し，さらに別のヘッダに割り当てられるために起こります．これらの操作はそれぞれO(1)、つまりデータはコピーされないことを覚えておいてください。したがって，j番目の行がi番目の行にコピーされることを期待していたかもしれない場合には，上記の割り当ては真ではありません．これを実現するには，この単純な代入を式に変えるか， Mat::copyTo メソッドを利用する必要があります．A.row(j) = A.row(j) + 0;// これは少し長いですが，推奨される方法です．A.row(j).copyTo(A.row(i));例： samples/cpp/pca.cpp，samples/cpp/train_HOG.cpp．"
Creates a matrix header for the specified matrix column.,指定された行列列の行列ヘッダを作成します。
x : A 0-based column index.,x : 0ベースの列インデックス．
"The method makes a new header for the specified matrix column and returns it. This is an O(1) operation, regardless of the matrix size. The underlying data of the new matrix is shared with the original matrix. See also the Mat::row description.Examples: samples/cpp/tutorial_code/features2D/Homography/pose_from_homography.cpp.",このメソッドは，指定された行列列に対する新しいヘッダを作成し，それを返します．これは，行列のサイズに関わらず，O(1)の処理です．この新しい行列の基礎データは，元の行列と共有されます．Mat::row の説明も参照してください．例： samples/cpp/tutorial_code/features2D/Homography/pose_from_homography.cpp.
Creates a matrix header for the specified row span.,指定された行範囲に対する行列ヘッダを作成します．
startrow : An inclusive 0-based start index of the row span.,startrow : 0を基準とした，行スパンの開始インデックス．
endrow : An exclusive 0-based ending index of the row span.,endrow : 排他的な，0を基準とした行範囲の終了インデックス．
"The method makes a new header for the specified row span of the matrix. Similarly to Mat::row and Mat::col , this is an O(1) operation.Examples: samples/cpp/kmeans.cpp, and samples/dnn/segmentation.cpp.",このメソッドは，行列の指定された行範囲に対して新しいヘッダを作成します．Mat::row や Mat::col と同様に，これは O(1) の処理です．例： samples/cpp/kmeans.cpp，samples/dnn/segmentation.cpp．
Creates a matrix header for the specified column span.,指定された列数に対する行列ヘッダを作成します．
startcol : An inclusive 0-based start index of the column span.,startcol : 0 を基準とした，列スパンの開始インデックスを含みます．
endcol : An exclusive 0-based ending index of the column span.,endcol : カラムスパンの，0を基準とした終了インデックス（排他的）．
"The method makes a new header for the specified column span of the matrix. Similarly to Mat::row and Mat::col , this is an O(1) operation.",このメソッドは，行列の指定された列スパンに対して新しいヘッダを作成します．Mat::row や Mat::col と同様に，これは O(1) の処理です．
Extracts a diagonal from a matrix.,行列から対角線を抽出します．
"d : index of the diagonal, with the following values:",d : 対角線のインデックス，以下の値をとります．
d=0 is the main diagonal.,d=0 は，主対角線です．
"d<0 is a diagonal from the lower half. For example, d=-1 means the diagonal is set immediately below the main one.",d=0 は主対角線，d<0 は下半分の対角線を表す．例えば，d=-1 は，主対角線のすぐ下に対角線が設定されていることを意味する．
"d>0 is a diagonal from the upper half. For example, d=1 means the diagonal is set immediately above the main one. For example: Mat m = (Mat_<int>(3,3) <<            1,2,3,            4,5,6,            7,8,9);Mat d0 = m.diag(0);Mat d1 = m.diag(1);Mat d_1 = m.diag(-1); The resulting matrices are d0 =  [1;   5;   9]d1 =  [2;   6]d_1 =  [4;   8]","d>0は、上半分からの対角線である。例えば、d=1は対角線が主対角線のすぐ上に設定されていることを意味します。例えば，以下のようになります．Mat m = (Mat_<int>(3,3) << 1,2,3, 4,5,6, 7,8,9);Mat d0 = m.diag(0);Mat d1 = m.diag(1);Mat d_1 = m.diag(-1); 結果として得られる行列は，d0 = [1; 5; 9]d1 = [2; 6]d_1 = [4; 8] です．"
"The method makes a new header for the specified matrix diagonal. The new matrix is represented as a single-column matrix. Similarly to Mat::row and Mat::col, this is an O(1) operation.",このメソッドは，指定された行列の対角線上に新しいヘッダを作成します．この新しい行列は，1 列の行列として表現されます．Mat::row や Mat::col と同様に，これは O(1) の処理です．
Creates a full copy of the array and the underlying data.,配列とその基礎となるデータの完全なコピーを作成します．
"The method creates a full copy of the array. The original step[] is not taken into account. So, the array copy is a continuous array occupying total()*elemSize() bytes.Examples: samples/cpp/create_mask.cpp, samples/cpp/facedetect.cpp, samples/cpp/stitching_detailed.cpp, samples/cpp/train_HOG.cpp, samples/cpp/tutorial_code/features2D/Homography/homography_from_camera_displacement.cpp, samples/cpp/tutorial_code/features2D/Homography/pose_from_homography.cpp, samples/cpp/tutorial_code/ImgProc/Smoothing/Smoothing.cpp, samples/cpp/tutorial_code/ImgTrans/houghlines.cpp, and samples/cpp/warpPerspective_demo.cpp.","このメソッドは，配列の完全なコピーを作成します．元の step[] は考慮されません．したがって，配列のコピーは，total()*elemSize() バイトを占める連続した配列となります．例： samples/cpp/create_mask.cpp，samples/cpp/facedetect.cpp，samples/cpp/stitching_detailed.cpp，samples/cpp/train_HOG.cpp，samples/cpp/tutorial_code/features2D/Homography/homography_from_camera_displacement.cpp, samples/cpp/tutorial_code/features2D/Homography/pose_from_homography.cpp, samples/cpp/tutorial_code/ImgProc/Smoothing/Smoothing.cpp, samples/cpp/tutorial_code/ImgTrans/houghlines.cpp, and samples/cpp/warpPerspective_demo.cpp."
Copies the matrix to another one.,行列を別の行列にコピーします。
"m : Destination matrix. If it does not have a proper size or type before the operation, it is reallocated.",m : コピー先の行列．操作前に適切なサイズや型を持っていない場合は，再割り当てされます．
"The method copies the matrix data to another matrix. Before copying the data, the method invokes :m.create(this->size(), this->type());fragmentso that the destination matrix is reallocated if needed. While m.copyTo(m); works flawlessly, the function does not handle the case of a partial overlap between the source and the destination matrices.When the operation mask is specified, if the Mat::create call shown above reallocates the matrix, the newly allocated matrix is initialized with all zeros before copying the data.Examples: samples/cpp/camshiftdemo.cpp, samples/cpp/edge.cpp, samples/cpp/grabcut.cpp, samples/cpp/image_alignment.cpp, samples/cpp/lkdemo.cpp, samples/cpp/tutorial_code/Histograms_Matching/MatchTemplate_Demo.cpp, samples/cpp/watershed.cpp, samples/tapi/hog.cpp, and samples/tapi/squares.cpp.","このメソッドは，行列のデータを別の行列にコピーします．データをコピーする前に，このメソッドは :m.create(this->size(), this->type()) を呼び出し，必要に応じてコピー先の行列が再割り当てされるようにします．操作マスクが指定されている場合，上述の Mat::create 呼び出しで行列が再割り当てされると，データをコピーする前に，新しく割り当てられた行列がすべて 0 で初期化されます．例： samples/cpp/camshiftdemo.cpp，samples/cpp/edge.cpp，samples/cpp/grabcut.cpp，samples/cpp/image_alignment.cpp，samples/cpp/lkdemo.cpp，samples/cpp/tutorial_code/Histograms_Matching/MatchTemplate_Demo.cpp，samples/cpp/watershed.cpp，samples/tapi/hog.cpp，samples/tapi/squares.cpp．"
Converts an array to another data type with optional scaling.,配列を別のデータ型に変換し，オプションでスケーリングを行います．
"m : output matrix; if it does not have a proper size or type before the operation, it is reallocated.",m : 出力行列．操作前に適切なサイズや型でなかった場合は，再割り当てされます．
"rtype : desired output matrix type or, rather, the depth since the number of channels are the same as the input has; if rtype is negative, the output matrix will have the same type as the input.",rtype : 希望する出力行列の型，というよりも，チャンネル数が入力と同じであることから得られる深さ．rtype が負の値の場合，出力行列は入力と同じ型になります．
"The method converts source pixel values to the target data type. saturate_cast<> is applied at the end to avoid possible overflows:\[m(x,y) = saturate \_ cast<rType>( \alpha (*this)(x,y) + \beta )\]Examples: samples/cpp/demhist.cpp, samples/cpp/distrans.cpp, samples/cpp/fitellipse.cpp, samples/cpp/pca.cpp, samples/cpp/stitching_detailed.cpp, and samples/dnn/colorization.cpp.","このメソッドは，ソースピクセルの値をターゲットデータ型に変換します．オーバーフローの可能性を回避するために，最後に saturate_cast<> が適用されます：[m(x,y) = saturate ″cast<rType>( ″alpha (*this)(x,y) + ″beta″ )]例： samples/cpp/demhist.cpp、samples/cpp/distrans.cpp、samples/cpp/fitellipse.cpp、samples/cpp/pca.cpp、samples/cpp/stitching_detailed.cpp、samples/dnn/colorization.cppです。"
Provides a functional form of convertTo.,convertTo の関数形式を提供します。
m : Destination array.,m : 送信先の配列。
type : Desired destination array depth (or -1 if it should be the same as the source type).,type :目的とする出力配列の深さ（ソースの型と同じにする場合は-1）．
This is an internally used method called by the MatrixExpressions engine.,これは，MatrixExpressions エンジンによって呼び出される，内部的に使用されるメソッドです．
Sets all or some of the array elements to the specified value.,配列の全てまたは一部の要素を指定された値に設定します。
value : Assigned scalar converted to the actual array type.,value : 割り当てられたスカラーを実際の配列型に変換したもの。
mask : Operation mask of the same size as *this. Its non-zero elements indicate which matrix elements need to be copied. The mask has to be of type CV_8U and can have 1 or multiple channels,mask : *this と同じサイズのオペレーションマスク．その非0の要素は，どの行列要素をコピーする必要があるかを示します．このマスクは CV_8U 型でなければならず，1 つまたは複数のチャンネルを持つことができます．
"This is an advanced variant of the Mat::operator=(const Scalar& s) operator.Examples: samples/cpp/stitching_detailed.cpp, and samples/dnn/segmentation.cpp.",これは， Mat::operator=(const Scalar& s) 演算子の発展型です．例： samples/cpp/stitching_detailed.cpp ，samples/dnn/segmentation.cpp など．
Changes the shape and/or the number of channels of a 2D matrix without copying the data.,データをコピーすることなく，2 次元行列の形状やチャンネル数を変更します．
"cn : New number of channels. If the parameter is 0, the number of channels remains the same.",cn : 新しいチャンネル数。パラメータが0の場合は，チャンネル数はそのままです．
"rows : New number of rows. If the parameter is 0, the number of rows remains the same.",rows :新しい行数を指定します。このパラメータが 0 の場合，行数はそのままです．
"The method makes a new matrix header for *this elements. The new matrix may have a different size and/or different number of channels. Any combination is possible if:No extra elements are included into the new matrix and no elements are excluded. Consequently, the product rows*cols*channels() must stay the same after the transformation.",このメソッドは，*this の要素に対する新しい行列のヘッダを作成します．この新しい行列は，サイズやチャンネル数が異なっていても構いません．以下の条件であれば，どのような組み合わせも可能です：新しい行列に余分な要素が含まれておらず，また要素が除外されていない場合．したがって，行*列*チャネル() の積は，変換後も同じでなければいけません．
"No data is copied. That is, this is an O(1) operation. Consequently, if you change the number of rows, or the operation changes the indices of elements row in some other way, the matrix must be continuous. See Mat::isContinuous .For example, if there is a set of 3D points stored as an STL vector, and you want to represent the points as a 3xN matrix, do the following:std::vector<Point3f> vec;...Mat pointMat = Mat(vec). // convert vector to Mat, O(1) operation                  reshape(1). // make Nx3 1-channel matrix out of Nx1 3-channel.                              // Also, an O(1) operation                     t(); // finally, transpose the Nx3 matrix.                          // This involves copying all the elementsfragmentExamples: samples/cpp/pca.cpp, and samples/dnn/classification.cpp.","データはコピーされません。つまり，これは O(1) の演算です．したがって，行数を変更したり，その他の方法で行の要素のインデックスを変更したりした場合，その行列は連続したものでなければいけません．Mat::isContinuous を参照してください．例えば，STL のベクトルとして格納された 3D ポイントの集合があり，そのポイントを 3xN の行列として表現したい場合，次のようにします： std::vector<Point3f> vec;...Mat pointMat = Mat(vec)．// ベクトルを Mat に変換，O(1) の演算 reshape(1)．// Nx1 の 3 チャンネル行列から，Nx3 の 1 チャンネル行列を作成します．                              // これも O(1) の処理です． t(); // 最後に，Nx3 の行列を転置します．                          これは， // すべての要素をコピーすることになりますfragmentExamples: samples/cpp/pca.cpp, samples/dnn/classification.cpp."
"The method performs matrix transposition by means of matrix expressions. It does not perform the actual transposition but returns a temporary matrix transposition object that can be further used as a part of more complex matrix expressions or can be assigned to a matrix:Mat A1 = A + Mat::eye(A.size(), A.type())*lambda;Mat C = A1.t()*A1; // compute (A + lambda*I)^t * (A + lamda*I)fragmentExamples: samples/cpp/tutorial_code/features2D/Homography/decompose_homography.cpp, and samples/cpp/tutorial_code/features2D/Homography/homography_from_camera_displacement.cpp.","このメソッドは，行列式を用いて行列の転置を行います．このオブジェクトは，より複雑な行列式の一部として利用したり，行列に割り当てたりすることができます： Mat A1 = A + Mat::eye(A.size(), A.type())*lambda;Mat C = A1.t()*A1; // (A + lambda*I)^t * (A + lambda*I)fragmentExamples: samples/cpp/tutorial_code/features2D/Homography/decompose_homography.cpp, and samples/cpp/tutorial_code/features2D/Homography/homography_from_camera_displacement.cpp."
Inverses a matrix.,行列の逆行列を作成します。
method : Matrix inversion method. One of cv::DecompTypes,method : 行列逆転法．cv::DecompTypesの1つ．
"The method performs a matrix inversion by means of matrix expressions. This means that a temporary matrix inversion object is returned by the method and can be used further as a part of more complex matrix expressions or can be assigned to a matrix.Examples: samples/cpp/tutorial_code/features2D/Homography/decompose_homography.cpp, and samples/cpp/tutorial_code/features2D/Homography/homography_from_camera_displacement.cpp.","このメソッドは，行列式を用いて行列の反転を行います．つまり，一時的な行列反転オブジェクトが返され，より複雑な行列式の一部として利用したり，行列に代入したりすることができます．例： samples/cpp/tutorial_code/features2D/Homography/decompose_homography.cpp, samples/cpp/tutorial_code/features2D/Homography/homography_from_camera_displacement.cpp."
Performs an element-wise multiplication or division of the two matrices.,2つの行列の要素ごとの乗算・除算を行います．
"m : Another array of the same type and the same size as *this, or a matrix expression.",m : *this と同じ型，同じサイズの別の配列，あるいは，行列式．
scale : Optional scale factor.,scale : オプションのスケールファクター．
"The method returns a temporary object encoding per-element array multiplication, with optional scale. Note that this is not a matrix multiplication that corresponds to a simpler ""\*"" operator.Example:Mat C = A.mul(5/B); // equivalent to divide(A, B, C, 5)fragment","このメソッドは，要素毎の配列の乗算を，オプションの scale を用いてエンコードした一時的なオブジェクトを返します．例： Mat C = A.mul(5/B); // divide(A, B, C, 5)と等価なフラグメンテーション．"
Computes a cross-product of two 3-element vectors.,2 つの 3 要素のベクトルの外積を計算します．
m : Another cross-product operand.,m : もう1つの外積演算子．
The method computes a cross-product of two 3-element vectors. The vectors must be 3-element floating-point vectors of the same shape and size. The result is another 3-element vector of the same shape and type as operands.Examples: samples/cpp/tutorial_code/features2D/Homography/pose_from_homography.cpp.,このメソッドは，2 つの 3 要素のベクトルの外積を計算します。ベクトルは，同じ形とサイズの 3 要素浮動小数点ベクトルでなければなりません。例： samples/cpp/tutorial_code/features2D/Homography/pose_from_homography.cpp.
Computes a dot-product of two vectors.,2つのベクトルのドット積を計算します。
m : another dot-product operand.,m : もう1つのドット・プロダクツ・オペランド。
"The method computes a dot-product of two matrices. If the matrices are not single-column or single-row vectors, the top-to-bottom left-to-right scan ordering is used to treat them as 1D vectors. The vectors must have the same size and type. If the matrices have more than one channel, the dot products from all the channels are summed together.Examples: samples/cpp/tutorial_code/features2D/Homography/decompose_homography.cpp, and samples/cpp/tutorial_code/features2D/Homography/homography_from_camera_displacement.cpp.","このメソッドは，2つの行列のドット積を計算します．行列が1列または1行のベクトルではない場合，上から下，左から右への走査順序が用いられ，1次元ベクトルとして扱われます。ベクトルのサイズとタイプは同じでなければなりません。例: samples/cpp/tutorial_code/features2D/Homography/decompose_homography.cpp, and samples/cpp/tutorial_code/features2D/Homography/homography_from_camera_displacement.cpp."
Returns a zero array of the specified size and type.,指定されたサイズとタイプの0個の配列を返します。
rows : Number of rows.,rows :rows : 行の数．
cols : Number of columns.,cols : 柱の数．
type : Created matrix type.,type :作成された行列の型．
"The method returns a Matlab-style zero array initializer. It can be used to quickly form a constant array as a function parameter, part of a matrix expression, or as a matrix initializer:Mat A;A = Mat::zeros(3, 3, CV_32F);fragmentIn the example above, a new matrix is allocated only if A is not a 3x3 floating-point matrix. Otherwise, the existing matrix A is filled with zeros.Examples: samples/cpp/camshiftdemo.cpp, samples/cpp/contours2.cpp, samples/cpp/falsecolor.cpp, samples/cpp/fitellipse.cpp, samples/cpp/kalman.cpp, samples/cpp/tutorial_code/ImgProc/basic_drawing/Drawing_1.cpp, samples/cpp/tutorial_code/ImgProc/basic_drawing/Drawing_2.cpp, and samples/dnn/segmentation.cpp.","このメソッドは，Matlab 形式のゼロ配列イニシャライザを返します．これは，関数のパラメータ，行列式の一部，あるいは行列のイニシャライザとして，定数配列を高速に形成するために利用できます： Mat A;A = Mat::zeros(3, 3, CV_32F);fragment上記の例では，A が 3x3 の浮動小数点型行列ではない場合にのみ，新しい行列が確保されます．例： samples/cpp/camshiftdemo.cpp, samples/cpp/contours2.cpp, samples/cpp/falecolor.cpp, samples/cpp/fitellipse.cpp, samples/cpp/kalman.cpp, samples/cpp/tutorial_code/ImgProc/basic_drawing/Drawing_1.cpp, samples/cpp/tutorial_code/ImgProc/basic_drawing/Drawing_2.cpp, and samples/dnn/segmentation.cpp."
Returns an array of all 1's of the specified size and type.,指定されたサイズと型のすべての 1 の配列を返します。
"The method returns a Matlab-style 1's array initializer, similarly to Mat::zeros. Note that using this method you can initialize an array with an arbitrary value, using the following Matlab idiom:Mat A = Mat::ones(100, 100, CV_8U)*3; // make 100x100 matrix filled with 3.fragmentThe above operation does not form a 100x100 matrix of 1's and then multiply it by 3. Instead, it just remembers the scale factor (3 in this case) and use it when actually invoking the matrix initializer.NoteIn case of multi-channels type, only the first channel will be initialized with 1's, the others will be set to 0's.Examples: samples/cpp/demhist.cpp.","このメソッドは， Mat::zeros と同様に，Matlab 形式の 1 の配列イニシャライザを返します．このメソッドを利用すると，次のような Matlab イディオムを用いて，任意の値で配列を初期化できることに注意してください： Mat A = Mat::ones(100, 100, CV_8U)*3; // 3.fragment で埋め尽くされた 100x100 の行列を作成します．上記の処理では，1 の 100x100 の行列を作成し，それに 3 を掛けることはありません．注意マルチチャンネルの場合，最初のチャンネルだけが 1 で初期化され，他のチャンネルは 0 になります．例： samples/cpp/demhist.cpp."
Returns an identity matrix of the specified size and type.,指定されたサイズと型の単位行列を返します．
"The method returns a Matlab-style identity matrix initializer, similarly to Mat::zeros. Similarly to Mat::ones, you can use a scale operation to create a scaled identity matrix efficiently:// make a 4x4 diagonal matrix with 0.1's on the diagonal.Mat A = Mat::eye(4, 4, CV_32F)*0.1;fragmentNoteIn case of multi-channels type, identity matrix will be initialized only for the first channel, the others will be set to 0'sExamples: samples/cpp/image_alignment.cpp.","このメソッドは，Mat::zeros と同様に，Matlab 形式の単位行列イニシャライザを返します．Mat::ones と同様に，スケール操作を利用して，スケーリングされた単位行列を効率的に作成することができます： // 対角線上に 0.1 を配置した 4x4 の対角行列を作成します．Mat A = Mat::eye(4, 4, CV_32F)*0.1;fragmentNoteマルチチャンネルタイプの場合，単位行列は最初のチャンネルに対してのみ初期化され，その他のチャンネルは 0's にセットされます．"
Allocates new array data if needed.,必要に応じて，新しい配列データを確保します．
rows : New number of rows.,rows :行数を指定します。
cols : New number of columns.,cols : 列数を指定します。
type : New matrix type.,type :新しい行列の型．
"This is one of the key Mat methods. Most new-style OpenCV functions and methods that produce arrays call this method for each output array. The method uses the following algorithm:If the current array shape and the type match the new ones, return immediately. Otherwise, de-reference the previous data by calling Mat::release.",これは，Mat の重要なメソッドの 1 つです．配列を生成する新スタイルの OpenCV 関数やメソッドのほとんどは，各出力配列に対してこのメソッドを呼び出します．このメソッドは，次のようなアルゴリズムを用います：現在の配列の形状と型が新しいものと一致する場合は，直ちに戻ります．そうでない場合は， Mat::release を呼び出して以前のデータの参照を解除します．
Initialize the new header.,新しいヘッダを初期化します．
Allocate the new data of total()*elemSize() bytes.,total()*elemSize() バイトの新しいデータを確保します．
"Allocate the new, associated with the data, reference counter and set it to 1.Such a scheme makes the memory management robust and efficient at the same time and helps avoid extra typing for you. This means that usually there is no need to explicitly allocate output arrays. That is, instead of writing:Mat color;...Mat gray(color.rows, color.cols, color.depth());cvtColor(color, gray, COLOR_BGR2GRAY);fragmentyou can simply write:Mat color;...Mat gray;cvtColor(color, gray, COLOR_BGR2GRAY);fragmentbecause cvtColor, as well as the most of OpenCV functions, calls Mat::create() for the output array internally.Examples: samples/cpp/camshiftdemo.cpp, samples/cpp/edge.cpp, samples/cpp/ffilldemo.cpp, samples/cpp/grabcut.cpp, samples/cpp/stitching_detailed.cpp, samples/cpp/tutorial_code/Histograms_Matching/MatchTemplate_Demo.cpp, and samples/dnn/segmentation.cpp.","データに関連付けられた新しい参照カウンタを確保し，それを 1 にセットします．このような方式は，メモリ管理を頑健かつ効率的にすると同時に，余計なタイプミスを防ぐのに役立ちます．つまり，通常，出力配列を明示的に確保する必要はありません。つまり，次のように書く代わりに，:Mat color;...Mat gray(color.rows, color.cols, color.depth());cvtColor(color, gray, COLOR_BGR2GRAY);fragmentty 単に次のように書くことができます：Mat color;...Mat gray;cvtColor(color, gray, COLOR_BGR2GRAY);fragmentなぜならば，cvtColor は，OpenCV のほとんどの関数と同様に，出力配列に対して Mat::create() を内部的に呼び出しているからです．例： samples/cpp/camshiftdemo.cpp，samples/cpp/edge.cpp，samples/cpp/ffilldemo.cpp，samples/cpp/grabcut.cpp，samples/cpp/stitching_detailed.cpp，samples/cpp/tutorial_code/Histograms_Matching/MatchTemplate_Demo.cpp，samples/dnn/segmentation.cpp．"
Reserves space for the certain number of rows.,指定した行数分のスペースを確保します。
sz : Number of rows.,sz : 行の数。
"The method reserves space for sz rows. If the matrix already has enough space to store sz rows, nothing happens. If the matrix is reallocated, the first Mat::rows rows are preserved. The method emulates the corresponding method of the STL vector class.",このメソッドは sz 行分のスペースを確保します。行列が既に sz 行分のスペースを持っている場合は何も起こりません。行列が再割り当てされた場合は，最初の Mat::rows 行が保存されます．このメソッドは，STL vector クラスの対応するメソッドをエミュレートします．
Reserves space for the certain number of bytes.,指定されたバイト数分の領域を確保します。
sz : Number of bytes.,sz : バイト数。
"The method reserves space for sz bytes. If the matrix already has enough space to store sz bytes, nothing happens. If matrix has to be reallocated its previous content could be lost.",このメソッドは，sz バイト分の領域を確保します。行列が既に sz バイト分のスペースを持っている場合は何も起こりません。行列を再配置する必要がある場合、以前の内容が失われる可能性があります。
Changes the number of matrix rows.,行列の行数を変更します。
sz : New number of rows.,sz : 新しい行数。
"The methods change the number of matrix rows. If the matrix is reallocated, the first min(Mat::rows, sz) rows are preserved. The methods emulate the corresponding methods of the STL vector class.","これらのメソッドは、行列の行数を変更します。行列が再割り当てされた場合，最初の min(Mat::rows, sz) 行が保存されます。これらのメソッドは，STL vector クラスの対応するメソッドを模したものです．"
s : Value assigned to the newly added elements.,s :新しく追加された要素に割り当てられる値．
Removes elements from the bottom of the matrix.,nelems ：削除される行数．
"nelems : Number of removed rows. If it is greater than the total number of rows, an exception is thrown.",nelems : 削除された行の数。これが行の総数よりも大きい場合は，例外が発生します．
The method removes one or more rows from the bottom of the matrix.,このメソッドは，行列の最下部から 1 つ以上の行を削除します．
Locates the matrix header within a parent matrix.,行列のヘッダを親行列内に配置します。
wholeSize : Output parameter that contains the size of the whole matrix containing this as a part.,wholeSize : この行列を部分的に含む行列全体のサイズを含む出力パラメータ．
ofs : Output parameter that contains an offset of this inside the whole matrix.,ofs : 行列全体の中での，this のオフセットを含む出力パラメータ．
"After you extracted a submatrix from a matrix using Mat::row, Mat::col, Mat::rowRange, Mat::colRange, and others, the resultant submatrix points just to the part of the original big matrix. However, each submatrix contains information (represented by datastart and dataend fields) that helps reconstruct the original matrix size and the position of the extracted submatrix within the original matrix. The method locateROI does exactly that.","Mat::row, Mat::col, Mat::rowRange, Mat::colRange などを用いて，行列から部分行列を抽出した後，結果として得られる部分行列は，元の大きな行列の一部を指し示します．しかし，各部分行列には，元の行列のサイズや，元の行列内での抽出された部分行列の位置を再構成するのに役立つ情報（ datastart と dataend フィールドによって表現されます）が含まれています．メソッド locateROI は，まさにそれを行います．"
Adjusts a submatrix size and position within the parent matrix.,親行列内の部分行列のサイズと位置を調整します．
dtop : Shift of the top submatrix boundary upwards.,dtop : 上部の部分行列の境界を上に移動します．
dbottom : Shift of the bottom submatrix boundary downwards.,dbottom : 部分行列の最下部の境界を下方向に移動します．
dleft : Shift of the left submatrix boundary to the left.,dleft : 左の部分行列の境界を左に移動します．
dright : Shift of the right submatrix boundary to the right.,dright : 右の部分行列の境界を，右にシフトします．
"The method is complimentary to Mat::locateROI . The typical use of these functions is to determine the submatrix position within the parent matrix and then shift the position somehow. Typically, it can be required for filtering operations when pixels outside of the ROI should be taken into account. When all the method parameters are positive, the ROI needs to grow in all directions by the specified amount, for example:A.adjustROI(2, 2, 2, 2);fragmentIn this example, the matrix size is increased by 4 elements in each direction. The matrix is shifted by 2 elements to the left and 2 elements up, which brings in all the necessary pixels for the filtering with the 5x5 kernel.adjustROI forces the adjusted ROI to be inside of the parent matrix that is boundaries of the adjusted ROI are constrained by boundaries of the parent matrix. For example, if the submatrix A is located in the first row of a parent matrix and you called A.adjustROI(2, 2, 2, 2) then A will not be increased in the upward direction.The function is used internally by the OpenCV filtering functions, like filter2D , morphological operations, and so on.See alsocopyMakeBorder","このメソッドは， Mat::locateROI を補完するものです．これらの関数の典型的な利用方法は，親行列内の部分行列の位置を決定し，その位置を何らかの方法でシフトすることです．典型的には，ROI の外側にあるピクセルを考慮に入れるべきフィルタリング処理に必要になることがあります．A.adjustROI(2, 2, 2, 2);fragmentこの例では，行列のサイズが各方向に 4 要素ずつ増加しています．adjustROI は，調整後の ROI が親行列の内部に入るようにします．つまり，調整後の ROI の境界は，親行列の境界によって制約されます．例えば，部分行列 A が親行列の最初の行に位置していて，A.adjustROI(2, 2, 2, 2) と呼ばれた場合，A は上方向には増加しません．この関数は，OpenCV のフィルタリング関数（ filter2D や形態素解析など）によって内部的に利用されます．"
Reports whether the matrix is continuous or not.,行列が連続しているかどうかを報告します．
"The method returns true if the matrix elements are stored continuously without gaps at the end of each row. Otherwise, it returns false. Obviously, 1x1 or 1xN matrices are always continuous. Matrices created with Mat::create are always continuous. But if you extract a part of the matrix using Mat::col, Mat::diag, and so on, or constructed a matrix header for externally allocated data, such matrices may no longer have this property.The continuity flag is stored as a bit in the Mat::flags field and is computed automatically when you construct a matrix header. Thus, the continuity check is a very fast operation, though theoretically it could be done as follows:// alternative implementation of Mat::isContinuous()bool myCheckMatContinuity(const Mat& m){    //return (m.flags & Mat::CONTINUOUS_FLAG) != 0;    return m.rows == 1 || m.step == m.cols*m.elemSize();}fragmentThe method is used in quite a few of OpenCV functions. The point is that element-wise operations (such as arithmetic and logical operations, math functions, alpha blending, color space transformations, and others) do not depend on the image geometry. Thus, if all the input and output arrays are continuous, the functions can process them as very long single-row vectors. The example below illustrates how an alpha-blending function can be implemented:template<typename T>void alphaBlendRGBA(const Mat& src1, const Mat& src2, Mat& dst){    const float alpha_scale = (float)std::numeric_limits<T>::max(),                inv_scale = 1.f/alpha_scale;    CV_Assert( src1.type() == src2.type() &&               src1.type() == CV_MAKETYPE(traits::Depth<T>::value, 4) &&               src1.size() == src2.size());    Size size = src1.size();    dst.create(size, src1.type());    // here is the idiom: check the arrays for continuity and,    // if this is the case,    // treat the arrays as 1D vectors    if( src1.isContinuous() && src2.isContinuous() && dst.isContinuous() )    {        size.width *= size.height;        size.height = 1;    }    size.width *= 4;    for( int i = 0; i < size.height; i++ )    {        // when the arrays are continuous,        // the outer loop is executed only once        const T* ptr1 = src1.ptr<T>(i);        const T* ptr2 = src2.ptr<T>(i);        T* dptr = dst.ptr<T>(i);        for( int j = 0; j < size.width; j += 4 )        {            float alpha = ptr1[j+3]*inv_scale, beta = ptr2[j+3]*inv_scale;            dptr[j] = saturate_cast<T>(ptr1[j]*alpha + ptr2[j]*beta);            dptr[j+1] = saturate_cast<T>(ptr1[j+1]*alpha + ptr2[j+1]*beta);            dptr[j+2] = saturate_cast<T>(ptr1[j+2]*alpha + ptr2[j+2]*beta);            dptr[j+3] = saturate_cast<T>((1 - (1-alpha)*(1-beta))*alpha_scale);        }    }}fragmentThis approach, while being very simple, can boost the performance of a simple element-operation by 10-20 percents, especially if the image is rather small and the operation is quite simple.Another OpenCV idiom in this function, a call of Mat::create for the destination array, that allocates the destination array unless it already has the proper size and type. And while the newly allocated arrays are always continuous, you still need to check the destination array because Mat::create does not always allocate a new matrix.","このメソッドは，行列の要素が各行の終わりに隙間なく連続して格納されている場合は true を返します．そうでない場合は，false を返します。明らかに，1x1 や 1xN の行列は常に連続しています．Mat::create で作成された行列は，常に連続しています．しかし， Mat::col や Mat::diag などを用いて行列の一部を抽出したり，外部から割り当てられたデータのために行列ヘッダを作成したりすると，そのような行列はもはやこの特性を持たないかもしれません．連続性フラグは Mat::flags フィールドのビットとして保存され，行列ヘッダを作成する際に自動的に計算されます．連続性フラグは Mat::flags フィールドのビットとして保存され，行列のヘッダを作成する際に自動的に計算されます．したがって，連続性チェックは非常に高速な処理ですが，理論的には以下のように行うこともできます： // Mat::isContinuous()の代替実装 bool myCheckMatContinuity(const Mat& m){ //return (m.flags & Mat::CONTINUOUS_FLAG) != 0; return m.rows == 1 || m.step == m.cols*m.elemSize();}fragmentこのメソッドは，OpenCV のかなり多くの関数で利用されています．ポイントは，要素単位の演算（算術演算や論理演算，数学関数，アルファブレンディング，色空間変換など）は，画像の形状に依存しないということです．したがって，入出力配列がすべて連続している場合，関数はそれらを非常に長い1列のベクトルとして処理することができます．以下の例は，アルファブレンディング関数がどのように実装されるかを示しています： template<typename T>void alphaBlendRGBA(const Mat& src1, const Mat& src2, Mat& dst){ const float alpha_scale = (float)std::numeric_limits<T>::max(), inv_scale = 1.f/alpha_scale; CV_Assert( src1.type() == src2.type() && src1.type() == CV_MAKETYPE(traits::Depth<T>::value, 4) && src1.size() == src2.size()); Size size = src1.size(); dst.create(size, src1.type()); // ここでイディオムですが，配列が連続しているかどうかをチェックし， // 連続している場合は， // 配列を 1 次元のベクトルとして扱う if( src1.isContinuous() && src2.isContinuous() && dst.isContinuous() )    { size.width *= size.height; size.height = 1; } size.width *= 4; for( int i = 0; i < size.height; i++ ) { // 配列が連続している場合， // 外側のループは一度だけ実行されます const T* ptr1 = src1.ptr<T>(i); const T* ptr2 = src2.ptr<T>(i); T* dptr = dst.ptr<T>(i); for( int j = 0; j < size.width; j += 4 ) { float alpha = ptr1[j+3]*inv_scale, beta = ptr2[j+3]*inv_scale; dptr[j] = saturate_cast<T>(ptr1[j]*alpha + ptr2[j]*beta);            dptr[j+1] = saturate_cast<T>(ptr1[j+1]*alpha + ptr2[j+1]*beta); dptr[j+2] = saturate_cast<T>(ptr1[j+2]*alpha + ptr2[j+2]*beta); dptr[j+3] = saturate_cast<T>((1 - (1-alpha)*(1-beta))*alpha_scale); }。    この関数に含まれるもう1つの OpenCV のイディオムである，出力配列に対する Mat::create の呼び出しは，出力配列が既に適切なサイズと型を持っている場合を除いて，その出力配列を確保します．また，新たに確保された配列は常に連続していますが， Mat::create が常に新しい行列を確保するとは限らないので，出力配列をチェックする必要があります．"
returns true if the matrix is a submatrix of another matrix,行列が別の行列の部分行列である場合は真を返します．
Returns the matrix element size in bytes.,行列の要素サイズをバイト単位で返します．
"The method returns the matrix element size in bytes. For example, if the matrix type is CV_16SC3 , the method returns 3*sizeof(short) or 6.",このメソッドは，行列の要素サイズをバイト単位で返します．例えば，行列の種類が CV_16SC3 の場合，このメソッドは 3*sizeof(short) または 6 を返します．
Returns the size of each matrix element channel in bytes.,各行列要素のチャンネルのサイズをバイト単位で返します。
"The method returns the matrix element channel size in bytes, that is, it ignores the number of channels. For example, if the matrix type is CV_16SC3 , the method returns sizeof(short) or 2.",このメソッドは，行列要素のチャンネルのサイズをバイト単位で返します（つまり，チャンネル数は無視します）．例えば，行列の種類が CV_16SC3 の場合，このメソッドは sizeof(short) または 2 を返します．
Returns the type of a matrix element.,行列の要素の型を返します．
"The method returns a matrix element type. This is an identifier compatible with the CvMat type system, like CV_16SC3 or 16-bit signed 3-channel array, and so on.Examples: samples/cpp/grabcut.cpp, samples/cpp/image_alignment.cpp, samples/cpp/train_HOG.cpp, and samples/cpp/tutorial_code/ImgProc/Smoothing/Smoothing.cpp.",このメソッドは，行列の要素の型を返します．これは， CV_16SC3 や 16 ビット符号付き 3 チャンネル配列などのように，CvMat の型システムと互換性のある識別子です．例： samples/cpp/grabcut.cpp， samples/cpp/image_alignment.cpp， samples/cpp/train_HOG.cpp， samples/cpp/tutorial_code/ImgProc/Smoothing/Smoothing.cpp．
Returns the depth of a matrix element.,行列の要素の深さを返します。
"The method returns the identifier of the matrix element depth (the type of each individual channel). For example, for a 16-bit signed element array, the method returns CV_16S . A complete list of matrix types contains the following values:CV_8U - 8-bit unsigned integers ( 0..255 )",このメソッドは，行列要素の深度（個々のチャンネルの型）の識別子を返します．例えば，16 ビット符号付き要素の配列の場合，このメソッドは CV_16S を返します．行列の型の完全なリストには，以下の値が含まれます： CV_8U - 8 ビット符号なし整数（0 ～ 255 ）．
CV_8S - 8-bit signed integers ( -128..127 ),CV_8S - 8 ビットの符号付き整数 ( -128..127 )
CV_16U - 16-bit unsigned integers ( 0..65535 ),CV_16U - 16 ビット符号なし整数 ( 0 ～ 65535 )
CV_16S - 16-bit signed integers ( -32768..32767 ),CV_16S - 16 ビット符号付き整数 ( -32768 ...32767 )
CV_32S - 32-bit signed integers ( -2147483648..2147483647 ),CV_32S - 32ビット符号付き整数 ( -2147483648 ～.2147483647 )
"CV_32F - 32-bit floating-point numbers ( -FLT_MAX..FLT_MAX, INF, NAN )","CV_32F - 32ビット浮動小数点数 ( -FLT_MAX..FLT_MAX, INF, NAN )"
"CV_64F - 64-bit floating-point numbers ( -DBL_MAX..DBL_MAX, INF, NAN )Examples: samples/cpp/camshiftdemo.cpp.","CV_64F - 64 ビット浮動小数点数 ( -DBL_MAX..DBL_MAX, INF, NAN )例： samples/cpp/camshiftdemo.cpp."
Returns the number of matrix channels.,マトリックスのチャンネル数を返します。
The method returns the number of matrix channels.Examples: samples/cpp/pca.cpp.,このメソッドは、行列のチャネル数を返します。例：samples/cpp/pca.cpp.
Returns true if the array has no elements.,配列に要素がない場合に真を返します。
"The method returns true if Mat::total() is 0 or if Mat::data is NULL. Because of pop_back() and resize() methods M.total() == 0 does not imply that M.data == NULL.Examples: fld_lines.cpp, samples/cpp/facedetect.cpp, samples/cpp/fitellipse.cpp, samples/cpp/grabcut.cpp, samples/cpp/image_alignment.cpp, samples/cpp/lkdemo.cpp, samples/cpp/squares.cpp, samples/cpp/stitching.cpp, samples/cpp/stitching_detailed.cpp, samples/cpp/train_HOG.cpp, samples/cpp/tutorial_code/HighGUI/AddingImagesTrackbar.cpp, samples/cpp/tutorial_code/Histograms_Matching/MatchTemplate_Demo.cpp, samples/cpp/tutorial_code/ImgProc/Morphology_1.cpp, samples/cpp/tutorial_code/ImgProc/Morphology_2.cpp, samples/cpp/tutorial_code/ImgProc/Pyramids/Pyramids.cpp, samples/cpp/tutorial_code/ImgProc/Smoothing/Smoothing.cpp, samples/cpp/tutorial_code/ImgTrans/copyMakeBorder_demo.cpp, samples/cpp/tutorial_code/ImgTrans/houghcircles.cpp, samples/cpp/tutorial_code/ImgTrans/houghlines.cpp, samples/cpp/tutorial_code/ml/introduction_to_pca/introduction_to_pca.cpp, samples/cpp/tutorial_code/photo/non_photorealistic_rendering/npr_demo.cpp, samples/cpp/tutorial_code/photo/seamless_cloning/cloning_demo.cpp, samples/cpp/watershed.cpp, samples/dnn/colorization.cpp, samples/dnn/object_detection.cpp, samples/dnn/openpose.cpp, and samples/dnn/segmentation.cpp.","このメソッドは， Mat::total() が 0 であるか， Mat::data が NULL である場合に真を返します．pop_back() や resize() メソッドがあるので， M.total() == 0 は M.data == NULL を意味しません． 例： fld_lines.cpp, samples/cpp/facedetect.cpp, samples/cpp/fitellipse.cpp, samples/cpp/grabcut.cpp, samples/cpp/image_alignment.cpp, samples/cpp/lkdemo.cpp, samples/cpp/squares.cpp, samples/cpp/stitching.cpp, samples/cpp/stitching_detailed.cpp, samples/cpp/train_HOG.cpp, samples/cpp/tutorial_code/HighGUI/AddingImagesTrackbar.cpp, samples/cpp/tutorial_code/Histograms_Matching/MatchTemplate_Demo.cpp, samples/cpp/tutorial_code/ImgProc/Morphology_1.cpp, samples/cpp/tutorial_code/ImgProc/Morphology_2.cpp, samples/cpp/tutorial_code/ImgProc/Pyramids/Pyramids.cpp, samples/cpp/tutorial_code/ImgProc/Smoothing/Smoothing.cpp, samples/cpp/tutorial_code/ImgTrans/copyMakeBorder_demo.cpp, samples/cpp/tutorial_code/ImgTrans/houghcircles.cpp, samples/cpp/tutorial_code/ImgTrans/houghlines.cpp, samples/cpp/tutorial_code/ml/introduction_to_pca/introduction_to_pca.cpp、samples/cpp/tutorial_code/photo/non_photorealistic_rendering/npr_demo.cpp、samples/cpp/tutorial_code/photo/seamless_cloning/cloning_demo.cpp、samples/cpp/watershed.cpp、samples/dnn/colorization.cpp、samples/dnn/object_detection.cpp、samples/dnn/openpose.cpp、samples/dnn/segmentation.cppです。"
Returns the total number of array elements.,配列の総要素数を返します。
The method returns the number of array elements (a number of pixels if the array represents an image).Examples: samples/cpp/train_HOG.cpp.,サンプル/cpp/train_HOG.cpp. サンプル/cpp/train_HOG.cpp.
The following code demonstrates its usage for a 2-d matrix:,以下のコードは，2 次元の行列に対する使用例です．
"elemChannels : Number of channels or number of columns the matrix should have. For a 2-D matrix, when the matrix has only 1 column, then it should have elemChannels channels; When the matrix has only 1 channel, then it should have elemChannels columns. For a 3-D matrix, it should have only one channel. Furthermore, if the number of planes is not one, then the number of rows within every plane has to be 1; if the number of rows within every plane is not 1, then the number of planes has to be 1.",elemChannels : 行列が持つべきチャンネル数，または列数．2 次元の行列の場合，行列が 1 列しかない場合は elemChannels channels，行列が 1 チャンネルしかない場合は elemChannels columns を持つべきです．3 次元の行列の場合は，1 つのチャンネルのみを持つ必要があります．さらに，プレーン数が 1 でない場合は，各プレーン内の行数は 1 でなければいけません．
depth : The depth the matrix should have. Set it to -1 when any depth is fine.,depth : 行列が持つべき深さです．任意の深さで良い場合は -1 をセットします．
requireContinuous : Set it to true to require the matrix to be continuous,requireContinuous : 行列が連続的であることを要求する場合は，これを true に設定します．
"cv::Mat mat(20, 1, CV_32FC2);    int n = mat.checkVector(2);    CV_Assert(n == 20); // mat has 20 elements    mat.create(20, 2, CV_32FC1);    n = mat.checkVector(1);    CV_Assert(n == -1); // mat is neither a column nor a row vector    n = mat.checkVector(2);    CV_Assert(n == 20); // the 2 columns are considered as 1 elementfragmentThe following code demonstrates its usage for a 3-d matrix:int dims[] = {1, 3, 5}; // 1 plane, every plane has 3 rows and 5 columns    mat.create(3, dims, CV_32FC1); // for 3-d mat, it MUST have only 1 channel    n = mat.checkVector(5); // the 5 columns are considered as 1 element    CV_Assert(n == 3);    int dims2[] = {3, 1, 5}; // 3 planes, every plane has 1 row and 5 columns    mat.create(3, dims2, CV_32FC1);    n = mat.checkVector(5); // the 5 columns are considered as 1 element    CV_Assert(n == 3);fragment","cv::Mat mat(20, 1, CV_32FC2); int n = mat.checkVector(2); CV_Assert(n == 20); // mat は 20 個の要素を持ちます mat.create(20, 2, CV_32FC1); n = mat.checkVector(1); CV_Assert(n == -1); // mat は，列ベクトルでも行ベクトルでもありません n = mat.checkVector(2); CV_Assert(n == 20); // 2 列を 1 つの要素と見なすfragment次のコードは，3 次元の行列に対する使い方を示しています： int dims[] = {1, 3, 5}; // 1 つの平面，すべての平面は 3 行 5 列である mat.create(3, dims, CV_32FC1); // 3-d の行列では，1つのチャンネルのみを持たなければいけません n = mat.checkVector(5); // 5つの列は1つの要素とみなされます CV_Assert(n == 3); int dims2[] = {3, 1, 5}; // 3つの平面，各平面は1つの行と5つの列を持ちます mat.create(3, dims2, CV_32FC1); n = mat.checkVector(5); // 5 つの列を 1 つの要素と見なす CV_Assert(n == 3);fragment"
Returns a pointer to the specified matrix row.,行列の指定された行へのポインタを返します．
i0 : A 0-based row index.,i0 : 0ベースの行インデックス．
"The methods return uchar* or typed pointer to the specified matrix row. See the sample in Mat::isContinuous to know how to use these methods.Examples: samples/cpp/image_alignment.cpp, samples/cpp/train_HOG.cpp, samples/dnn/colorization.cpp, samples/dnn/openpose.cpp, and samples/dnn/segmentation.cpp.","これらのメソッドは，指定された行列の行に対する uchar* または型付きポインタを返します．これらのメソッドの使い方については， Mat::isContinuous のサンプルを参照してください．例： samples/cpp/image_alignment.cpp, samples/cpp/train_HOG.cpp, samples/dnn/colorization.cpp, samples/dnn/openpose.cpp, and samples/dnn/segmentation.cpp."
row : Index along the dimension 0,row : 0次元のインデックス
col : Index along the dimension 1,col : 1次元目のインデックス
includes several bit-fields:,には，いくつかのビットフィールドが含まれています．
the magic signature,マジックサイン
continuity flag,連続性フラグ
depth,深さ
number of channels,チャンネルの数
"the matrix dimensionality, >= 2",行列の次元，≧2
"the number of rows and columns or (-1, -1) when the matrix has more than 2 dimensions","行列の行数と列数，または行列が2次元以上の場合は (-1, -1) を指定します．"
"Examples: samples/cpp/camshiftdemo.cpp, samples/cpp/convexhull.cpp, samples/cpp/demhist.cpp, samples/cpp/falsecolor.cpp, samples/cpp/fitellipse.cpp, samples/cpp/grabcut.cpp, samples/cpp/image_alignment.cpp, samples/cpp/kalman.cpp, samples/cpp/kmeans.cpp, samples/cpp/minarea.cpp, samples/cpp/squares.cpp, samples/cpp/stitching.cpp, samples/cpp/train_HOG.cpp, samples/cpp/tutorial_code/Histograms_Matching/MatchTemplate_Demo.cpp, samples/cpp/tutorial_code/ImgProc/Pyramids/Pyramids.cpp, samples/cpp/tutorial_code/ImgProc/Smoothing/Smoothing.cpp, samples/cpp/tutorial_code/ImgTrans/copyMakeBorder_demo.cpp, samples/cpp/tutorial_code/ImgTrans/houghcircles.cpp, samples/cpp/tutorial_code/ml/introduction_to_pca/introduction_to_pca.cpp, samples/cpp/warpPerspective_demo.cpp, samples/cpp/watershed.cpp, samples/dnn/object_detection.cpp, and samples/dnn/openpose.cpp.","例： samples/cpp/camshiftdemo.cpp, samples/cpp/convexhull.cpp, samples/cpp/demhist.cpp, samples/cpp/falecolor.cpp, samples/cpp/fitellipse.cpp, samples/cpp/grabcut.cpp, samples/cpp/image_alignment.cpp, samples/cpp/kalman.cpp, samples/cpp/kmeans.cpp, samples/cpp/minarea.cpp, samples/cpp/squares.cpp, samples/cpp/stitching.cpp, samples/cpp/train_HOG.cpp, samples/cpp/tutorial_code/Histograms_Matching/MatchTemplate_Demo.cpp, samples/cpp/tutorial_code/ImgProc/Pyramids/Pyramids.cpp, samples/cpp/tutorial_code/ImgProc/Smoothing/Smoothing.cpp, samples/cpp/tutorial_code/ImgTrans/copyMakeBorder_demo.cpp, samples/cpp/tutorial_code/ImgTrans/houghcircles.cpp、samples/cpp/tutorial_code/ml/introduction_to_pca/introduction_to_pca.cpp、samples/cpp/warpPerspective_demo.cpp、samples/cpp/watershed.cpp、samples/dnn/object_detection.cpp、samples/dnn/openpose.cppがあります。"
"Examples: samples/cpp/camshiftdemo.cpp, samples/cpp/convexhull.cpp, samples/cpp/demhist.cpp, samples/cpp/facedetect.cpp, samples/cpp/falsecolor.cpp, samples/cpp/fitellipse.cpp, samples/cpp/grabcut.cpp, samples/cpp/image_alignment.cpp, samples/cpp/kalman.cpp, samples/cpp/kmeans.cpp, samples/cpp/minarea.cpp, samples/cpp/squares.cpp, samples/cpp/stitching.cpp, samples/cpp/train_HOG.cpp, samples/cpp/tutorial_code/Histograms_Matching/MatchTemplate_Demo.cpp, samples/cpp/tutorial_code/ImgProc/Morphology_1.cpp, samples/cpp/tutorial_code/ImgProc/Pyramids/Pyramids.cpp, samples/cpp/tutorial_code/ImgProc/Smoothing/Smoothing.cpp, samples/cpp/tutorial_code/ImgTrans/copyMakeBorder_demo.cpp, samples/cpp/warpPerspective_demo.cpp, samples/cpp/watershed.cpp, samples/dnn/object_detection.cpp, and samples/dnn/openpose.cpp.","例： samples/cpp/camshiftdemo.cpp、samples/cpp/convexhull.cpp、samples/cpp/demhist.cpp、samples/cpp/facedetect.cpp、samples/cpp/falecolor.cpp、samples/cpp/fitellipse.cpp、samples/cpp/grabcut.cpp, samples/cpp/image_alignment.cpp, samples/cpp/kalman.cpp, samples/cpp/kmeans.cpp, samples/cpp/minarea.cpp, samples/cpp/squares.cpp, samples/cpp/stitching.cpp, samples/cpp/train_HOG.cpp, samples/cpp/tutorial_code/Histograms_Matching/MatchTemplate_Demo.cpp, samples/cpp/tutorial_code/ImgProc/Morphology_1.cpp, samples/cpp/tutorial_code/ImgProc/Pyramids/Pyramids.cpp, samples/cpp/tutorial_code/ImgProc/Smoothing/Smoothing.cpp.cpp、samples/cpp/tutorial_code/ImgTrans/copyMakeBorder_demo.cpp、samples/cpp/warpPerspective_demo.cpp、samples/cpp/watershed.cpp、samples/dn/object_detection.cpp、samples/dn/openpose.cpp。"
pointer to the data,データへのポインタ
Examples: samples/dnn/segmentation.cpp.,例：samples/dnn/segmentation.cpp.
helper fields used in locateROI and adjustROI,locateROI および adjustROI で使用するヘルパー・フィールド
"Examples: fld_lines.cpp, samples/cpp/camshiftdemo.cpp, samples/cpp/connected_components.cpp, samples/cpp/create_mask.cpp, samples/cpp/grabcut.cpp, samples/cpp/image_alignment.cpp, samples/cpp/polar_transforms.cpp, samples/cpp/segment_objects.cpp, samples/cpp/squares.cpp, samples/cpp/stitching_detailed.cpp, samples/cpp/tutorial_code/features2D/Homography/homography_from_camera_displacement.cpp, samples/cpp/tutorial_code/ImgProc/Smoothing/Smoothing.cpp, samples/cpp/tutorial_code/photo/seamless_cloning/cloning_demo.cpp, samples/cpp/watershed.cpp, samples/dnn/colorization.cpp, samples/dnn/openpose.cpp, and samples/dnn/segmentation.cpp.","例：fld_lines.cpp、samples/cpp/camshiftdemo.cpp、samples/cpp/connected_components.cpp、samples/cpp/create_mask.cpp、samples/cpp/grabcut.cpp、samples/cpp/image_alignment.cpp、samples/cpp/polar_transforms.cpp、samples/cpp/segment_objects.cpp、samples/cpp/squares.cpp、samples/cpp/stitching_detailed.cpp。cpp, samples/cpp/tutorial_code/features2D/Homography/homography_from_camera_displacement.cpp, samples/cpp/tutorial_code/ImgProc/Smoothing/Smoothing.cpp、samples/cpp/tutorial_code/photo/seamless_cloning/cloning_demo.cpp、samples/cpp/watershed.cpp、samples/dnn/colorization.cpp、samples/dnn/openpose.cpp、samples/dnn/segmentation.cppです。"
Returns a normalized step.,正規化されたステップを返します。
The method returns a matrix step divided by Mat::elemSize1() . It can be useful to quickly access an arbitrary matrix element.,このメソッドは，行列のステップを Mat::elemSize1() で割った値を返します．これは，任意の行列要素に素早くアクセスするのに役立ちます．
Adds elements to the bottom of the matrix.,行列の底に要素を追加します．
elem : Added element(s).,elem : 追加される要素（複数可）．
"The methods add one or more elements to the bottom of the matrix. They emulate the corresponding method of the STL vector class. When elem is Mat , its type and the number of columns must be the same as in the container matrix.",これらのメソッドは，1 つ以上の要素を行列の底に追加します．これらのメソッドは，STL vector クラスの対応するメソッドをエミュレートしています．elem が Mat の場合，その型と列数は，コンテナ行列と同じでなければいけません．
Matrix expression representation. ,行列式の表現．
"This is a list of implemented matrix operations that can be combined in arbitrary complex expressions (here A, B stand for matrices ( Mat ), s for a scalar ( Scalar ), alpha for a real-valued scalar ( double )):","これは，実装されている行列演算のリストで，任意の複素数表現に組み合わせることができます（ここで A, B は行列（Mat ）を，s はスカラ（Scalar ）を，alpha は実数値のスカラ（double ）を表します）．"
"Addition, subtraction, negation: A+B, A-B, A+s, A-s, s+A, s-A, -A","加算，減算，否定．A+B, A-B, A+s, A-s, s+A, s-A, -A"
Scaling: A*alpha,スケーリングA*α
"Per-element multiplication and division: A.mul(B), A/B, alpha/A",要素ごとの乗算と除算。A.mul(B)、A/B、α/A
Matrix multiplication: A*B,マトリックスの乗算。A*B
Transposition: A.t() (means AT),転置。A.t() (ATの意)
"Matrix inversion and pseudo-inversion, solving linear systems and least-squares problems: A.inv([method]) (~ A<sup>-1</sup>), A.inv([method])*B (~ X: AX=B)",行列の逆変換や擬似逆変換、連立方程式や最小二乗問題の解法。A.inv([方法])（～A<sup>-1</sup>）、A.inv([方法])*B（～X: AX=B)
"Comparison: A cmpop B, A cmpop alpha, alpha cmpop A, where cmpop is one of >, >=, ==, !=, <=, <. The result of comparison is an 8-bit single channel mask whose elements are set to 255 (if the particular element or pair of elements satisfy the condition) or 0.","比較する。A cmpop B, A cmpop alpha, alpha cmpop A, ここで cmpop は >, >=, ==, !=, <=, < のいずれかです。比較の結果は、要素が 255 (特定の要素または要素のペアが条件を満たす場合) または 0 に設定される 8 ビットのシングルチャネルマスクです。"
"Bitwise logical operations: A logicop B, A logicop s, s logicop A, ~A, where logicop is one of &, |, ^.","ビット単位の論理演算。A logicop B, A logicop s, s logicop A, ~A（logicop は &, |, ^ のいずれか）。"
"Element-wise minimum and maximum: min(A, B), min(A, alpha), max(A, B), max(A, alpha)","要素単位の最小値と最大値：min(A, B), min(A, alpha), max(A, B), max(A, alpha)"
Element-wise absolute value: abs(A),要素ごとの絶対値：abs(A)
"Cross-product, dot-product: A.cross(B), A.dot(B)","クロスプロダクト、ドットプロダクトA.cross(B), A.dot(B)"
"Any function of matrix or matrices and scalars that returns a matrix or a scalar, such as norm, mean, sum, countNonZero, trace, determinant, repeat, and others.","norm, mean, sum, countNonZero, trace, determinant, repeat など，行列や行列とスカラを返すあらゆる関数．"
"Matrix initializers ( Mat::eye(), Mat::zeros(), Mat::ones() ), matrix comma-separated initializers, matrix constructors and operators that extract sub-matrices (see Mat description).","行列の初期化子（ Mat::eye(), Mat::zeros(), Mat::ones() ），カンマで区切られた行列の初期化子，行列のコンストラクタ，サブ行列を抽出する演算子（ Mat の説明を参照してください）．"
Mat_<destination_type>() constructors to cast the result to the proper type. NoteComma-separated initializers and probably some other operations may require additional explicit Mat() or Mat_<T>() constructor calls to resolve a possible ambiguity.,結果を適切な型にキャストする Mat_<destination_type>() コンストラクタ．注意カンマで区切られた初期化子や，おそらくその他のいくつかの演算子は，起こりうる曖昧さを解決するために，明示的な Mat() または Mat_<T>() コンストラクタの呼び出しを必要とします．
"Here are examples of matrix expressions: // compute pseudo-inverse of A, equivalent to A.inv(DECOMP_SVD)SVD svd(A);Mat pinvA = svd.vt.t()*Mat::diag(1./svd.w)*svd.u.t();// compute the new vector of parameters in the Levenberg-Marquardt algorithmx -= (A.t()*A + lambda*Mat::eye(A.cols,A.cols,A.type())).inv(DECOMP_CHOLESKY)*(A.t()*err);// sharpen image using ""unsharp mask"" algorithmMat blurred; double sigma = 1, threshold = 5, amount = 1;GaussianBlur(img, blurred, Size(), sigma, sigma);Mat lowContrastMask = abs(img - blurred) < threshold;Mat sharpened = img*(1+amount) + blurred*(-amount);img.copyTo(sharpened, lowContrastMask); ","以下は，行列式の例です．A.inv(DECOMP_SVD)SVD svd(A);Mat pinvA = svd.vt.t()*Mat::diag(1./svd.w)*svd.u. t()； // A の擬似逆行列を計算する（A.inv(DECOMP_SVD)SVD svd(A) と同等）。t();// Levenberg-Marquardt アルゴリズムのパラメータの新しいベクトルを計算するx -= (A.t()*A + lambda*Mat::eye(A.cols,A.cols,A.type()).inv(DECOMP_CHOLESKY)*(A.t()*err);// 「アンシャープマスク」アルゴリズムを用いて画像をシャープにするMat blurred; double sigma = 1, threshold = 5, amount = 1;GaussianBlur(img, blurred, Size(), sigma, sigma);Mat lowContrastMask = abs(img - blurred) < threshold;Mat sharpened = img*(1+amount) + blurred*(-amount);img.copyTo(sharpened, lowContrastMask)．"
This type is very similar to InputArray except that it is used for input/output and output function parameters. ,この型は，入出力や出力関数のパラメータに使われることを除けば，InputArray と非常によく似ています．
"Just like with InputArray, OpenCV users should not care about OutputArray, they just pass Mat, vector<T> etc. to the functions. The same limitation as for InputArray: Do not explicitly create OutputArray instances applies here too.",InputArray の場合と同様に，OpenCV ユーザは OutputArray を気にする必要はなく， Mat や vector<T> などを関数に渡すだけです．InputArray と同様の制限があります．OutputArray のインスタンスを明示的に作成してはいけません。
"If you want to make your function polymorphic (i.e. accept different arrays as output parameters), it is also not very difficult. Take the sample above as the reference. Note that _OutputArray::create() needs to be called before _OutputArray::getMat(). This way you guarantee that the output array is properly allocated.",関数を多相型にしたい（つまり，出力パラメータとして異なる配列を受け付ける）場合も，それほど難しいことではありません．上のサンプルを参考にしてください。_OutputArray::create() は， _OutputArray::getMat() の前に呼ばれる必要があることに注意してください．こうすることで，出力配列が適切に確保されていることが保証されます．
"Optional output parameters. If you do not need certain output array to be computed and returned to you, pass cv::noArray(), just like you would in the case of optional input array. At the implementation level, use _OutputArray::needed() to check if certain output array needs to be computed or not.",オプションの出力パラメータ．特定の出力配列を計算して返す必要がない場合は，入力配列がオプションの場合と同様に， cv::noArray() を渡します．実装レベルでは， _OutputArray::needed() を用いて，ある出力配列を計算する必要があるかどうかをチェックします．
There are several synonyms for OutputArray that are used to assist automatic Python/Java/... wrapper generators: typedef OutputArray OutputArrayOfArrays;typedef OutputArray InputOutputArray;typedef OutputArray InputOutputArrayOfArrays; ,OutputArray には，Python/Java/... ラッパーの自動生成を補助するために利用される，いくつかの同義語があります： typedef OutputArray OutputArrayOfArrays;typedef OutputArray InputOutputArray;typedef OutputArray InputOutputArrayOfArrays;
The class SparseMat represents multi-dimensional sparse numerical arrays. ,SparseMat クラスは，多次元の疎な数値配列を表します．
"Such a sparse array can store elements of any type that Mat can store. Sparse means that only non-zero elements are stored (though, as a result of operations on a sparse matrix, some of its stored elements can actually become 0. It is up to you to detect such elements and delete them using SparseMat::erase ). The non-zero elements are stored in a hash table that grows when it is filled so that the search time is O(1) in average (regardless of whether element is there or not). Elements can be accessed using the following methods:",このような疎な配列は， Mat が格納できるあらゆる型の要素を格納できます．疎な配列とは，0 ではない要素のみが格納されていることを意味します（ただし，疎な行列に対する演算の結果，格納されている要素のいくつかは実際に 0 になる可能性があります．）0 ではない要素は，ハッシュテーブルに保存されます．このハッシュテーブルは，（要素が存在するかどうかに関わらず）平均して検索時間が O(1) になるように，埋められるたびに大きくなります．要素へのアクセスには，以下の方法があります．
"Query operations (SparseMat::ptr and the higher-level SparseMat::ref, SparseMat::value and SparseMat::find), for example: const int dims = 5;int size[5] = {10, 10, 10, 10, 10};SparseMat sparse_mat(dims, size, CV_32F);for(int i = 0; i < 1000; i++){    int idx[dims];    for(int k = 0; k < dims; k++)        idx[k] = rand() % size[k];    sparse_mat.ref<float>(idx) += 1.f;}cout << ""nnz = "" << sparse_mat.nzcount() << endl;","例えば，クエリ操作（SparseMat::ptr や，上位の SparseMat::ref，SparseMat::value，SparseMat::find）．const int dims = 5;int size[5] = {10, 10, 10, 10, 10};SparseMat sparse_mat(dims, size, CV_32F);for(int i = 0; i < 1000; i++){ int idx[dims]; for(int k = 0; k < dims; k++) idx[k] = rand() % size[k]; sparse_mat.ref<float>(idx) += 1.f;}cout << ""nnz = "" << sparse_mat.nzcount() << endl;"
"Sparse matrix iterators. They are similar to MatIterator but different from NAryMatIterator. That is, the iteration loop is familiar to STL users: // prints elements of a sparse floating-point matrix// and the sum of elements.SparseMatConstIterator_<float>    it = sparse_mat.begin<float>(),    it_end = sparse_mat.end<float>();double s = 0;int dims = sparse_mat.dims();for(; it != it_end; ++it){    // print element indices and the element value    const SparseMat::Node* n = it.node();    printf(""("");    for(int i = 0; i < dims; i++)        printf(""%d%s"", n->idx[i], i < dims-1 ? "", "" : "")"");    printf("": %g\n"", it.value<float>());    s += *it;}printf(""Element sum is %g\n"", s); If you run this loop, you will notice that elements are not enumerated in a logical order (lexicographical, and so on). They come in the same order as they are stored in the hash table (semi-randomly). You may collect pointers to the nodes and sort them to get the proper ordering. Note, however, that pointers to the nodes may become invalid when you add more elements to the matrix. This may happen due to possible buffer reallocation.","疎行列のイテレータ．これらは MatIterator と似ていますが，NAryMatIterator とは異なります．つまり，STL ユーザーにはおなじみの反復ループです．// 疎な浮動小数点型行列の要素//と要素の合計を表示します．SparseMatConstIterator_<float> it = sparse_mat.begin<float>(), it_end = sparse_mat.end<float>();double s = 0;int dims = sparse_mat.dims();for(; it != it_end; ++it){ // 要素のインデックスと要素の値を表示 const SparseMat::Node* n = it.node(); printf(""(""); for(int i = 0; i < dims; i++) printf(""%d%s"", n->idx[i], i < dims-1 ? "", "" : "")""); printf("":%g\n"", it.value<float>()); s += *it;}printf(""Element sum is %g\n"", s); このループを実行すると、要素が論理的な順序（辞書的な順序など）で列挙されていないことに気がつきます。ハッシュテーブルに格納されているのと同じ順序（半ランダム）で並んでいます。ノードへのポインタを集めてソートすれば、適切な順序になります。ただし，行列に要素を追加すると，ノードへのポインタが無効になることがあるので注意が必要です．これは，バッファが再割り当てされる可能性があるために起こります．"
"Combination of the above 2 methods when you need to process 2 or more sparse matrices simultaneously. For example, this is how you can compute unnormalized cross-correlation of the 2 floating-point sparse matrices: double cross_corr(const SparseMat& a, const SparseMat& b){    const SparseMat *_a = &a, *_b = &b;    // if b contains less elements than a,    // it is faster to iterate through b    if(_a->nzcount() > _b->nzcount())        std::swap(_a, _b);    SparseMatConstIterator_<float> it = _a->begin<float>(),                                   it_end = _a->end<float>();    double ccorr = 0;    for(; it != it_end; ++it)    {        // take the next element from the first matrix        float avalue = *it;        const Node* anode = it.node();        // and try to find an element with the same index in the second matrix.        // since the hash value depends only on the element index,        // reuse the hash value stored in the node        float bvalue = _b->value<float>(anode->idx,&anode->hashval);        ccorr += avalue*bvalue;    }    return ccorr;} ","2つ以上の疎な行列を同時に処理する必要がある場合は，上記2つの方法を組み合わせます．double cross_corr(const SparseMat& a, const SparseMat& b){ const SparseMat *_a = &a, *_b = &b; // b の要素数が a よりも少ない場合は， // b を繰り返し処理した方が高速です if(_a->nzcount() > _b->nzcount()) std::swap(_a, _b); SparseMatConstIterator_<float> it = _a->begin<float>(), it_end = _a->end<float>(); double ccorr = 0; for(; it != it_end; ++it) { // 1 番目の行列から次の要素を取り出します float avalue = *it; const Node* anode = it.node(); // そして，2 番目の行列から同じインデックスを持つ要素を見つけようとします．        // ハッシュ値は要素のインデックスにのみ依存するので， // ノードに格納されているハッシュ値を再利用します float bvalue = _b->value<float>(anode->idx,&anode->hashval); ccorr += avalue*bvalue; } return ccorr;}."
creates full copy of the matrix,行列の完全なコピーを作成
copies all the data to the destination matrix. All the previous content of m is erased,は，すべてのデータをコピー先の行列にコピーします．m の以前の内容はすべて消去されます。
converts sparse matrix to dense n-dim matrix with optional type conversion and scaling.,疎な行列を，オプションの型変換とスケーリングを用いて，密な n-dim 行列に変換します．
m : [out],m : [out］
rtype : [in],rtype :[in］
alpha : [in],アルファ : [in］
beta : [in],beta : [in]:[in］
reallocates sparse matrix.,疎な行列を再割り当てします．
"If the matrix already had the proper size and type, it is simply cleared with clear(), otherwise, the old matrix is released (using release()) and the new one is allocated.",行列が既に適切なサイズと型を持っている場合は，単に clear() でクリアされます．そうでない場合は，（release() を用いて）古い行列が解放され，新しい行列が割り当てられます．
"sets all the sparse matrix elements to 0, which means clearing the hash table.",疎な行列の要素をすべて 0 にし，ハッシュテーブルをクリアします．
manually increments the reference counter to the header.,ヘッダへの参照カウンタを手動でインクリメントします．
converts sparse matrix to the old-style representation; all the elements are copied.,疎行列を古い形式の表現に変換し，すべての要素がコピーされます．
returns the size of each element in bytes (not including the overhead - the space occupied by SparseMat::Node elements),各要素のサイズをバイト単位で返します（オーバーヘッド，つまり SparseMat::Node 要素によって占有される領域は含まれません）．
returns elemSize()/channels(),elemSize()/channels() を返します．
returns type of sparse matrix elements,疎な行列要素の型を返します．
returns the depth of sparse matrix elements,疎な行列の要素の深さを返します．
returns the number of channels,チャンネルの数を返します．
"returns the array of sizes, or NULL if the matrix is not allocated",サイズの配列を返します．また，行列が割り当てられていない場合は NULL を返します．
returns the matrix dimensionality,行列の次元を返します．
returns the number of non-zero elements (=the number of hash table nodes),0 ではない要素の数（＝ハッシュテーブルのノード数）を返します．
computes the element hash value (1D case),要素のハッシュ値を計算します（1次元の場合）．
returns pointer to the specified element (1D case),指定された要素へのポインタを返します（1次元の場合）．
"specialized variants for 1D, 2D, 3D cases and the generic_type one for n-D case. return pointer to the matrix element.if the element is there (it's non-zero), the pointer to it is returned","1D, 2D, 3D に特化したバージョンと，n-D に特化した generic_type があります． 行列の要素へのポインタを返します．要素が存在する（0 ではない）場合は，そのポインタが返されます．"
"if it's not there and createMissing=false, NULL pointer is returned",要素が存在せず，かつ createMissing=false の場合は，NULLポインタが返されます．
"if it's not there and createMissing=true, then the new element is created and initialized with 0. Pointer to it is returned",要素が存在せず createMissing=true であれば，新しい要素が作成され，0 で初期化されます．そのポインタが返されます．
"if the optional hashval pointer is not NULL, the element hash value is not computed, but *hashval is taken instead.",オプションの hashval ポインタが NULL でない場合，要素のハッシュ値は計算されず，代わりに *hashval が取得されます．
Reads a network model stored in Darknet model files.,Darknetモデルファイルに格納されているネットワークモデルを読み込みます。
cfgFile : path to the .cfg file with text description of the network architecture.,cfgFile : ネットワークアーキテクチャのテキスト記述がある.cfgファイルへのパス。
darknetModel : path to the .weights file with learned network.,darknetModel : ネットワークを学習した .weights ファイルへのパス。
bufferCfg : A buffer contains a content of .cfg file with text description of the network architecture.,bufferCfg : ネットワークアーキテクチャをテキストで記述した .cfg ファイルの内容を格納したバッファです。
lenCfg : Number of bytes to read from bufferCfg,lenCfg : bufferCfg から読み取るバイト数。
bufferModel : A buffer contains a content of .weights file with learned network.,bufferModel : 学習したネットワークを含む .weights ファイルの内容を含むバッファ。
lenModel : Number of bytes to read from bufferModel,lenModel : bufferModelから読み込まれるバイト数
Reads a network model stored in Caffe framework's format.,Caffe フレームワークのフォーマットで保存されたネットワークモデルを読み込みます。
prototxt : path to the .prototxt file with text description of the network architecture.,prototxt : ネットワークアーキテクチャをテキストで記述した .prototxt ファイルへのパス。
caffeModel : path to the .caffemodel file with learned network.,caffeModel : ネットワークを学習した .caffemodel ファイルへのパス。
Examples: samples/dnn/colorization.cpp.,例：samples/dnn/colorization.cpp.
Reads a network model stored in Caffe model in memory.,Caffeモデルに格納されたネットワークモデルをメモリ上に読み込みます。
bufferProto : buffer containing the content of the .prototxt file,bufferProto : .prototxtファイルの内容を含むバッファ。
lenProto : length of bufferProto,lenProto : bufferProtoの長さ
bufferModel : buffer containing the content of the .caffemodel file,bufferModel : .caffemodelファイルの内容を含むバッファ
lenModel : length of bufferModel,lenModel : bufferModel の長さ
Reads a network model stored in TensorFlow framework's format.,TensorFlow フレームワークのフォーマットで保存されたネットワークモデルを読み込みます。
model : path to the .pb file with binary protobuf description of the network architecture,model : ネットワークアーキテクチャのバイナリprotobufを記述した.pbファイルへのパス
config : path to the .pbtxt file that contains text graph definition in protobuf format. Resulting Net object is built by text graph using weights from a binary one that let us make it more flexible.,config : protobuf形式のテキストグラフ定義を格納した.pbtxtファイルへのパス。結果として得られるネットオブジェクトは、バイナリグラフの重みを利用したテキストグラフによって構築されており、より柔軟性の高いものとなっています。
bufferModel : buffer containing the content of the pb file,bufferModel : pbファイルの内容を含むバッファ
bufferConfig : buffer containing the content of the pbtxt file,bufferConfig : pbtxtファイルの内容を含むバッファ
lenConfig : length of bufferConfig,lenConfig : bufferConfigの長さ
Reads a network model stored in Torch7 framework's format.,Torch7フレームワークのフォーマットで保存されたネットワークモデルを読み込みます。
"model : path to the file, dumped from Torch by using torch.save() function.",model : torch.save()関数を使ってTorchからダンプされたファイルのパスを指定します。
isBinary : specifies whether the network was serialized in ascii mode or binary.,isBinary : ネットワークがasciiモードでシリアル化されているか、バイナリであるかを指定します。
"evaluate : specifies testing phase of network. If true, it's similar to evaluate() method in Torch.",evaluate : ネットワークのテスト段階を指定します。trueの場合は、Torchのevaluate()メソッドに似ています。
"NoteAscii mode of Torch serializer is more preferable, because binary mode extensively use long type of C language, which has various bit-length on different systems.The loading file must contain serialized nn.Module object with importing network. Try to eliminate a custom objects from serialazing data to avoid importing errors.List of supported layers (i.e. object instances derived from Torch nn.Module class):nn.Sequential",注）バイナリモードでは、C言語のロングタイプが多用され、システムによってビット長が異なるため、Torchシリアライザのアスキーモードの方が好ましいです。ローディングファイルには、ネットワークをインポートしたnn.Moduleオブジェクトをシリアライズしたものが含まれていなければなりません。サポートされているレイヤ(Torch nn.Moduleクラスから派生したオブジェクトインスタンス)のリスト:nn.Sequential
nn.Parallel,nn.パラレル
nn.Concat,nn.コンカト
nn.Linear,nn.リニア
nn.SpatialConvolution,nn.SpatialConvolution（空間コンボリューション
"nn.SpatialMaxPooling, nn.SpatialAveragePooling","nn.SpatialMaxPooling, nn.SpatialAveragePooling"
"nn.ReLU, nn.TanH, nn.Sigmoid","nn.ReLU, nn.TanH, nn.Sigmoid"
nn.Reshape,nn.リシェープ
"nn.SoftMax, nn.LogSoftMaxAlso some equivalents of these classes from cunn, cudnn, and fbcunn may be successfully imported.","nn.SoftMax, nn.LogSoftMaxまた、cunn、cudnn、fbcunnのこれらのクラスの同等のものが正常にインポートされる場合もあります。"
Loads blob which was serialized as torch.Tensor object of Torch7 framework.,Torch7 フレームワークの torch.Tensor オブジェクトとしてシリアル化された blob を読み込みます。
WarningThis function has the same limitations as readNetFromTorch().,警告この関数には readNetFromTorch() と同じ制限があります。
Load a network from Intel's Model Optimizer intermediate representation.,インテルの Model Optimizer の中間表現からネットワークを読み込みます。
xml : [in],xml : [in].
bin : [in],bin : [in] です。
Reads a network model ONNX.,ネットワークモデルONNXを読み込みます。
onnxFile : path to the .onnx file with text description of the network architecture.,onnxFile : ネットワークアーキテクチャをテキストで記述した.onnxファイルへのパスを指定します。
Reads a network model from ONNX in-memory buffer.,ONNXのインメモリバッファからネットワークモデルを読み込みます。
buffer : memory address of the first byte of the buffer.,buffer : バッファの最初のバイトのメモリアドレス。
sizeBuffer : size of the buffer.,sizeBuffer : バッファのサイズを指定します。
Creates blob from .pb file.,.pbファイルからblobを作成します。
path : to the .pb file with input tensor.,path : 入力テンソルのある.pbファイルへのパスです。
"Creates 4-dimensional blob from image. Optionally resizes and crops image from center, subtract mean values, scales values by scalefactor, swap Blue and Red channels.",画像から4次元のblobを作成します。オプションとして，画像のリサイズと中央からの切り取り，平均値の差し引き，scalefactorによる値のスケーリング，青と赤のチャンネルの入れ替えを行います．
"image : input image (with 1-, 3- or 4-channels).",image : 入力画像（1，3，4チャンネル）．
size : spatial size for output image,size : 出力画像の空間サイズ．
"mean : scalar with mean values which are subtracted from channels. Values are intended to be in (mean-R, mean-G, mean-B) order if image has BGR ordering and swapRB is true.","mean : チャンネルから減算される平均値を表すスカラー．image が BGR order で swapRB が true の場合，値は (mean-R, mean-G, mean-B) の順になります．"
scalefactor : multiplier for image values.,scalefactor : 画像の値に対する乗数．
swapRB : flag which indicates that swap first and last channels in 3-channel image is necessary.,swapRB : 3チャンネル画像の最初と最後のチャンネルを入れ替える必要があることを示すフラグ．
crop : flag which indicates whether image will be cropped after resize or not,crop : リサイズ後に画像をクロップするかどうかを示すフラグ
ddepth : Depth of output blob. Choose CV_32F or CV_8U.,ddepth : 出力されるblobの深さを指定します．CV_32F または CV_8U を選択します．
"if crop is true, input image is resized so one side after resize is equal to corresponding dimension in size and another one is equal or larger. Then, crop from the center is performed. If crop is false, direct resize without cropping and preserving aspect ratio is performed.Examples: samples/dnn/classification.cpp, samples/dnn/colorization.cpp, samples/dnn/object_detection.cpp, samples/dnn/openpose.cpp, and samples/dnn/segmentation.cpp.","crop が真の場合，入力画像はリサイズされ，リサイズ後の片側が対応する次元のサイズと等しくなり，もう片側が同等以上のサイズになります．そして，中央からのクロップが行われます．例： samples/dnn/classification.cpp, samples/dnn/colorization.cpp, samples/dnn/object_detection.cpp, samples/dnn/openpose.cpp, samples/dnn/segmentation.cpp."
"Creates 4-dimensional blob from series of images. Optionally resizes and crops images from center, subtract mean values, scales values by scalefactor, swap Blue and Red channels.",一連の画像から 4 次元の blob を作成します。オプションで，画像のリサイズや中央からの切り出し，平均値の減算，scalefactor による値のスケーリング，青と赤のチャンネルの入れ替えなどを行います．
"images : input images (all with 1-, 3- or 4-channels).",images : 入力画像（1，3，4チャンネルのいずれか）．
scalefactor : multiplier for images values.,scalefactor : 画像の値に対する乗数．
"if crop is true, input image is resized so one side after resize is equal to corresponding dimension in size and another one is equal or larger. Then, crop from the center is performed. If crop is false, direct resize without cropping and preserving aspect ratio is performed.",crop が true の場合，入力画像はリサイズされ，リサイズ後の片側は対応する次元のサイズと等しくなり，もう片側は同等かそれ以上になります．そして，中央からのクロップを行います．crop が false の場合、アスペクト比を維持したままトリミングを行わない直接のリサイズが行われます。
Convert all weights of Caffe network to half precision floating point.,Caffe ネットワークのすべての重みを半精度浮動小数点に変換します。
src : Path to origin model from Caffe framework contains single precision floating point weights (usually has .caffemodel extension).,src : 単精度浮動小数点の重みを含む、Caffe フレームワークからのオリジンモデルへのパス（通常、.caffemodel という拡張子が付いています）。
dst : Path to destination model with updated weights.,dst : 更新された重みを持つデスティネーションモデルへのパス。
"layersTypes : Set of layers types which parameters will be converted. By default, converts only Convolutional and Fully-Connected layers' weights.",layersTypes :パラメータが変換されるレイヤータイプのセット。デフォルトでは，ConvolutionalとFully-Connectedレイヤーのウェイトのみを変換します．
NoteShrinked model has no origin float32 weights so it can't be used in origin Caffe framework anymore. However the structure of data is taken from NVidia's Caffe fork: https://github.com/NVIDIA/caffe. So the resulting model may be used there.,NoteShrinked モデルにはオリジンの float32 重みがないため、オリジンの Caffe フレームワークでは使用できません。しかし、データの構造はNVidiaのCaffeフォーク（https://github.com/NVIDIA/caffe）から取られています。そのため、結果のモデルはそこで使用することができます。
Create a text representation for a binary network stored in protocol buffer format.,プロトコルバッファ形式で保存されたバイナリネットワークのテキスト表現を作成します。
model : [in],モデル : [in］
output : [in],出力 : [in］
"NoteTo reduce output file size, trained weights are not included.",注意 出力ファイルのサイズを小さくするため、学習済みの重みは含まれません。
Performs non maximum suppression given boxes and corresponding scores.,ボックスとそれに対応するスコアが与えられると、非最大級の抑制を行います。
bboxes : a set of bounding boxes to apply NMS.,bboxes : NMSを適用するバウンディングボックスのセット。
scores : a set of corresponding confidences.,score : 対応するコンフィデンスのセット．
score_threshold : a threshold used to filter boxes by score.,score_threshold : スコアでボックスをフィルタリングするための閾値。
nms_threshold : a threshold used in non maximum suppression.,nms_threshold : 非最大級の抑制を行う際に使用される閾値．
indices : the kept indices of bboxes after NMS.,indices : NMS後のbboxsの保持インデックス．
eta : a coefficient in adaptive threshold formula: \(nms\_threshold_{i+1}=eta\cdot nms\_threshold_i\).,eta : adaptive threshold formulaの係数。\(nms\_threshold_{i+1}=eta\cdot nms\_threshold_i\).
"top_k : if >0, keep at most top_k picked indices.",top_k : >0 の場合、最大で top_k 個のピックしたインデックスを保持する。
Examples: samples/dnn/object_detection.cpp.,例：samples/dnn/object_detection.cpp.
Release a Myriad device (binded by OpenCV).,Myriadデバイス（OpenCVにバインドされている）を解放する．
Single Myriad device cannot be shared across multiple processes which uses Inference Engine's Myriad plugin.,1つのMyriadデバイスを，推論エンジンのMyriadプラグインを利用する複数のプロセスで共有することはできません．
This class allows to create and manipulate comprehensive artificial neural networks. ,このクラスは，包括的な人工ニューラルネットワークの作成と操作を可能にします．
"Neural network is presented as directed acyclic graph (DAG), where vertices are Layer instances, and edges specify relationships between layers inputs and outputs.",ニューラルネットワークは，有向非循環グラフ（DAG）として表現され，頂点はレイヤのインスタンスであり，辺はレイヤの入力と出力の間の関係を指定する．
Each network layer has unique integer id and unique string name inside its network. LayerId can store either layer name or layer id.,各ネットワーク層は、そのネットワーク内で一意の整数IDと一意の文字列名を持っています。LayerIdには、レイヤ名とレイヤIDのいずれかが格納されます。
"This class supports reference counting of its instances, i. e. copies point to the same instance. ",このクラスは、そのインスタンスの参照カウントをサポートしています。
"Examples: samples/dnn/colorization.cpp, and samples/dnn/openpose.cpp.",例：samples/dnn/colorization.cpp、samples/dnn/openpose.cpp。
Create a network from Intel's Model Optimizer intermediate representation (IR).,インテルのモデル・オプティマイザーの中間表現 (IR) からネットワークを作成します。
Returns true if there are no layers in the network.,ネットワークにレイヤーが存在しない場合は true を返します。
Dump net to String.,ネットを文字列にダンプします。
"Dump net structure, hyperparameters, backend, target and fusion to dot file.",netの構造、ハイパーパラメータ、backend、target、fusionをdotファイルにダンプします。
path : path to output file with .dot extension,path : 出力ファイルへのパス、拡張子は.dot
See alsodump(),alsodump()参照
Converts string name of the layer to the integer identifier.,レイヤーの文字列名を整数の識別子に変換します。
Connects output of the first layer to input of the second layer.,第1層の出力を第2層の入力に接続します。
outPin : descriptor of the first layer output.,outPin : 第一階層の出力の記述子。
inpPin : descriptor of the second layer input.,inpPin : 第二層の入力のディスクリプタ．
Descriptors have the following template <layer_name>[.input_number]:the first part of the template layer_name is string name of the added layer. If this part is empty then the network input pseudo layer will be used;,記述子は，次のようなテンプレートを持っています。 <layer_name>[.input_number]:テンプレートの最初の部分 layer_name は，追加される層の文字列名です。この部分が空の場合は、ネットワーク入力の疑似レイヤーが使用されます。
"the second optional part of the template input_number is either number of the layer input, either label one. If this part is omitted then the first layer input will be used.",テンプレートの2番目のオプション部分 input_number は，入力されたレイヤの番号で，ラベル1の場合もあります。この部分が省略された場合は，最初の層の入力が使用されます。
"See alsosetNetInputs(), Layer::inputNameToIndex(), Layer::outputNameToIndex()","See alsetNetInputs(), Layer::inputNameToIndex(), Layer::outputNameToIndex()"
Connects #outNum output of the first layer to #inNum input of the second layer.,第1層の#outNum出力と第2層の#inNum入力を接続する。
outLayerId : identifier of the first layer,outLayerId : 第1レイヤーの識別子
outNum : number of the first layer output,outNum : 第一階層の出力の番号
inpLayerId : identifier of the second layer,inpLayerId : 第二の層の識別子
inpNum : number of the second layer input,inpNum : 第二層の入力の番号
Sets outputs names of the network input pseudo layer.,ネットワーク入力疑似層の出力名を設定する。
"Each net always has special own the network input pseudo layer with id=0. This layer stores the user blobs only and don't make any computations. In fact, this layer provides the only way to pass user data into the network. As any other layer, this layer can label its outputs and this function provides an easy way to do this.",各ネットは、id=0の特別なネットワーク入力疑似層を常に持っています。この層は、ユーザーblobを保存するだけで、いかなる計算も行いません。実際、この層は、ユーザーデータをネットワークに渡す唯一の方法を提供します。他の層と同様に、この層はその出力にラベルを付けることができ、この関数はこれを簡単に行う方法を提供します。
Runs forward pass to compute output of layer with name outputName.,outputName という名前のレイヤーの出力を計算するためにフォワードパスを実行します。
outputName : name for layer which output is needed to get,outputName : 出力を得るのに必要なレイヤーの名前
"By default runs forward pass for the whole network.Examples: samples/dnn/colorization.cpp, and samples/dnn/openpose.cpp.",sample/dn/colorization.cppやsamples/dn/openpose.cppのように、デフォルトではネットワーク全体のフォワードパスを実行します。
outputBlobs : contains all output blobs for specified layer.,outputBlobs : 指定されたレイヤのすべての出力blobを含む。
"If outputName is empty, runs forward pass for the whole network.",outputNameが空の場合は、ネットワーク全体のフォワードパスを実行します。
Compile Halide layers.,Halideレイヤーをコンパイルします。
scheduler : [in],scheduler : [in].
"See alsosetPreferableBackendSchedule layers that support Halide backend. Then compile them for specific target. For layers that not represented in scheduling file or if no manual scheduling used at all, automatic scheduling will be applied.",alsosetPreferableBackendScheduleを参照して、Halideバックエンドをサポートするレイヤーを設定します。その後、特定のターゲットのためにコンパイルします。スケジューリングファイルに記述されていないレイヤーや、手動でのスケジューリングが全く行われていない場合は、自動スケジューリングが適用されます。
Ask network to use specific computation backend where it supported.,ネットワークに、サポートされている特定の計算バックエンドを使用するように依頼します。
backendId : [in],backendId : [in].
"See alsoBackendIf OpenCV is compiled with Intel's Inference Engine library, DNN_BACKEND_DEFAULT means DNN_BACKEND_INFERENCE_ENGINE. Otherwise it equals to DNN_BACKEND_OPENCV.",Backend も参照してください。OpenCV が Intel の Inference Engine ライブラリと一緒にコンパイルされている場合，DNN_BACKEND_DEFAULT は DNN_BACKEND_INFERENCE_ENGINE を意味します．それ以外の場合は，DNN_BACKEND_OPENCVになります．
Ask network to make computations on specific target device.,ネットワークに特定のターゲットデバイスでの計算を依頼する。
targetId : [in],targetId : [in].
See alsoTargetList of supported combinations backend / target:DNN_BACKEND_OPENCV DNN_BACKEND_INFERENCE_ENGINE DNN_BACKEND_HALIDE DNN_BACKEND_CUDA  ,参照：サポートされる組み合わせのTargetList backend / target:DNN_BACKEND_OPENCV DNN_BACKEND_INFERENCE_ENGINE DNN_BACKEND_HALIDE DNN_BACKEND_CUDA
DNN_TARGET_CPU + + + ,DNN_ターゲット_CPU + + +
DNN_TARGET_OPENCL + + + ,DNN_Target_opencl + +
DNN_TARGET_OPENCL_FP16 + + ,Dnn_Target_opencl_fp16 + + Dnn_Target_opencl_fp16 + +
DNN_TARGET_MYRIAD + ,Dnn_Target_Myriad + +
DNN_TARGET_FPGA + ,DNN_Target_FPGA + +
DNN_TARGET_CUDA + ,Dnn_Target_cuda +
DNN_TARGET_CUDA_FP16 + ,Dnn_Target_Cuda_fp16 + + Dnn_Target_Cuda_fp16 +
DNN_TARGET_HDDL +Examples: samples/dnn/colorization.cpp.,DNN_TARGET_HDDL +例：samples/dnn/colorization.cpp.
Sets the new input value for the network.,ネットワークの新しい入力値を設定します。
blob : A new blob. Should have CV_32F or CV_8U depth.,blob : 新しい blob．CV_32F または CV_8U の深さを持ちます．
name : A name of input layer.,name : 入力レイヤの名前．
scalefactor : An optional normalization scale.,scalefactor : 任意の正規化スケール．
mean : An optional mean subtraction values.,mean : オプションである平均減算の値．
"See alsoconnect(String, String) to know format of the descriptor.If scale or mean values are specified, a final input blob is computed as:\[input(n,c,h,w) = scalefactor \times (blob(n,c,h,w) - mean_c)\]Examples: samples/dnn/colorization.cpp, and samples/dnn/openpose.cpp.","scale や mean が指定された場合，最終的な入力 blob は次のように計算されます：˶[input(n,c,h,w) = scalefactor ˶[times (blob(n,c,h,w) - mean_c) ˶]例：samples/dn/colorization.cpp, samples/dn/openpose.cpp."
Returns indexes of layers with unconnected outputs.,出力がつながっていないレイヤーのインデックスを返します。
Returns names of layers with unconnected outputs.,出力がつながっていないレイヤーの名前を返します。
Enables or disables layer fusion in the network.,ネットワークのレイヤーフュージョンを有効または無効にします。
"fusion : true to enable the fusion, false to disable. The fusion is enabled by default.",fusion : 融合を有効にするにはtrue，無効にするにはfalseを指定します。融合はデフォルトで有効です。
Returns overall time for inference and timings (in ticks) for layers.,推論にかかる全体の時間と、レイヤーのタイミング（ティック単位）を返します。
timings : [out],timings : [out］
"Indexes in returned vector correspond to layers ids. Some layers can be fused with others, in this case zero ticks count will be return for that skipped layers. Supported by DNN_BACKEND_OPENCV on DNN_TARGET_CPU only.",返されたベクトルのインデックスは，レイヤーのIDに対応する。いくつかのレイヤーは他のレイヤーと融合することができ、その場合、スキップされたレイヤーについては0ティックカウントが返されます。DNN_TARGET_CPU上のDNN_BACKEND_OPENCVでのみサポートされます。
A function to load the trained model before the fitting process.,フィット処理の前に，学習済みモデルをロードする関数です．
model : A string represent the filename of a trained model.,model : 学習したモデルのファイル名を表す文字列．
"Example of usagefacemark->loadModel(""../data/lbf.model"");fragment","使用例efacemark->loadModel(""../data/lbf.model"");fragment"
Detect facial landmarks from an image.,画像から顔のランドマークを検出します。
image : Input image.,image : 入力画像．
faces : Output of the function which represent region of interest of the detected faces. Each face is stored in cv::Rect container.,faces :検出された顔の注目領域を表す関数の出力．各顔は，cv::Rect コンテナに格納されます．
landmarks : The detected landmark points for each faces.,landmarks : 各顔に対して検出されたランドマークポイント．
"Example of usageMat image = imread(""image.jpg"");std::vector<Rect> faces;std::vector<std::vector<Point2f> > landmarks;facemark->fit(image, faces, landmarks);fragment","使用例Mat image = imread(""image.jpg"");std::vector<Rect> faces;std::vector<std::vector<Point2f> > landmarks;facemark->fit(image, faces, landmarks);fragment"
offset for the loaded face landmark points,読み込まれた顔のランドマークポイントのオフセット
filename of the face detector model,顔検出モデルのファイル名
show the training print-out,学習用プリントアウトの表示
number of landmark points,ランドマークポイントの数
multiplier for augment the training data,学習データを補強するための乗数
number of refinement stages,リファインメントのステージ数
number of tree in the model for each landmark point refinement,各ランドマークポイントの絞り込みのためのモデル内の木の数
"the depth of decision tree, defines the size of feature",決定木の深さで、特徴量の大きさを決める
overlap ratio for training the LBF feature,LBF特徴を学習する際のオーバーラップ率
filename where the trained model will be saved,学習されたモデルが保存されるファイル名
flag to save the trained model or not,学習したモデルを保存するかどうかのフラグ
seed for shuffling the training data,学習データをシャッフルするためのシード
initializer,イニシャライザ
"Read parameters from file, currently unused.",ファイルからパラメータを読み込みます、現在は使用されていません。
Trains a FaceRecognizer with given data and associated labels.,与えられたデータとラベルを使って、FaceRecognizerを学習します。
"src : The training images, that means the faces you want to learn. The data has to be given as a vector<Mat>.",src : 学習画像，つまり，学習したい顔です．データは， vector<Mat> として与えられなければいけません．
labels : The labels corresponding to the images have to be given either as a vector<int> or a Mat of type CV_32SC1.,labels : 画像に対応するラベルは，vector<int> または CV_32SC1 型の Mat として与えられます．
"The following source code snippet shows you how to learn a Fisherfaces model on a given set of images. The images are read with imread and pushed into a std::vector<Mat>. The labels of each image are stored within a std::vector<int> (you could also use a Mat of type CV_32SC1). Think of the label as the subject (the person) this image belongs to, so same subjects (persons) should have the same label. For the available FaceRecognizer you don't have to pay any attention to the order of the labels, just make sure same persons have the same label:// holds images and labelsvector<Mat> images;vector<int> labels;// using Mat of type CV_32SC1// Mat labels(number_of_samples, 1, CV_32SC1);// images for first personimages.push_back(imread(""person0/0.jpg"", IMREAD_GRAYSCALE)); labels.push_back(0);images.push_back(imread(""person0/1.jpg"", IMREAD_GRAYSCALE)); labels.push_back(0);images.push_back(imread(""person0/2.jpg"", IMREAD_GRAYSCALE)); labels.push_back(0);// images for second personimages.push_back(imread(""person1/0.jpg"", IMREAD_GRAYSCALE)); labels.push_back(1);images.push_back(imread(""person1/1.jpg"", IMREAD_GRAYSCALE)); labels.push_back(1);images.push_back(imread(""person1/2.jpg"", IMREAD_GRAYSCALE)); labels.push_back(1);fragmentNow that you have read some images, we can create a new FaceRecognizer. In this example I'll create a Fisherfaces model and decide to keep all of the possible Fisherfaces:// Create a new Fisherfaces model and retain all available Fisherfaces,// this is the most common usage of this specific FaceRecognizer://Ptr<FaceRecognizer> model =  FisherFaceRecognizer::create();fragmentAnd finally train it on the given dataset (the face images and labels):// This is the common interface to train all of the available cv::FaceRecognizer// implementations://model->train(images, labels);fragment","以下のソースコードは，与えられた画像群に対して，どのようにFisherfacesモデルを学習するかを示すものです．画像は imread によって読み込まれ，std::vector<Mat> に格納されます．各画像のラベルは，std::vector<int> に格納されます（CV_32SC1 型の Mat を利用することもできます）．ラベルとは，この画像が属する対象（人）のことであり，同じ対象（人）は同じラベルを持つべきです．利用可能な FaceRecognizer では，ラベルの順序を気にする必要はなく，同じ人物が同じラベルを持っていることを確認するだけです： // 画像とラベルを保持しますvector<Mat> images;vector<int> labels;// CV_32SC1 型の Mat を利用します// Mat labels(number_of_samples, 1, CV_32SC1);// 最初の人物に対する画像images.push_back(imread(""person0/0.jpg"", IMREAD_GRAYSCALE)); labels.push_back(0);images.push_back(imread(""person0/1.jpg"", IMREAD_GRAYSCALE)); labels.push_back(0);images.push_back(imread(""person0/2.jpg"", IMREAD_GRAYSCALE)); labels.push_back(0);// 2人目用の画像images.push_back(imread(""person1/0.jpg"", IMREAD_GRAYSCALE)); labels.push_back(1);images.push_back(imread(""person1/1.jpg"", IMREAD_GRAYSCALE)); labels.push_back(1);images.push_back(imread(""person1/2.jpg"", IMREAD_GRAYSCALE)); labels.push_back(1);fragmentさて、いくつかの画像を読み込んだところで、新しいFaceRecognizerを作成してみましょう。この例では、Fisherfaces モデルを作成し、可能なすべての Fisherfaces を保持することにします： // 新しい Fisherfaces モデルを作成し、利用可能なすべての Fisherfaces を保持します、 // これはこの特定の FaceRecognizer の最も一般的な使用法です。//Ptr<FaceRecognizer> model = FisherFaceRecognizer::create();fragmentそして最後に，与えられたデータセット（顔画像とラベル）に対してこれを学習させます： // これは，利用可能な cv::FaceRecognizer// のすべての実装を学習させるための，共通のインタフェースです： //model->train(images, labels);fragment"
Updates a FaceRecognizer with given data and associated labels.,与えられたデータと関連付けられたラベルを用いて，FaceRecognizer を更新します．
"This method updates a (probably trained) FaceRecognizer, but only if the algorithm supports it. The Local Binary Patterns Histograms (LBPH) recognizer (see createLBPHFaceRecognizer) can be updated. For the Eigenfaces and Fisherfaces method, this is algorithmically not possible and you have to re-estimate the model with FaceRecognizer::train. In any case, a call to train empties the existing model and learns a new model, while update does not delete any model data.// Create a new LBPH model (it can be updated) and use the default parameters,// this is the most common usage of this specific FaceRecognizer://Ptr<FaceRecognizer> model =  LBPHFaceRecognizer::create();// This is the common interface to train all of the available cv::FaceRecognizer// implementations://model->train(images, labels);// Some containers to hold new image:vector<Mat> newImages;vector<int> newLabels;// You should add some images to the containers://// ...//// Now updating the model is as easy as calling:model->update(newImages,newLabels);// This will preserve the old model data and extend the existing model// with the new features extracted from newImages!fragmentCalling update on an Eigenfaces model (see EigenFaceRecognizer::create), which doesn't support updating, will throw an error similar to:OpenCV Error: The function/feature is not implemented (This FaceRecognizer (FaceRecognizer.Eigenfaces) does not support updating, you have to use FaceRecognizer::train to update it.) in update, file /home/philipp/git/opencv/modules/contrib/src/facerec.cpp, line 305terminate called after throwing an instance of 'cv::Exception'fragmentNoteThe FaceRecognizer does not store your training images, because this would be very memory intense and it's not the responsibility of te FaceRecognizer to do so. The caller is responsible for maintaining the dataset, he want to work with.","このメソッドは，（おそらく学習済みの）FaceRecognizer を更新しますが，アルゴリズムがそれをサポートしている場合に限ります．LBPH（Local Binary Patterns Histograms）認識器（createLBPHFaceRecognizerを参照）を更新することができます。EigenfacesとFisherfacesメソッドの場合、これはアルゴリズム的に可能ではなく、FaceRecognizer::trainでモデルを再推定する必要があります。いずれにしても、train の呼び出しは既存のモデルを空にして新しいモデルを学習しますが、update はモデルデータを削除しません。// 新しい LBPH モデルを作成し（更新可能）、デフォルトのパラメータを使用します // これは、この特定の FaceRecognizer の最も一般的な使い方です： //Ptr<FaceRecognizer> model = LBPHFaceRecognizer::create();// これは、利用可能なすべての cv を学習するための共通のインタフェースです。:FaceRecognizer// implementations://model->train(images, labels);// 新しい画像を格納するいくつかのコンテナ:vector<Mat> newImages;vector<int> newLabels;// コンテナに画像を追加する必要があります://// ...//// ここで，モデルを更新します．...//// モデルの更新は，呼び出すだけで簡単に行えます:model->update(newImages,newLabels);// これにより，古いモデルデータが保存され，既存のモデル//を newImages から抽出された新しい特徴で拡張します！fragment更新をサポートしない Eigenfaces モデル（EigenFaceRecognizer::create を参照）に対して update を呼び出すと，以下のようなエラーが発生します：OpenCV Error:The function/feature is not implemented (This FaceRecognizer (FaceRecognizer.Eigenfaces) does not support updating, you must have to use FaceRecognizer::train to update it.) in update, file /home/philipp/git/opencv/modules/contrib/src/facerec.cpp, line 305terminate called after throwing an instance of 'cv::Exception'fragmentNote FaceRecognizer は，あなたのトレーニング画像を保存しません．それは，非常にメモリを消費するからであり，また，そうすることは FaceRecognizer の責任ではないからです．呼び出し側は、作業したいデータセットを維持する責任があります。"
Predicts a label and associated confidence (e.g. distance) for a given input image.,与えられた入力画像に対して，ラベルとそれに伴う信頼度（距離など）を予測します．
src : Sample image to get a prediction from.,src : 予測結果を得るためのサンプル画像．
label : The predicted label for the given image.,label : 与えられた画像に対して，予測されるラベル．
confidence : Associated confidence (e.g. distance) for the predicted label.,confidence : 予測されたラベルに関連付けられた信頼度（例えば，距離）．
"The suffix const means that prediction does not affect the internal model state, so the method can be safely called from within different threads.The following example shows how to get a prediction from a trained model:using namespace cv;// Do your initialization here (create the cv::FaceRecognizer model) ...// ...// Read in a sample image:Mat img = imread(""person1/3.jpg"", IMREAD_GRAYSCALE);// And get a prediction from the cv::FaceRecognizer:int predicted = model->predict(img);fragmentOr to get a prediction and the associated confidence (e.g. distance):using namespace cv;// Do your initialization here (create the cv::FaceRecognizer model) ...// ...Mat img = imread(""person1/3.jpg"", IMREAD_GRAYSCALE);// Some variables for the predicted label and associated confidence (e.g. distance):int predicted_label = -1;double predicted_confidence = 0.0;// Get the prediction and associated confidence from the modelmodel->predict(img, predicted_label, predicted_confidence);fragment","接尾辞 const は，予測値がモデルの内部状態に影響を与えないことを意味します．したがって，このメソッドは，異なるスレッドから安全に呼び出すことができます．次の例は，学習済みモデルから予測値を得る方法を示しています： using namespace cv;// ここで初期化を行います（cv::FaceRecognizer モデルを作成します）．...// ...// サンプル画像を読み込みます： Mat img = imread(""person1/3.jpg"", IMREAD_GRAYSCALE);// そして， cv::FaceRecognizer から予測値を取得します： int predicted = model->predict(img);fragmentあるいは，予測値とそれに関連する信頼度（例えば，距離）を取得します．例えば，距離）:using namespace cv;// ここで初期化を行います（cv::FaceRecognizer モデルを作成します） ...// ...Mat img = imread(""person1/3.jpg"", IMREAD_GRAYSCALE);// 予測されたラベルとそれに関連する信頼度（例えば，距離）のためのいくつかの変数．modelel->predict(img, predicted_label, predicted_confidence);fragment"
Saves a FaceRecognizer and its model state.,FaceRecognizerとそのモデルの状態を保存します。
filename : The filename to store this FaceRecognizer to (either XML/YAML).,filename : このFaceRecognizerを保存するファイル名（XML/YAMLのいずれか）。
"Saves this model to a given filename, either as XML or YAML.Every FaceRecognizer overwrites FaceRecognizer::save(FileStorage& fs) to save the internal model state. FaceRecognizer::save(const String& filename) saves the state of a model to the given filename.The suffix const means that prediction does not affect the internal model state, so the method can be safely called from within different threads.",すべての FaceRecognizer は、内部モデルの状態を保存するために FaceRecognizer::save(FileStorage& fs) を上書きします。FaceRecognizer::save(const String& filename) は、与えられたファイル名にモデルの状態を保存します。接尾辞 const は、予測が内部モデルの状態に影響しないことを意味し、このメソッドは異なるスレッド内から安全に呼び出すことができます。
Loads a FaceRecognizer and its model state.,FaceRecognizerとそのモデルの状態を読み込みます。
"Loads a persisted model and state from a given XML or YAML file . Every FaceRecognizer has to overwrite FaceRecognizer::load(FileStorage& fs) to enable loading the model state. FaceRecognizer::load(FileStorage& fs) in turn gets called by FaceRecognizer::load(const String& filename), to ease saving a model.",与えられたXMLまたはYAMLファイルから、永続化されたモデルと状態を読み込みます。すべての FaceRecognizer はモデル状態をロードするために FaceRecognizer::load(FileStorage& fs) を上書きしなければなりません。FaceRecognizer::load(FileStorage& fs) は、モデルの保存を容易にするために FaceRecognizer::load(const String& filename) によって呼び出されます。
Sets string info for the specified model's label.,指定されたモデルのラベルに文字列情報を設定します。
The string info is replaced by the provided value if it was set before for the specified label.,文字列情報は、指定されたラベルに以前に設定されていた場合は、指定された値で置き換えられます。
Gets string information by label.,ラベルの文字列情報を取得します。
If an unknown label id is provided or there is no label information associated with the specified label id the method returns an empty string.,未知のラベルIDが指定された場合や、指定されたラベルIDに関連するラベル情報がない場合、このメソッドは空の文字列を返します。
Gets vector of labels by string.,文字列によるラベルのベクトルを取得します。
The function searches for the labels containing the specified sub-string in the associated string info.,この関数は、関連付けられた文字列情報の中で、指定されたサブ文字列を含むラベルを検索します。
threshold parameter accessor - required for default BestMinDist collector,threshold パラメータアクセッサ - デフォルトの BestMinDist コレクターに必要です．
"Implemented in cv::face::LBPHFaceRecognizer, and cv::face::BasicFaceRecognizer.","cv::face::LBPHFaceRecognizer, および cv::face::BasicFaceRecognizer で実装されています．"
Sets threshold of model.,モデルの閾値を設定します．
See alsosetNumComponents,alsosetNumComponents を参照してください．
See alsogetNumComponents,alsogetNumComponents を参照してください．
See alsosetThreshold,alsosetThreshold を参照してください．
Implements cv::face::FaceRecognizer.,cv::face::FaceRecognizer をインプリメントします．
See alsogetThreshold,alsogetThreshold を参照してください．
Notes:,備考
"num_components : The number of components (read: Eigenfaces) kept for this Principal Component Analysis. As a hint: There's no rule how many components (read: Eigenfaces) should be kept for good reconstruction capabilities. It is based on your input data, so experiment with the number. Keeping 80 components should almost always be sufficient.",num_components :この主成分分析で保持する成分（固有顔）の数．ヒント：良い再構成機能のために、いくつの成分（固有顔）を保持すべきかという決まりはありません。入力データに応じて、その数を変えてみてください。ほとんどの場合、80個の成分を保持すれば十分でしょう。
threshold : The threshold applied in the prediction.,threshold : 予測時に適用される閾値です。
"Training and prediction must be done on grayscale images, use cvtColor to convert between the color spaces.",学習と予測は，グレースケール画像に対して行う必要があります．色空間間の変換には cvtColor を利用します．
"THE EIGENFACES METHOD MAKES THE ASSUMPTION, THAT THE TRAINING AND TEST IMAGES ARE OF EQUAL SIZE. (caps-lock, because I got so many mails asking for this). You have to make sure your input data has the correct shape, else a meaningful exception is thrown. Use resize to resize the images.","eigenfacesメソッドは，学習画像とテスト画像が同じサイズであることを仮定しています．(caps-lock, なぜなら、多くのメールでこのことを尋ねられたからです)。入力データが正しい形であることを確認しなければなりません。さもなければ、意味のある例外が投げられます。画像のサイズを変更するには resize を使います。"
This model does not support updating.Model internal data:num_components see EigenFaceRecognizer::create.,Model internal data:num_components see EigenFaceRecognizer::create.
threshold see EigenFaceRecognizer::create.,threshold EigenFaceRecognizer::createを参照。
eigenvalues The eigenvalues for this Principal Component Analysis (ordered descending).,eigenvalues この主成分分析の固有値（降順）。
eigenvectors The eigenvectors for this Principal Component Analysis (ordered by their eigenvalue).,固有ベクトル この主成分分析の固有ベクトル（固有値の順）。
mean The sample mean calculated from the training data.,mean 訓練データから計算された標本平均．
projections The projections of the training data.,projections 学習データの投影結果です。
"labels The threshold applied in the prediction. If the distance to the nearest neighbor is larger than the threshold, this method returns -1.",labels 予測に適用される閾値です。最近傍距離が閾値よりも大きい場合、このメソッドは-1を返します。
"num_components : The number of components (read: Fisherfaces) kept for this Linear Discriminant Analysis with the Fisherfaces criterion. It's useful to keep all components, that means the number of your classes c (read: subjects, persons you want to recognize). If you leave this at the default (0) or set it to a value less-equal 0 or greater (c-1), it will be set to the correct number (c-1) automatically.",num_components :Fisherfaces基準を用いたこの線形判別分析で保持する成分（フィッシャーフェイスと読みます）の数です。すべての成分、つまりクラスc（被験者、認識したい人物など）の数を保持しておくと便利です。これをデフォルト（0）のままにしておくか、0以下の値（c-1）に設定すると、自動的に正しい数（c-1）に設定されます。
"threshold : The threshold applied in the prediction. If the distance to the nearest neighbor is larger than the threshold, this method returns -1.",threshold : 予測時に適用される閾値です。直近の隣人との距離が閾値よりも大きい場合、このメソッドは-1を返します。
"THE FISHERFACES METHOD MAKES THE ASSUMPTION, THAT THE TRAINING AND TEST IMAGES ARE OF EQUAL SIZE. (caps-lock, because I got so many mails asking for this). You have to make sure your input data has the correct shape, else a meaningful exception is thrown. Use resize to resize the images.",fisherfacesメソッドは，トレーニング画像とテスト画像が同じサイズであることを仮定しています．(この方法は、訓練画像とテスト画像のサイズが等しいことを前提としています。）入力データが正しい形であることを確認しなければなりません。さもなければ、意味のある例外が投げられます。画像のサイズを変更するには resize を使います。
This model does not support updating.Model internal data:num_components see FisherFaceRecognizer::create.,Model internal data:num_components see FisherFaceRecognizer::create.
threshold see FisherFaceRecognizer::create.,threshold FisherFaceRecognizer::createを参照。
eigenvalues The eigenvalues for this Linear Discriminant Analysis (ordered descending).,eigenvalues この線形判別分析の固有値（降順）。
eigenvectors The eigenvectors for this Linear Discriminant Analysis (ordered by their eigenvalue).,eigenvectors この線形判別分析の固有ベクトル（固有値の順）。
labels The labels corresponding to the projections.,labels 射影に対応するラベルです。
"radius : The radius used for building the Circular Local Binary Pattern. The greater the radius, the smoother the image but more spatial information you can get.",radius : 円形ローカルバイナリパターンの構築に使用される半径です。半径が大きいほど、画像は滑らかになりますが、より多くの空間情報を得ることができます。
"neighbors : The number of sample points to build a Circular Local Binary Pattern from. An appropriate value is to use 8 sample points. Keep in mind: the more sample points you include, the higher the computational cost.",neighbors :Circular Local Binary Patternを構築するサンプルポイントの数です。8個のサンプルポイントを使用するのが適切な値です。サンプルポイントの数が多いほど、計算コストが高くなることに注意してください。
"grid_x : The number of cells in the horizontal direction, 8 is a common value used in publications. The more cells, the finer the grid, the higher the dimensionality of the resulting feature vector.",grid_x : 水平方向のセル数を指定します。セル数が多ければ多いほど、グリッドが細かくなり、結果として得られる特徴ベクトルの次元数が高くなります。
"grid_y : The number of cells in the vertical direction, 8 is a common value used in publications. The more cells, the finer the grid, the higher the dimensionality of the resulting feature vector.",grid_y :垂直方向のセル数，出版物では8が一般的に使われている値です．セル数が多ければ多いほど，グリッドが細かくなり，結果として得られる特徴ベクトルの次元が高くなります．
"The Circular Local Binary Patterns (used in training and prediction) expect the data given as grayscale images, use cvtColor to convert between the color spaces.",Circular Local Binary Patterns（学習および予測に利用される）は，グレースケール画像として与えられたデータを想定しており，色空間間の変換には cvtColor を利用します．
This model supports updating.Model internal data:radius see LBPHFaceRecognizer::create.,このモデルは，更新をサポートします．モデル内部データ： radius は LBPHFaceRecognizer::create を参照してください．
neighbors see LBPHFaceRecognizer::create.,neighbors は，LBPHFaceRecognizer::create を参照してください．
grid_x see LLBPHFaceRecognizer::create.,grid_xはLBPHFaceRecognizer::createを参照。
grid_y see LBPHFaceRecognizer::create.,grid_yはLBPHFaceRecognizer::createを参照。
threshold see LBPHFaceRecognizer::create.,threshold LBPHFaceRecognizer::createをご参照ください。
histograms Local Binary Patterns Histograms calculated from the given training data (empty if none was given).,histograms 与えられた学習データから算出されたローカルバイナリパターンのヒストグラム（何も与えられていない場合は空）。
labels Labels corresponding to the calculated Local Binary Patterns Histograms.,Labels 計算されたローカルバイナリパターンヒストグラムに対応するラベル。
See alsosetGridX,alsosetGridX を参照してください。
See alsogetGridX,alsogetGridXを参照。
See alsosetGridY,alsosetGridYを見る
See alsogetGridY,alsogetGridYを参照。
See alsosetRadius,See alsosetRadius
See alsogetRadius,alsogetRadiusを参照。
See alsosetNeighbors,See alsogetNeighbors
See alsogetNeighbors,alsogetNeighbors参照
Draws keypoints.,キーポイントを描画します．
image : Source image.,image : ソース画像．
keypoints : Keypoints from the source image.,keypoints :元画像からのキーポイント．
outImage : Output image. Its content depends on the flags value defining what is drawn in the output image. See possible flags bit values below.,outImage : 出力画像．その内容は，出力画像に何が描画されるかを定義する flags の値に依存します．以下の flags ビット値の例を参照してください．
color : Color of keypoints.,color : キーポイントの色を指定します．
flags : Flags setting drawing features. Possible flags bit values are defined by DrawMatchesFlags. See details above in drawMatches .,flags : キーポイントの色．描画機能を設定するFlags．可能な flags ビット値は， DrawMatchesFlags によって定義されます．詳細は，上記の drawMatches を参照してください．
"NoteFor Python API, flags are modified as cv.DRAW_MATCHES_FLAGS_DEFAULT, cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS, cv.DRAW_MATCHES_FLAGS_DRAW_OVER_OUTIMG, cv.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS","注意 Python API の場合， flags は cv.DRAW_MATCHES_FLAGS_DEFAULT, cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS, cv.DRAW_MATCHES_FLAGS_DRAW_OVER_OUTIMG, cv.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS のように変更されます．"
Draws the found matches of keypoints from two images.,2つの画像からキーポイントの一致を見つけて，それを描画します．
img1 : First source image.,img1 : 1枚目の入力画像．
keypoints1 : Keypoints from the first source image.,keypoints1 : 1枚目のソース画像からのキーポイント。
img2 : Second source image.,img2 : 2枚目の元画像。
keypoints2 : Keypoints from the second source image.,keypoints2 : 2枚目の画像から得られたキーポイント．
"matches1to2 : Matches from the first image to the second one, which means that keypoints1[i] has a corresponding point in keypoints2[matches[i]] .",matches1to2 : 1枚目の画像から2枚目の画像へのマッチ．つまり，keypoints1[i]がkeypoints2[matches[i]]内に対応する点を持つことを意味します．
outImg : Output image. Its content depends on the flags value defining what is drawn in the output image. See possible flags bit values below.,outImg : 出力画像．その内容は，出力画像に何を描画するかを定義する flags の値に依存します．以下の，可能な flags ビット値を参照してください．
"matchColor : Color of matches (lines and connected keypoints). If matchColor==Scalar::all(-1) , the color is generated randomly.",matchColor : マッチ（線と接続されたキーポイント）の色．matchColor==Scalar::all(-1) の場合，色はランダムに生成されます．
"singlePointColor : Color of single keypoints (circles), which means that keypoints do not have the matches. If singlePointColor==Scalar::all(-1) , the color is generated randomly.",singlePointColor : 単一のキーポイント（円）の色，つまりキーポイントにマッチがないことを意味します．singlePointColor==Scalar::all(-1) の場合，この色はランダムに生成されます．
"matchesMask : Mask determining which matches are drawn. If the mask is empty, all matches are drawn.",matchesMask : どのマッチを描画するかを決めるマスク．このマスクが空の場合，すべてのマッチが描画されます．
flags : Flags setting drawing features. Possible flags bit values are defined by DrawMatchesFlags.,flags :描画機能を設定するフラグ．flags のビット値は， DrawMatchesFlags で定義されます．
This function draws matches of keypoints from two images in the output image. Match is a line connecting two keypoints (circles). See cv::DrawMatchesFlags.,この関数は，2つの画像のキーポイント同士のマッチを，出力画像に描画します．マッチとは，2つのキーポイント（円）を結ぶ線のことです．cv::DrawMatchesFlags を参照してください．
Adds descriptors to a training set.,トレーニングセットにディスクリプタを追加します．
descriptors : Descriptors to add to a training set. Each row of the descriptors matrix is a descriptor.,descriptors :学習セットに追加するディスクリプタ．descriptors 行列の各行が，ディスクリプタになります．
The training set is clustered using clustermethod to construct the vocabulary.,学習セットは，clustermethod を用いてクラスタリングされ，語彙が構築されます．
Returns a training set of descriptors.,ディスクリプタのトレーニングセットを返します．
Returns the count of all descriptors stored in the training set.,学習セットに格納されている，すべての記述子の数を返します．
kmeans -based class to train visual vocabulary using the bag of visual words approach. : ,kmeans - based class を用いて，視覚的単語の袋（bag of visual words）アプローチで視覚的語彙を学習します．
Clusters train descriptors.,クラスタは，ディスクリプタを学習します．
descriptors : Descriptors to cluster. Each row of the descriptors matrix is a descriptor. Descriptors are not added to the inner train descriptor set.,descriptors :クラスタ化する記述子．descriptors 行列の各行は，ディスクリプタです．ディスクリプタは，内部の訓練ディスクリプタセットには追加されません．
"The vocabulary consists of cluster centers. So, this method returns the vocabulary. In the first variant of the method, train descriptors stored in the object are clustered. In the second variant, input descriptors are clustered.Implements cv::BOWTrainer.",語彙は，クラスタセンターから構成されます．そのため，このメソッドは，語彙を返します．このメソッドの第1のバージョンでは，オブジェクトに格納された訓練ディスクリプタがクラスタリングされます．第2のバージョンでは，入力ディスクリプタがクラスタリングされます． Implements cv::BOWTrainer.
Class to compute an image descriptor using the bag of visual words. ,視覚的単語の袋を用いて画像ディスクリプタを計算するために， cv::BOWTrainer.Class を実装します．
Such a computation consists of the following steps:,この計算は，以下のステップで構成されます．
Compute descriptors for a given image and its keypoints set.,与えられた画像とそのキーポイントセットに対するディスクリプタを計算します．
Find the nearest visual words from the vocabulary for each keypoint descriptor.,各キーポイントディスクリプタに対して，語彙から最も近い視覚的な単語を見つけます．
Compute the bag-of-words image descriptor as is a normalized histogram of vocabulary words encountered in the image. The i-th bin of the histogram is a frequency of i-th word of the vocabulary in the given image. ,画像中に現れる語彙の正規化されたヒストグラムである，bag-of-words画像記述子を計算します．ヒストグラムのi番目のビンは，与えられた画像における語彙のi番目の単語の頻度を表します．
Sets a visual vocabulary.,視覚的な語彙を設定します．
vocabulary : Vocabulary (can be trained using the inheritor of BOWTrainer ). Each row of the vocabulary is a visual word (cluster center).,vocabulary : 語彙（BOWTrainerの継承機能を用いて学習することができます）．語彙の各行は，視覚的な単語（クラスタの中心）を表します．
Returns the set vocabulary.,設定された語彙を返します．
Computes an image descriptor using the set visual vocabulary.,set visual vocabulary を用いて，画像ディスクリプタを計算します．
"image : Image, for which the descriptor is computed.",image : ディスクリプタが計算される画像．
keypoints : Keypoints detected in the input image.,keypoints :入力画像から検出されたキーポイント．
imgDescriptor : Computed output image descriptor.,imgDescriptor : 計算された出力画像のディスクリプタ．
pointIdxsOfClusters : Indices of keypoints that belong to the cluster. This means that pointIdxsOfClusters[i] are keypoint indices that belong to the i -th cluster (word of vocabulary) returned if it is non-zero.,pointIdxsOfClusters : クラスタに属するキーポイントのインデックス．つまり， pointIdxsOfClusters[i] は，非0の場合に返される，i番目のクラスタ（語彙の単語）に属するキーポイントのインデックスです．
descriptors : Descriptors of the image keypoints that are returned if they are non-zero.,descriptors :non-zeroの場合に返される，画像キーポイントのディスクリプタ．
keypointDescriptors : Computed descriptors to match with vocabulary.,keypointDescriptors :語彙と一致するように計算されたディスクリプタ．
"Returns an image descriptor size if the vocabulary is set. Otherwise, it returns 0.",vocabularyが設定されている場合は，画像ディスクリプタのサイズを返します．そうでない場合は，0を返します．
Returns an image descriptor type.,画像ディスクリプタのタイプを返します．
Adds descriptors to train a CPU(trainDescCollectionis) or GPU(utrainDescCollectionis) descriptor collection.,CPU(trainDescCollectionis) または GPU(utrainDescCollectionis) のディスクリプタコレクションを学習するために，ディスクリプタを追加します．
descriptors : Descriptors to add. Each descriptors[i] is a set of descriptors from the same train image.,descriptors :追加する記述子．各descriptors[i]は，同じ訓練画像から得られたディスクリプタの集合です．
"If the collection is not empty, the new descriptors are added to existing train descriptors.Reimplemented in cv::FlannBasedMatcher.",コレクションが空ではない場合，新しいディスクリプタは，既存の訓練ディスクリプタに追加されます．cv::FlannBasedMatcherで再実装されました．
Returns a constant link to the train descriptor collection trainDescCollection .,列車ディスクリプタコレクション trainDescCollection への定数リンクを返します．
Clears the train descriptor collections.,列車ディスクリプタコレクションをクリアします．
Reimplemented from cv::Algorithm.Reimplemented in cv::FlannBasedMatcher.,cv::Algorithm.Reimplemented in cv::FlannBasedMatcher.Reimplemented from cv::Algorithm.Reimplemented in cv::FlannBasedMatcher.
Returns true if there are no train descriptors in the both collections.,両方のコレクションに列車ディスクリプタが存在しない場合は，真を返します．
Reimplemented from cv::Algorithm.,cv::Algorithm を再実装したものです．
Returns true if the descriptor matcher supports masking permissible matches.,ディスクリプタ Matcher が，許可されたマッチのマスキングをサポートする場合に true を返します．
"Implemented in cv::FlannBasedMatcher, and cv::BFMatcher.","cv::FlannBasedMatcher, および cv::BFMatcher で実装されています．"
Trains a descriptor matcher.,ディスクリプタ Matcherを学習します．
"Trains a descriptor matcher (for example, the flann index). In all methods to match, the method train() is run every time before matching. Some descriptor matchers (for example, BruteForceMatcher) have an empty implementation of this method. Other matchers really train their inner structures (for example, FlannBasedMatcher trains flann::Index ).Reimplemented in cv::FlannBasedMatcher.",ディスクリプタ Matcherを学習します（例えば，flannインデックス）．マッチングを行うすべてのメソッドにおいて，マッチングの前に毎回 train() が実行されます．一部のディスクリプタ Matcher（例えば，BruteForceMatcher）は，このメソッドの実装が空です．その他の Matcher は，実際に内部構造を学習します（例えば， FlannBasedMatcher は flann::Index を学習します）．
Finds the best match for each descriptor from a query set.,クエリセットから，各ディスクリプタに最もマッチするものを見つけます．
queryDescriptors : Query set of descriptors.,queryDescriptors :ディスクリプタのクエリセット．
trainDescriptors : Train set of descriptors. This set is not added to the train descriptors collection stored in the class object.,trainDescriptors :ディスクリプタの訓練セット．この集合は，クラスオブジェクトに格納されている訓練ディスクリプタコレクションには追加されません．
"matches : Matches. If a query descriptor is masked out in mask , no match is added for this descriptor. So, matches size may be smaller than the query descriptors count.",matches : 一致するもの．クエリディスクリプタが mask でマスクアウトされている場合，このディスクリプタに対するマッチは追加されません．そのため， matches のサイズは，クエリディスクリプタの数よりも小さくなる可能性があります．
mask : Mask specifying permissible matches between an input query and train matrices of descriptors.,mask : 入力されたクエリと訓練ディスクリプタの行列との間の，許容されるマッチを指定するマスク．
"In the first variant of this method, the train descriptors are passed as an input argument. In the second variant of the method, train descriptors collection that was set by DescriptorMatcher::add is used. Optional mask (or masks) can be passed to specify which query and training descriptors can be matched. Namely, queryDescriptors[i] can be matched with trainDescriptors[j] only if mask.at<uchar>(i,j) is non-zero.","このメソッドの第1のバージョンでは，訓練ディスクリプタが入力引数として渡されます．このメソッドの第2のバージョンでは， DescriptorMatcher::add によってセットされた訓練ディスクリプタコレクションが利用されます．オプションの mask （複数可）を渡すことで，どのクエリと訓練ディスクリプタをマッチングさせるかを指定できます．つまり， mask.at<uchar>(i,j) が0ではない場合にのみ， queryDescriptors[i] と trainDescriptors[j] がマッチングされます．"
Finds the k best matches for each descriptor from a query set.,クエリセットから各ディスクリプタに対して，k個のベストマッチを見つけます．
matches : Matches. Each matches[i] is k or less matches for the same query descriptor.,matches : マッチ．各 matches[i] は，同じクエリディスクリプタに対する k 個以下のマッチである．
k : Count of best matches found per each query descriptor or less if a query descriptor has less than k possible matches in total.,k : 各クエリ記述子ごとに見つかったベストマッチの数，またはクエリ記述子のマッチ可能数が合計で k 個以下の場合はそれ以下．
"compactResult : Parameter used when the mask (or masks) is not empty. If compactResult is false, the matches vector has the same size as queryDescriptors rows. If compactResult is true, the matches vector does not contain matches for fully masked-out query descriptors.",compactResult : マスク（複数可）が空ではない場合に使用されるパラメータ．compactResult が false の場合， matches vector は queryDescriptors の行数と同じサイズになります．compactResult が true の場合， matches vector には，完全にマスクアウトされたクエリディスクリプタに対するマッチは含まれません．
These extended variants of DescriptorMatcher::match methods find several best matches for each query descriptor. The matches are returned in the distance increasing order. See DescriptorMatcher::match for the details about query and train descriptors.,これらの DescriptorMatcher::match メソッドの拡張版は，各クエリディスクリプタに対して，複数のベストマッチを見つけます．これらのマッチは，距離の昇順で返されます．クエリディスクリプタと訓練ディスクリプタの詳細については， DescriptorMatcher::match を参照してください．
"For each query descriptor, finds the training descriptors not farther than the specified distance.",各クエリディスクリプタに対して，指定された距離よりも遠くないトレーニングディスクリプタを見つけます．
matches : Found matches.,matches : 見つかったマッチ。
"maxDistance : Threshold for the distance between matched descriptors. Distance means here metric distance (e.g. Hamming distance), not the distance between coordinates (which is measured in Pixels)!",maxDistance : マッチしたディスクリプタ間の距離の閾値．ここでいう距離とは，座標間の距離（これはピクセル単位で計測されます）ではなく，メトリックな距離（例えば，ハミング距離）を意味します
"For each query descriptor, the methods find such training descriptors that the distance between the query descriptor and the training descriptor is equal or smaller than maxDistance. Found matches are returned in the distance increasing order.",各クエリディスクリプタに対して，メソッドは，クエリディスクリプタとトレーニングディスクリプタの間の距離が maxDistance と同等かそれよりも小さくなるようなトレーニングディスクリプタを見つけます．検索されたマッチは，距離が大きい順に返されます．
masks : Set of masks. Each masks[i] specifies permissible matches between the input query descriptors and stored train descriptors from the i-th image trainDescCollection[i].,masks :masks : マスクの集合．各 masks[i] は，入力されたクエリディスクリプタと，i番目の画像 trainDescCollection[i] から保存された訓練ディスクリプタとの間の許容されるマッチングを指定する．
Creates a descriptor matcher of a given type with the default parameters (using default constructor).,与えられたタイプのディスクリプタ Matcher を，デフォルトのパラメータ（デフォルトコンストラクタを利用）で作成します．
descriptorMatcherType : Descriptor matcher type. Now the following matcher types are supported:,descriptorMatcherType :ディスクリプタ・マッチャータイプ．現在，以下のマッチャータイプがサポートされています．
BruteForce (it uses L2 ),BruteForce（L2 を利用します）．
BruteForce-L1,BruteForce-L1
BruteForce-Hamming,BruteForce-Hamming
BruteForce-Hamming(2),BruteForce-Hamming(2)
FlannBased,FlannBased
Abstract base class for matching keypoint descriptors. ,キーポイントのディスクリプタをマッチングするための抽象的な基底クラス．
It has two groups of match methods: for matching descriptors of an image with another image or with an image set. ,これには2つのマッチングメソッドがあり，画像のディスクリプタを別の画像や画像セットとマッチングさせることができます．
Brute-force descriptor matcher. ,ブルートフォースディスクリプタ Matcher．
"For each descriptor in the first set, this matcher finds the closest descriptor in the second set by trying each one. This descriptor matcher supports masking permissible matches of descriptor sets. ",この Matcherは，第1の集合の各ディスクリプタに対して，第2の集合の中で最も近いディスクリプタを，それぞれ試しながら見つけます．このディスクリプタ Matcherは，ディスクリプタ集合の許容されるマッチングのマスクをサポートします．
Implements cv::DescriptorMatcher.,cv::DescriptorMatcher を実装しています．
Flann-based descriptor matcher. ,Flann ベースのディスクリプタ Matcher．
"This matcher trains cv::flann::Index on a train descriptor collection and calls its nearest search methods to find the best matches. So, this matcher may be faster when matching a large train collection than the brute force matcher. FlannBasedMatcher does not support masking permissible matches of descriptor sets because flann::Index does not support this. : ",この Matcherは，訓練ディスクリプタ集合に対して cv::flann::Index を学習させ，最適なマッチを見つけるためにその最近接探索メソッドを呼び出します．そのため，大規模な訓練コレクションをマッチングする場合，このマッチャは，ブルートフォース・マッチャよりも高速になる可能性があります．flann::Index がサポートしていないので，FlannBasedMatcher は，ディスクリプタ集合の許容されるマッチをマスクすることをサポートしません．
"If the collection is not empty, the new descriptors are added to existing train descriptors.Reimplemented from cv::DescriptorMatcher.",コレクションが空ではない場合，新しいディスクリプタは，既存の訓練ディスクリプタに追加されます．
Reimplemented from cv::DescriptorMatcher.,cv::DescriptorMatcher を再実装したものです．
"Trains a descriptor matcher (for example, the flann index). In all methods to match, the method train() is run every time before matching. Some descriptor matchers (for example, BruteForceMatcher) have an empty implementation of this method. Other matchers really train their inner structures (for example, FlannBasedMatcher trains flann::Index ).Reimplemented from cv::DescriptorMatcher.",ディスクリプタ Matcherを学習します（例えば，flannインデックス）．マッチングを行うすべてのメソッドにおいて，マッチングの前に毎回 train() が実行されます．一部のディスクリプタ Matcher（例えば，BruteForceMatcher）は，このメソッドの実装が空です．その他の Matcher は，実際に内部構造を学習します（例えば， FlannBasedMatcher は flann::Index を学習します）． cv::DescriptorMatcher からの再実装．
Detects keypoints in an image (first variant) or image set (second variant).,画像（第1の形式）または画像集合（第2の形式）からキーポイントを検出します．
image : Image.,image : 画像．
keypoints : The detected keypoints. In the second variant of the method keypoints[i] is a set of keypoints detected in images[i] .,keypoints :検出されたキーポイント．このメソッドの第2のバージョンでは，keypoints[i]は，images[i]から検出されたキーポイントの集合になります．
mask : Mask specifying where to look for keypoints (optional). It must be a 8-bit integer matrix with non-zero values in the region of interest.,mask : キーポイントを探す場所を指定するマスク（オプション）．これは，8ビットの整数行列でなければならず，関心領域内では非0の値を持ちます．
Computes the descriptors for a set of keypoints detected in an image (first variant) or image set (second variant).,画像（第1の形式）または画像集合（第2の形式）から検出されたキーポイントの集合に対するディスクリプタを計算します．
"keypoints : Input collection of keypoints. Keypoints for which a descriptor cannot be computed are removed. Sometimes new keypoints can be added, for example: SIFT duplicates keypoint with several dominant orientations (for each orientation).",keypoints :キーポイントの入力コレクション．ディスクリプタが計算できないキーポイントは削除されます．時には，新しいキーポイントが追加されることもあります．SIFTは，複数の優勢な方向を持つキーポイントを複製します（各方向に対して）．
descriptors : Computed descriptors. In the second variant of the method descriptors[i] are descriptors computed for a keypoints[i]. Row j is the keypoints (or keypoints[i]) is the descriptor for keypoint j-th keypoint.,ディスクリプタ．計算されたディスクリプタ．この手法の第2のバージョンでは，descriptors[i]は，keypoints[i]に対して計算されたディスクリプタです．j行は，keypoints（または keypoints[i] ）が，j番目のキーポイントに対するディスクリプタです．
Reimplemented in cv::xfeatures2d::DAISY.,cv::xfeatures2d::DAISYで再実装されています．
Detects keypoints and computes the descriptors,キーポイントを検出し，ディスクリプタを計算します．
Return true if detector object is empty.,検出器オブジェクトが空の場合は，trueを返します．
Returns the algorithm string identifier. This string is used as top level xml/yml node tag when the object is saved to a file or string.,アルゴリズムの文字列識別子を返します．この文字列は，オブジェクトがファイルや文字列に保存される際に，xml/yml のトップレベルノードタグとして利用されます．
"Reimplemented from cv::Algorithm.Reimplemented in cv::AKAZE, cv::KAZE, cv::SimpleBlobDetector, cv::GFTTDetector, cv::AgastFeatureDetector, cv::FastFeatureDetector, cv::MSER, cv::ORB, cv::BRISK, cv::SIFT, and cv::AffineFeature.","cv::Algorithm.Reimplemented from cv::AKAZE, cv::KAZE, cv::SimpleBlobDetector, cv::GFTTDetector, cv::AgastFeatureDetector, cv::FastFeatureDetector, cv::MSER, cv::ORB, cv::BRISK, cv::SIFT, and cv::AffineFeature."
"NoteThe contrast threshold will be divided by nOctaveLayers when the filtering is applied. When nOctaveLayers is set to default and if you want to use the value used in D. Lowe paper, 0.03, set this argument to 0.09.",注意 フィルタリングが適用されると，コントラスト閾値は nOctaveLayers で割られます．nOctaveLayers がデフォルトに設定されていて，D. Lowe の論文で使われている値（0.03）を利用したい場合は，この引数を 0.09 に設定してください．
nfeatures : The number of best features to retain. The features are ranked by their scores (measured in SIFT algorithm as the local contrast),nfeatures :保持するベストフィーチャーの数。特徴量は、そのスコア（SIFTアルゴリズムでは、局所的なコントラストとして測定される）によってランク付けされる。
nOctaveLayers : The number of layers in each octave. 3 is the value used in D. Lowe paper. The number of octaves is computed automatically from the image resolution.,nOctaveLayers : 各オクターブのレイヤー数。3は、D. Loweの論文で使われている値です。オクターブの数は、画像の解像度から自動的に計算されます。
"contrastThreshold : The contrast threshold used to filter out weak features in semi-uniform (low-contrast) regions. The larger the threshold, the less features are produced by the detector.",contrastThreshold : 半均一（低コントラスト）な領域の弱い特徴をフィルタリングするために使用するコントラスト閾値。閾値が大きいほど，検出器で生成される特徴は少なくなります。
"edgeThreshold : The threshold used to filter out edge-like features. Note that the its meaning is different from the contrastThreshold, i.e. the larger the edgeThreshold, the less features are filtered out (more features are retained).",edgeThreshold : エッジ状の特徴を除外するための閾値。すなわち，edgeThresholdが大きければ大きいほど，フィルタリングされる特徴は少なくなります（より多くの特徴が保持されます）．
"sigma : The sigma of the Gaussian applied to the input image at the octave #0. If your image is captured with a weak camera with soft lenses, you might want to reduce the number.",sigma : オクターブ#0の入力画像に適用されるガウスのシグマ。柔らかいレンズを使った弱いカメラで撮影された画像の場合は，この数値を小さくするとよいでしょう．
Class for extracting keypoints and computing descriptors using the Scale Invariant Feature Transform (SIFT) algorithm by D. Lowe [153] . ,D. LoweによるScale Invariant Feature Transform (SIFT) アルゴリズムを用いて，キーポイントを抽出し，ディスクリプタを計算するクラス [153] ．
The BRISK constructor.,BRISKのコンストラクタです．
thresh : AGAST detection threshold score.,thresh : AGAST検出の閾値スコア．
octaves : detection octaves. Use 0 to do single scale.,octaves : 検出オクターブ数．シングルスケールの場合は0を使用します．
patternScale : apply this scale to the pattern used for sampling the neighbourhood of a keypoint.,patternScale : キーポイントの近傍をサンプリングするためのパターンにこのスケールを適用します．
The BRISK constructor for a custom pattern.,カスタムパターン用のBRISKコンストラクタです。
radiusList : defines the radii (in pixels) where the samples around a keypoint are taken (for keypoint scale 1).,radiusList : キーポイント周辺のサンプルを採取する半径（ピクセル単位）を定義します（キーポイントスケール1の場合）。
numberList : defines the number of sampling points on the sampling circle. Must be the same size as radiusList..,numberList : サンプリングサークル上のサンプリングポイントの数を定義します。radiusListと同じサイズである必要があります。
dMax : threshold for the short pairings used for descriptor formation (in pixels for keypoint scale 1).,dMax : ディスクリプタの形成に使用される短いペアリングの閾値（キーポイントスケール1の場合，ピクセル単位）．
dMin : threshold for the long pairings used for orientation determination (in pixels for keypoint scale 1).,dMin : 向きの決定に使用される長いペアリングの閾値（キーポイントスケール1のピクセル単位）．
indexChange : index remapping of the bits.,indexChange : ビットのインデックスリマッピング．
"The BRISK constructor for a custom pattern, detection threshold and octaves.",カスタムパターン，検出閾値，オクターブのためのBRISKコンストラクタ．
"Class implementing the BRISK keypoint detector and descriptor extractor, described in [139] . ",139]で述べられている，BRISKキーポイント検出器とディスクリプタ抽出器を実装したクラス．
The ORB constructor.,ORB コンストラクタ．
nfeatures : The maximum number of features to retain.,nfeatures :保持する特徴の最大数．
"scaleFactor : Pyramid decimation ratio, greater than 1. scaleFactor==2 means the classical pyramid, where each next level has 4x less pixels than the previous, but such a big scale factor will degrade feature matching scores dramatically. On the other hand, too close to 1 scale factor will mean that to cover certain scale range you will need more pyramid levels and so the speed will suffer.",scaleFactor : ピラミッドのデシメーション比（1より大きい）． scaleFactor==2は，次の各レベルが前のレベルよりも4倍少ないピクセルを持つ古典的なピラミッドを意味しますが，このような大きなスケールファクターは，特徴のマッチングスコアを劇的に低下させます．一方、スケールファクターが1に近すぎると、特定のスケール範囲をカバーするために、より多くのピラミッドレベルが必要となり、速度が低下します。
"nlevels : The number of pyramid levels. The smallest level will have linear size equal to input_image_linear_size/pow(scaleFactor, nlevels - firstLevel).","nlevels : ピラミッドレベルの数を指定します。最小レベルのリニアサイズは、input_image_linear_size/pow(scaleFactor, nlevels - firstLevel)に相当します。"
edgeThreshold : This is size of the border where the features are not detected. It should roughly match the patchSize parameter.,edgeThreshold : 特徴が検出されない境界線のサイズです。patchSizeパラメータとほぼ一致している必要がある。
firstLevel : The level of pyramid to put source image to. Previous layers are filled with upscaled source image.,firstLevel : ソース画像を置くピラミッドのレベル。それ以前のレイヤーは、アップスケールされたソース画像で埋められます。
"WTA_K : The number of points that produce each element of the oriented BRIEF descriptor. The default value 2 means the BRIEF where we take a random point pair and compare their brightnesses, so we get 0/1 response. Other possible values are 3 and 4. For example, 3 means that we take 3 random points (of course, those point coordinates are random, but they are generated from the pre-defined seed, so each element of BRIEF descriptor is computed deterministically from the pixel rectangle), find point of maximum brightness and output index of the winner (0, 1 or 2). Such output will occupy 2 bits, and therefore it will need a special variant of Hamming distance, denoted as NORM_HAMMING2 (2 bits per bin). When WTA_K=4, we take 4 random points to compute each bin (that will also occupy 2 bits with possible values 0, 1, 2 or 3).",WTA_K : オリエンテッドBRIEF記述子の各要素を生成するポイントの数．デフォルト値の2は，ランダムな点のペアを取り，それらの明るさを比較するBRIEFを意味し，0/1の応答が得られます．他の可能な値は3と4です。例えば，3は，3つのランダムな点（もちろん，これらの点の座標はランダムですが，あらかじめ定義されたシードから生成されるので，BRIEFディスクリプタの各要素は，ピクセル矩形から決定論的に計算されます）を取り，最大の明るさの点を見つけ，勝者のインデックス（0，1，2）を出力することを意味します。このような出力は2ビットを占めるため，NORM_HAMMING2（1ビンあたり2ビット）と呼ばれる特別なハミング距離の変種が必要になります．WTA_K=4の場合は、4つのランダムなポイントを取り、各ビンを計算します（これも2ビットを占め、0、1、2、3のいずれかの値を取ります）。
"scoreType : The default HARRIS_SCORE means that Harris algorithm is used to rank features (the score is written to KeyPoint::score and is used to retain best nfeatures features); FAST_SCORE is alternative value of the parameter that produces slightly less stable keypoints, but it is a little faster to compute.",scoreType :デフォルトの HARRIS_SCORE は，Harris アルゴリズムを用いて特徴をランク付けすることを意味します（このスコアは KeyPoint::score に書き込まれ，最良の nfeatures 特徴を保持するために利用されます）．FAST_SCORE は，パラメータの代替値であり，キーポイントの安定性はやや劣りますが，計算速度はやや速くなります．
"patchSize : size of the patch used by the oriented BRIEF descriptor. Of course, on smaller pyramid layers the perceived image area covered by a feature will be larger.",patchSize : oriented BRIEF descriptorで利用されるパッチのサイズ．もちろん，小さなピラミッド層では，特徴で覆われた画像領域の知覚はより大きくなります．
fastThreshold : the fast threshold,fastThreshold : 高速閾値
Class implementing the ORB (oriented BRIEF) keypoint detector and descriptor extractor. ,ORB（oriented BRIEF）キーポイント検出器とディスクリプタ抽出器を実装したクラス．
"described in [206] . The algorithm uses FAST in pyramids to detect stable keypoints, selects the strongest features using FAST or Harris response, finds their orientation using first-order moments and computes the descriptors using BRIEF (where the coordinates of random point pairs (or k-tuples) are rotated according to the measured orientation). ",は，[206]で説明されています．このアルゴリズムは，安定したキーポイントを検出するためにピラミッド状のFASTを利用し，FASTやHarris応答を利用して最も強い特徴を選択し，一次モーメントを利用してその向きを求め，BRIEF（ランダムな点のペア（またはk-タプル）の座標が，測定された向きに応じて回転する）を利用してディスクリプタを計算します．
Full constructor for MSER detector.,MSER 検出器のフルコンストラクタ．
delta : it compares \((size_{i}-size_{i-delta})/size_{i-delta}\),delta ：\((size_{i}-size_{i-delta})/size_{i-delta}\)を比較します．
min_area : prune the area which smaller than minArea,min_area : minAreaよりも小さい領域を刈り取る。
max_area : prune the area which bigger than maxArea,max_area：maxAreaよりも大きい領域を削除する。
max_variation : prune the area have similar size to its children,max_variation : 子領域と同じような大きさの領域を刈り取る。
"min_diversity : for color image, trace back to cut off mser with diversity less than min_diversity",min_diversity : カラー画像の場合、min_diversity以下の多様性を持つMSERをカットするためにトレースバックする。
"max_evolution : for color image, the evolution steps",max_evolution : カラー画像の場合、進化のステップ数
"area_threshold : for color image, the area threshold to cause re-initialize",area_threshold : カラー画像において、再初期化を行うための面積閾値
"min_margin : for color image, ignore too small margin",min_margin : カラー画像の場合，小さすぎるマージンを無視する．
"edge_blur_size : for color image, the aperture size for edge blur",edge_blur_size : カラー画像に対して、エッジをぼかすための開口サイズを指定します。
Maximally stable extremal region extractor. ,最大限に安定した極値領域抽出器．
The class encapsulates all the parameters of the MSER extraction algorithm (see wiki article).,このクラスは，MSER抽出アルゴリズムのすべてのパラメータをカプセル化しています（wiki記事参照）．
"there are two different implementation of MSER: one for grey image, one for color image",MSERには，灰色画像用とカラー画像用の2種類の実装があります。
the grey image algorithm is taken from: [185] ; the paper claims to be faster than union-find method; it actually get 1.5~2m/s on my centrino L7200 1.2GHz laptop.,灰色画像のアルゴリズムは以下から引用されています: [185]; この論文では union-find 法よりも高速であると主張していますが、私の centrino L7200 1.2GHz ラップトップでは実際に 1.5~2m/s の速度が得られました。
the color image algorithm is taken from: [80] ; it should be much slower than grey image method ( 3~4 times ),カラー画像のアルゴリズムは以下から引用しています: [80] ;グレー画像法よりもはるかに遅いはずです ( 3~4倍 )。
(Python) A complete example showing the use of the MSER detector can be found at samples/python/mser.py ,(Python) MSER検出器の完全な使用例は、samples/python/mser.pyにあります。
Detect MSER regions.,MSER領域を検出します。
"image : input image (8UC1, 8UC3 or 8UC4, must be greater or equal than 3x3)","image : 入力画像（8UC1, 8UC3 または 8UC4, 3x3 以上であること）．"
msers : resulting list of point sets,msers : 結果として得られる点群のリスト
bboxes : resulting bounding boxes,bboxes : 結果として得られるバウンディングボックス
Detects corners using the FAST algorithm.,FASTアルゴリズムを用いてコーナーを検出します．
image : grayscale image where keypoints (corners) are detected.,image : キーポイント（コーナー）が検出されたグレースケール画像．
keypoints : keypoints detected on the image.,keypoints : 画像上で検出されたキーポイント．
threshold : threshold on difference between intensity of the central pixel and pixels of a circle around this pixel.,threshold : 中心となるピクセルの強度と，そのピクセルを囲む円のピクセルの強度の差に対する閾値．
"nonmaxSuppression : if true, non-maximum suppression is applied to detected corners (keypoints).",nonmaxSuppression : trueの場合，検出されたコーナー（キーポイント）に対して non-maximum suppression が適用されます．
"type : one of the three neighborhoods as defined in the paper: FastFeatureDetector::TYPE_9_16, FastFeatureDetector::TYPE_7_12, FastFeatureDetector::TYPE_5_8","type : 論文で定義されている3つの近傍領域のうちの1つです．FastFeatureDetector::TYPE_9_16, FastFeatureDetector::TYPE_7_12, FastFeatureDetector::TYPE_5_8。"
"Detects corners using the FAST algorithm by [205] .NoteIn Python API, types are given as cv.FAST_FEATURE_DETECTOR_TYPE_5_8, cv.FAST_FEATURE_DETECTOR_TYPE_7_12 and cv.FAST_FEATURE_DETECTOR_TYPE_9_16. For corner detection, use cv.FAST.detect() method.","205] によるFASTアルゴリズムを用いてコーナーを検出します．メモPython APIでは， cv.FAST_FEATURE_DETECTOR_TYPE_5_8, cv.FAST_FEATURE_DETECTOR_TYPE_7_12, cv.FAST_FEATURE_DETECTOR_TYPE_9_16 のようにタイプが指定されています．コーナー検出には， cv.FAST.detect() メソッドを利用します．"
Wrapping class for feature detection using the FAST method. : ,FASTメソッドを用いた特徴検出のためのラッパークラス．
Detects corners using the AGAST algorithm.,AGASTアルゴリズムを用いてコーナーを検出します．
"type : one of the four neighborhoods as defined in the paper: AgastFeatureDetector::AGAST_5_8, AgastFeatureDetector::AGAST_7_12d, AgastFeatureDetector::AGAST_7_12s, AgastFeatureDetector::OAST_9_16","type : 論文で定義された4つの近傍領域のうちの1つ．AgastFeatureDetector::AGAST_5_8, AgastFeatureDetector::AGAST_7_12d, AgastFeatureDetector::AGAST_7_12s, AgastFeatureDetector::OAST_9_16．"
"For non-Intel platforms, there is a tree optimised variant of AGAST with same numerical results. The 32-bit binary tree tables were generated automatically from original code using perl script. The perl script and examples of tree generation are placed in features2d/doc folder. Detects corners using the AGAST algorithm by [159] .",Intel以外のプラットフォームでは、同じ数値結果を持つAGASTのツリー最適化バリアントがあります。32ビットバイナリのツリーテーブルは、perlスクリプトを使ってオリジナルコードから自動的に生成されました。perlスクリプトとツリー生成の例はfeatures2d/docフォルダにあります。159]のAGASTアルゴリズムを用いてコーナーを検出します。
Wrapping class for feature detection using the AGAST method. : ,AGAST法を用いた特徴検出のためのラッピングクラス．
Wrapping class for feature detection using the goodFeaturesToTrack function. : ,goodFeaturesToTrack関数を用いた特徴検出のためのラッパークラス．
Class for extracting blobs from an image. : ,画像からblobを抽出するためのクラス．
The class implements a simple algorithm for extracting blobs from an image:,このクラスは，画像からblobを抽出するための簡単なアルゴリズムを実装しています．
Convert the source image to binary images by applying thresholding with several thresholds from minThreshold (inclusive) to maxThreshold (exclusive) with distance thresholdStep between neighboring thresholds.,minThreshold (including) から maxThreshold (exclusive) までの複数の閾値を用いて，隣接する閾値間の距離 thresholdStep で閾値処理を行い，元画像を2値画像に変換する．
Extract connected components from every binary image by findContours and calculate their centers.,findContoursにより各2値画像から連結成分を抽出し、その中心を算出する。
"Group centers from several binary images by their coordinates. Close centers form one group that corresponds to one blob, which is controlled by the minDistBetweenBlobs parameter.",複数の2値画像の中心を、その座標によってグループ化する。近い中心は1つのグループを形成し、そのグループは1つのblobに対応します。
"From the groups, estimate final centers of blobs and their radiuses and return as locations and sizes of keypoints.",これらのグループから，blobの最終的な中心とその半径を推定し，キーポイントの位置とサイズとして返します．
This class performs several filtrations of returned blobs. You should set filterBy* to true/false to turn on/off corresponding filtration. Available filtrations:,このクラスは、返されたblobに対して、いくつかのフィルタリングを行います。filterBy*をtrue/falseに設定することで、対応するフィルタリングをオン/オフすることができます。利用可能なフィルタリング。
"By color. This filter compares the intensity of a binary image at the center of a blob to blobColor. If they differ, the blob is filtered out. Use blobColor = 0 to extract dark blobs and blobColor = 255 to extract light blobs.",色によるフィルタ。このフィルタは、blobの中心にある2値画像の強度をblobColorと比較します。両者が異なる場合、そのblobはフィルタリングされます。暗いブロブを抽出する場合はblobColor = 0、明るいブロブを抽出する場合はblobColor = 255を使用します。
By area. Extracted blobs have an area between minArea (inclusive) and maxArea (exclusive).,面積で抽出されたblobは、minArea（含む）とmaxArea（含む）の間の面積を持ちます。
By circularity. Extracted blobs have circularity ( \(\frac{4*\pi*Area}{perimeter * perimeter}\)) between minCircularity (inclusive) and maxCircularity (exclusive).,円形度別。抽出されたblobはminCircularity(包含)とmaxCircularity(排他)の間の円形度( ˶ˆ꒳ˆ˵ )を持つ。
By ratio of the minimum inertia to maximum inertia. Extracted blobs have this ratio between minInertiaRatio (inclusive) and maxInertiaRatio (exclusive).,最小イナーシャと最大イナーシャの比で。抽出されたblobはこの比率がminInertiaRatio(包含)とmaxInertiaRatio(排他)の間にある。
By convexity. Extracted blobs have convexity (area / area of blob convex hull) between minConvexity (inclusive) and maxConvexity (exclusive).,凸性によるもの。抽出されたblobは、凸性（面積／blobの凸包の面積）がminConvexity（含む）からmaxConvexity（含む）の間にある。
Default values of parameters are tuned to extract dark circular blobs. ,パラメータのデフォルト値は、暗い円形のblobを抽出するように調整されています。
The KAZE constructor.,KAZEのコンストラクタです。
extended : Set to enable extraction of extended (128-byte) descriptor.,extended : 拡張(128バイト)記述子の抽出を有効にするために設定します．
upright : Set to enable use of upright descriptors (non rotation-invariant).,upright : 正立した記述子（非回転不変）の使用を有効にするために設定します。
threshold : Detector response threshold to accept point,threshold : 点を受け入れるための検出器応答閾値
nOctaves : Maximum octave evolution of the image,nOctaves : 画像の最大オクターブ展開
nOctaveLayers : Default number of sublevels per scale level,nOctaveLayers : スケールレベルごとのサブレベルのデフォルト数
"diffusivity : Diffusivity type. DIFF_PM_G1, DIFF_PM_G2, DIFF_WEICKERT or DIFF_CHARBONNIER",diffusivity : 拡散性のタイプ。DIFF_PM_G1、DIFF_PM_G2、DIFF_WEICKERT、DIFF_CHARBONNIERのいずれか。
"Class implementing the KAZE keypoint detector and descriptor extractor, described in [10] . ",10]で説明した KAZE キーポイント検出器とディスクリプタ抽出器を実装したクラス．
"NoteAKAZE descriptor can only be used with KAZE or AKAZE keypoints .. [ABD12] KAZE Features. Pablo F. Alcantarilla, Adrien Bartoli and Andrew J. Davison. In European Conference on Computer Vision (ECCV), Fiorenze, Italy, October 2012. ","注AKAZE ディスクリプタは，KAZE または AKAZE キーポイントでのみ使用できます． [ABD12] KAZE の特徴Pablo F. Alcantarilla, Adrien Bartoli and Andrew J. Davison.In European Conference on Computer Vision (ECCV), Fiorenze, Italy, October 2012."
The AKAZE constructor.,AKAZEのコンストラクタです。
"descriptor_type : Type of the extracted descriptor: DESCRIPTOR_KAZE, DESCRIPTOR_KAZE_UPRIGHT, DESCRIPTOR_MLDB or DESCRIPTOR_MLDB_UPRIGHT.","descriptor_type :抽出されたディスクリプターのタイプ。DESCRIPTOR_KAZE, DESCRIPTOR_KAZE_UPRIGHT, DESCRIPTOR_MLDB or DESCRIPTOR_MLDB_UPRIGHT."
descriptor_size : Size of the descriptor in bits. 0 -> Full size,descriptor_size : ディスクリプタのサイズ(ビット単位)。0 -> フルサイズ
"descriptor_channels : Number of channels in the descriptor (1, 2, 3)","descriptor_channels : ディスクリプタ内のチャンネル数 (1, 2, 3)"
"Class implementing the AKAZE keypoint detector and descriptor extractor, described in [9]. ",9]で述べた，AKAZEのキーポイント検出器とディスクリプタ抽出器を実装したクラス．
AKAZE descriptors can only be used with KAZE or AKAZE keypoints. This class is thread-safe.,AKAZE ディスクリプタは，KAZE または AKAZE のキーポイントでのみ利用できます．このクラスはスレッドセーフです。
"NoteWhen you need descriptors use Feature2D::detectAndCompute, which provides better performance. When using Feature2D::detect followed by Feature2D::compute scale space pyramid is computed twice.",注意記述子が必要な場合は，性能の良い Feature2D::detectAndCompute を使用してください．Feature2D::detectの後にFeature2D::computeを使用すると、スケールスペースピラミッドが2回計算されます。
AKAZE implements T-API. When image is passed as UMat some parts of the algorithm will use OpenCL.,AKAZE は T-API を実装しています．画像が UMat として渡された場合，アルゴリズムの一部は OpenCL を使用します．
"[ANB13] Fast Explicit Diffusion for Accelerated Features in Nonlinear Scale Spaces. Pablo F. Alcantarilla, Jesús Nuevo and Adrien Bartoli. In British Machine Vision Conference (BMVC), Bristol, UK, September 2013. ","[ANB13] Fast Explicit Diffusion for Accelerated Features in Nonlinear Scale Spaces.Pablo F. Alcantarilla, Jesús Nuevo, Adrien Bartoli.British Machine Vision Conference (BMVC), Bristol, UK, September 2013 で発表しました。"
Returns Gaussian filter coefficients.,ガウスフィルターの係数を返します。
ksize : Aperture size. It should be odd ( \(\texttt{ksize} \mod 2 = 1\) ) and positive.,ksize : 絞りサイズ．奇数 ( ˶‾᷄ -̫ ‾᷅˵ ) かつ正数である必要があります。
"sigma : Gaussian standard deviation. If it is non-positive, it is computed from ksize as sigma = 0.3*((ksize-1)*0.5 - 1) + 0.8.",sigma : ガウス型標準偏差．正数でない場合は、ksize から sigma = 0.3*((ksize-1)*0.5 - 1) + 0.8 と計算される。
ktype : Type of filter coefficients. It can be CV_32F or CV_64F .,ktype :フィルタ係数の種類．CV_32F または CV_64F を指定します．
"The function computes and returns the \(\texttt{ksize} \times 1\) matrix of Gaussian filter coefficients:\[G_i= \alpha *e^{-(i-( \texttt{ksize} -1)/2)^2/(2* \texttt{sigma}^2)},\]where \(i=0..\texttt{ksize}-1\) and \(\alpha\) is the scale factor chosen so that \(\sum_i G_i=1\).Two of such generated kernels can be passed to sepFilter2D. Those functions automatically recognize smoothing kernels (a symmetrical kernel with sum of weights equal to 1) and handle them accordingly. You may also use the higher-level GaussianBlur.See alsosepFilter2D, getDerivKernels, getStructuringElement, GaussianBlur","この関数は，ガウスフィルタ係数の行列を計算して返します：\[G_i= ˶ˆ꒳ˆ˵ *e^{-(i-( ˶ˆ꒳ˆ˵ )/2)^2/(2* ˶ˆ꒳ˆ˵ )}, ˶ˆ꒳ˆ˵]ここで，(i=0..texttt{ksize}-1）、\（\alpha\）は、\（sum_i G_i=1\）となるように選ばれたスケールファクターです。このようにして生成された2つのカーネルは，sepFilter2Dに渡すことができます．これらの関数は，平滑化カーネル（重みの合計が1になるような対称的なカーネル）を自動的に認識して，それに応じた処理を行います．関連項目： sepFilter2D, getDerivKernels, getStructuringElement, GaussianBlur"
Returns filter coefficients for computing spatial image derivatives.,画像の空間微分を計算するためのフィルタ係数を返します。
kx : Output matrix of row filter coefficients. It has the type ktype .,kx : 行フィルタ係数の出力行列．タイプは ktype です．
ky : Output matrix of column filter coefficients. It has the type ktype .,ky : 列方向のフィルタ係数を表す出力行列．タイプは ktype です．
dx : Derivative order in respect of x.,dx : xを基準とした微分順序。
dy : Derivative order in respect of y.,dy : yに関する微分次数。
"ksize : Aperture size. It can be FILTER_SCHARR, 1, 3, 5, or 7.","ksize : 絞りサイズ。FILTER_SCHARR, 1, 3, 5, 7 のいずれかである。"
"normalize : Flag indicating whether to normalize (scale down) the filter coefficients or not. Theoretically, the coefficients should have the denominator \(=2^{ksize*2-dx-dy-2}\). If you are going to filter floating-point images, you are likely to use the normalized kernels. But if you compute derivatives of an 8-bit image, store the results in a 16-bit image, and wish to preserve all the fractional bits, you may want to set normalize=false .",normalize : フィルタ係数を正規化（スケールダウン）するかどうかを示すフラグ。理論的には、係数は分母が\(=2^{ksize*2-dx-dy-2}\)になるはずです。浮動小数点の画像をフィルタリングする場合は，正規化カーネルを使うことが多いと思います．しかし，8ビット画像の導関数を計算し，その結果を16ビット画像に保存する場合，すべての小数ビットを保存したいのであれば，normalize=falseを設定するとよいでしょう．
ktype : Type of filter coefficients. It can be CV_32f or CV_64F .,ktype :フィルタ係数の種類．CV_32f または CV_64F になります．
"The function computes and returns the filter coefficients for spatial image derivatives. When ksize=FILTER_SCHARR, the Scharr \(3 \times 3\) kernels are generated (see Scharr). Otherwise, Sobel kernels are generated (see Sobel). The filters are normally passed to sepFilter2D or to","この関数は，空間画像導出のためのフィルタ係数を計算し，それを返します．ksize=FILTER_SCHARR の場合， Scharr ˶ˆ꒳ˆ˵ (""Scharr"" を参照してください) カーネルが生成されます．それ以外の場合は，Sobelカーネルが生成されます（Sobel参照）．フィルタは，通常，sepFilter2D や，Sobel に渡されます．"
Returns Gabor filter coefficients.,ガボールフィルタの係数を返す。
ksize : Size of the filter returned.,ksize : 返されるフィルターのサイズ．
sigma : Standard deviation of the gaussian envelope.,sigma : ガウシアンエンベロープの標準偏差．
theta : Orientation of the normal to the parallel stripes of a Gabor function.,theta :ガボール関数の平行な縞に対する法線の向き．
lambd : Wavelength of the sinusoidal factor.,lambd : sinusoidal factorの波長．
gamma : Spatial aspect ratio.,gamma : 空間的なアスペクト比。
psi : Phase offset.,psi :位相オフセット.
"For more details about gabor filter equations and parameters, see: Gabor Filter.",ガボールフィルターの方程式やパラメーターの詳細については，以下を参照してください。ガボールフィルタ
Returns a structuring element of the specified size and shape for morphological operations.,モフォロジカルな操作を行うために、指定されたサイズと形状の構造化要素を返します。
shape : Element shape that could be one of MorphShapes,shape : MorphShapesの1つである要素の形状。
ksize : Size of the structuring element.,ksize : 構造化要素のサイズ．
"anchor : Anchor position within the element. The default value \((-1, -1)\) means that the anchor is at the center. Note that only the shape of a cross-shaped element depends on the anchor position. In other cases the anchor just regulates how much the result of the morphological operation is shifted.","anchor : 要素内のアンカー位置．既定値の\((-1, -1)୨୧)は、アンカーが中心にあることを意味します。なお，アンカーの位置に依存するのは，十字型要素の形状のみです．その他の場合，アンカーは，形態素解析の結果をどれだけずらすかを調整するだけです．"
"The function constructs and returns the structuring element that can be further passed to erode, dilate or morphologyEx. But you can also construct an arbitrary binary mask yourself and use it as the structuring element.Examples: samples/cpp/tutorial_code/ImgProc/Morphology_1.cpp, and samples/cpp/tutorial_code/ImgProc/Morphology_2.cpp.","この関数は，構造化要素を作成して返します．この構造化要素は，さらに erode, dilate, morphologyEx に渡すことができます．サンプル: samples/cpp/tutorial_code/ImgProc/Morphology_1.cpp, samples/cpp/tutorial_code/ImgProc/Morphology_2.cpp."
Blurs an image using the median filter.,メディアンフィルターを使って，画像をぼかします．
"src : input 1-, 3-, or 4-channel image; when ksize is 3 or 5, the image depth should be CV_8U, CV_16U, or CV_32F, for larger aperture sizes, it can only be CV_8U.","src : 1, 3, 4 チャンネルの入力画像．ksize が 3 または 5 の場合，画像の深度は CV_8U, CV_16U, CV_32F のいずれかでなければいけませんが，より大きなアパーチャサイズの場合は CV_8U のみとなります．"
dst : destination array of the same size and type as src.,dst : src と同じサイズ，同じタイプの出力配列．
"ksize : aperture linear size; it must be odd and greater than 1, for example: 3, 5, 7 ...","ksize : アパーチャのリニアサイズ．1より大きい奇数でなければいけません．3, 5, 7 ..."
"The function smoothes an image using the median filter with the \(\texttt{ksize} \times \texttt{ksize}\) aperture. Each channel of a multi-channel image is processed independently. In-place operation is supported.NoteThe median filter uses BORDER_REPLICATE internally to cope with border pixels, see BorderTypesSee alsobilateralFilter, blur, boxFilter, GaussianBlurExamples: samples/cpp/laplace.cpp, samples/cpp/tutorial_code/ImgProc/Smoothing/Smoothing.cpp, and samples/cpp/tutorial_code/ImgTrans/houghcircles.cpp.",この関数は，メジアンフィルタを用いて画像を平滑化します．マルチチャンネル画像の各チャンネルは，独立して処理されます．インプレース操作に対応しています。注意中央値フィルタは、境界線のピクセルに対処するために、内部的にBORDER_REPLICATEを使用しています。
Blurs an image using a Gaussian filter.,ガウシアンフィルターを使って画像をぼかします。
"src : input image; the image can have any number of channels, which are processed independently, but the depth should be CV_8U, CV_16U, CV_16S, CV_32F or CV_64F.","src : 入力画像．画像は任意の数のチャンネルを持つことができ，それらは独立して処理されますが，深度は CV_8U, CV_16U, CV_16S, CV_32F または CV_64F である必要があります．"
dst : output image of the same size and type as src.,dst : src と同じサイズ，同じ種類の出力画像．
"ksize : Gaussian kernel size. ksize.width and ksize.height can differ but they both must be positive and odd. Or, they can be zero's and then they are computed from sigma.",ksize : ガウスカーネルサイズ． ksize.width と ksize.height は，異なっていても構いませんが，両方とも正の奇数でなければいけません．また，これらを0にして sigma から計算することもできます．
sigmaX : Gaussian kernel standard deviation in X direction.,sigmaX : X方向のガウシアンカーネル標準偏差．
"sigmaY : Gaussian kernel standard deviation in Y direction; if sigmaY is zero, it is set to be equal to sigmaX, if both sigmas are zeros, they are computed from ksize.width and ksize.height, respectively (see getGaussianKernel for details); to fully control the result regardless of possible future modifications of all this semantics, it is recommended to specify all of ksize, sigmaX, and sigmaY.",sigmaY : Y方向のガウスカーネル標準偏差． sigmaYが0の場合は，sigmaXと等しくなるように設定され，両方のsigmaが0の場合は，それぞれksize.widthとksize.heightから計算されます（詳細はgetGaussianKernelを参照してください）．将来的にこれらのセマンティクスが変更される可能性があっても，結果を完全にコントロールするためには，ksize，sigmaX，sigmaYのすべてを指定することが推奨されます．
"borderType : pixel extrapolation method, see BorderTypes. BORDER_WRAP is not supported.",borderType : ピクセルの外挿法，BorderTypesを参照．BORDER_WRAPはサポートされていません．
"The function convolves the source image with the specified Gaussian kernel. In-place filtering is supported.See alsosepFilter2D, filter2D, blur, boxFilter, bilateralFilter, medianBlurExamples: samples/cpp/laplace.cpp, samples/cpp/tutorial_code/ImgProc/Smoothing/Smoothing.cpp, and samples/cpp/tutorial_code/ImgTrans/Sobel_Demo.cpp.","この関数は，入力画像を指定されたガウスカーネルで畳み込みます．参照：sosepFilter2D, filter2D, blur, boxFilter, bilateralFilter, medianBlurExamples: samples/cpp/laplace.cpp, samples/cpp/tutorial_code/ImgProc/Smoothing/Smoothing.cpp, and samples/cpp/tutorial_code/ImgTrans/Sobel_Demo.cpp."
Applies the bilateral filter to an image.,バイラテラルフィルターを画像に適用します。
"src : Source 8-bit or floating-point, 1-channel or 3-channel image.",src : 8ビットまたは浮動小数点，1チャンネルまたは3チャンネルの入力画像．
dst : Destination image of the same size and type as src .,dst : src と同じサイズ，同じタイプの出力画像．
"d : Diameter of each pixel neighborhood that is used during filtering. If it is non-positive, it is computed from sigmaSpace.",d : フィルタリングの際に利用される，各ピクセルの近傍領域の直径．非正値の場合は，sigmaSpace から計算されます．
"sigmaColor : Filter sigma in the color space. A larger value of the parameter means that farther colors within the pixel neighborhood (see sigmaSpace) will be mixed together, resulting in larger areas of semi-equal color.",sigmaColor : 色空間におけるフィルタリングシグマ．このパラメータの値が大きいほど、ピクセル近傍（sigmaSpace参照）の遠方の色が混ざり合い、半均等な色の領域が広くなることを意味します。
"sigmaSpace : Filter sigma in the coordinate space. A larger value of the parameter means that farther pixels will influence each other as long as their colors are close enough (see sigmaColor ). When d>0, it specifies the neighborhood size regardless of sigmaSpace. Otherwise, d is proportional to sigmaSpace.",sigmaSpace : 座標空間でのフィルターシグマ。このパラメータの値を大きくすると，色が近ければ遠いピクセル同士が影響しあうことになります（sigmaColor参照）．d>0の場合は，sigmaSpaceに関わらず，近傍のサイズを指定します．それ以外の場合，dはsigmaSpaceに比例する．
"borderType : border mode used to extrapolate pixels outside of the image, see BorderTypes",borderType : 画像の外側にあるピクセルを外挿する際に利用される境界モード，BorderTypesを参照してください．
"The function applies bilateral filtering to the input image, as described in http://www.dai.ed.ac.uk/CVonline/LOCAL_COPIES/MANDUCHI1/Bilateral_Filtering.html bilateralFilter can reduce unwanted noise very well while keeping edges fairly sharp. However, it is very slow compared to most filters.Sigma values: For simplicity, you can set the 2 sigma values to be the same. If they are small (< 10), the filter will not have much effect, whereas if they are large (> 150), they will have a very strong effect, making the image look ""cartoonish"".Filter size: Large filters (d > 5) are very slow, so it is recommended to use d=5 for real-time applications, and perhaps d=9 for offline applications that need heavy noise filtering.This filter does not work inplace.Examples: samples/cpp/tutorial_code/ImgProc/Smoothing/Smoothing.cpp.",この関数は， http://www.dai.ed.ac.uk/CVonline/LOCAL_COPIES/MANDUCHI1/Bilateral_Filtering.html で説明されているように，入力画像にバイラテラルフィルタリングを適用します．bilateralFilter は，エッジをかなりシャープに保ちつつ，不要なノイズを非常によく減らします．ただし、他のフィルタ.Sigmaに比べて、非常に低速です。簡単にするために、2つのシグマ値を同じにすることができます。シグマ値が小さい（10未満）とフィルターの効果はあまりなく、逆に大きい（150以上）と非常に強い効果が得られ、画像が「漫画的」になります。フィルターサイズ。大きなフィルタ（d > 5）は非常に遅いので、リアルタイムのアプリケーションにはd=5を、重いノイズフィルタを必要とするオフラインのアプリケーションにはd=9を使用することをお勧めします。
Blurs an image using the box filter.,ボックスフィルターを使って、画像をぼかします。
src : input image.,src : 入力画像．
ddepth : the output image depth (-1 to use src.depth()).,ddepth : 出力画像の深度（-1であれば，src.depth()を使用します）．
ksize : blurring kernel size.,ksize : ぼかし処理のカーネルサイズ．
"anchor : anchor point; default value Point(-1,-1) means that the anchor is at the kernel center.","anchor : アンカーポイント．デフォルト値 Point(-1,-1) は，アンカーがカーネルの中心にあることを意味します．"
"normalize : flag, specifying whether the kernel is normalized by its area or not.",normalize : カーネルをその面積で正規化するかどうかを指定するフラグ．
"borderType : border mode used to extrapolate pixels outside of the image, see BorderTypes. BORDER_WRAP is not supported.",borderType : 画像の外側のピクセルを外挿する際に使われるボーダーモード，BorderTypesを参照．BORDER_WRAPはサポートされていません．
"The function smooths an image using the kernel:\[\texttt{K} = \alpha \begin{bmatrix} 1 & 1 & 1 & \cdots & 1 & 1 \\ 1 & 1 & 1 & \cdots & 1 & 1 \\ \hdotsfor{6} \\ 1 & 1 & 1 & \cdots & 1 & 1 \end{bmatrix}\]where\[\alpha = \begin{cases} \frac{1}{\texttt{ksize.width*ksize.height}} & \texttt{when } \texttt{normalize=true} \\1 & \texttt{otherwise}\end{cases}\]Unnormalized box filter is useful for computing various integral characteristics over each pixel neighborhood, such as covariance matrices of image derivatives (used in dense optical flow algorithms, and so on). If you need to compute pixel sums over variable-size windows, use integral.See alsoblur, bilateralFilter, GaussianBlur, medianBlur, integral",この関数は，カーネルを用いて画像を平滑化します：˶˙º̬˙˶1 & 1 & 1 & ˶ˆ꒳ˆ˵ )\\ ♪♪～\What's New Year!\normalize=true}。\\非正規化ボックスフィルターは，画像導関数の共分散行列（密なオプティカルフローアルゴリズムなどで使用される）など，各ピクセルの近傍におけるさまざまな積分特性を計算するのに便利です．他にも、oblur、bilateralFilter、GaussianBlur、medianBlur、integralがあります。
Calculates the normalized sum of squares of the pixel values overlapping the filter.,フィルタをかけた部分のピクセル値の正規化二乗和を計算します。
src : input image,src : 入力画像
dst : output image of the same size and type as src,dst : src と同じサイズ・タイプの出力画像
ddepth : the output image depth (-1 to use src.depth()),ddepth : 出力画像の深度（-1 の場合は， src.depth() を利用します）．
ksize : kernel size,ksize : カーネルサイズ
"anchor : kernel anchor point. The default value of Point(-1, -1) denotes that the anchor is at the kernel center.","anchor : カーネルのアンカーポイント．デフォルト値である Point(-1, -1) は，カーネルの中心にアンカーを置くことを意味します．"
"normalize : flag, specifying whether the kernel is to be normalized by it's area or not.",normalize : カーネルをその面積で正規化するかどうかを指定するフラグ．
"For every pixel \( (x, y) \) in the source image, the function calculates the sum of squares of those neighboring pixel values which overlap the filter placed over the pixel \( (x, y) \).The unnormalized square box filter can be useful in computing local image statistics such as the the local variance and standard deviation around the neighborhood of a pixel.See alsoboxFilter","この関数は，入力画像中の各ピクセル ˶( (x, y) ˶ )に対して，そのピクセル上に配置されたフィルタと重なる隣接ピクセル値の2乗和を計算します．この正規化されていない正方形のボックスフィルタは，ピクセル周辺の局所的な分散や標準偏差など，画像の局所的な統計情報を計算するのに役立ちます．"
Blurs an image using the normalized box filter.,正規化されたボックスフィルターを使って、画像をぼかします。
"src : input image; it can have any number of channels, which are processed independently, but the depth should be CV_8U, CV_16U, CV_16S, CV_32F or CV_64F.","src : 入力画像．任意の数のチャンネルを持つことができ，それらは独立して処理されますが，深度は CV_8U, CV_16U, CV_16S, CV_32F または CV_64F である必要があります．"
"The function smooths an image using the kernel:\[\texttt{K} = \frac{1}{\texttt{ksize.width*ksize.height}} \begin{bmatrix} 1 & 1 & 1 & \cdots & 1 & 1 \\ 1 & 1 & 1 & \cdots & 1 & 1 \\ \hdotsfor{6} \\ 1 & 1 & 1 & \cdots & 1 & 1 \\ \end{bmatrix}\]The call blur(src, dst, ksize, anchor, borderType) is equivalent to boxFilter(src, dst, src.type(), ksize, anchor, true, borderType).See alsoboxFilter, bilateralFilter, GaussianBlur, medianBlurExamples: samples/cpp/edge.cpp, samples/cpp/laplace.cpp, and samples/cpp/tutorial_code/ImgProc/Smoothing/Smoothing.cpp.","この関数は，カーネルを用いて画像を平滑化します．\˶ˆ꒳ˆ˵ )♪ 1 & 1 & 1 & ♪ 1 & 1 ♪ 1 & 1 ♪ ♪\\ Blur(src, dst, ksize, anchor, borderType)の呼び出しは、boxFilter(src, dst, src.type(), ksize, anchor, true, borderType)と同等です。alsoboxFilter, bilateralFilter, GaussianBlur, medianBlurExamples: samples/cpp/edge.cpp, samples/cpp/laplace.cpp, and samples/cpp/tutorial_code/ImgProc/Smoothing/Smoothing.cppをご参照ください。"
Convolves an image with the kernel.,画像をカーネルで畳み込みます．
dst : output image of the same size and the same number of channels as src.,dst : src と同じサイズ，同じチャンネル数の出力画像．
"ddepth : desired depth of the destination image, see combinations",ddepth : 出力画像の希望深度，組み合わせを参照してください．
"kernel : convolution kernel (or rather a correlation kernel), a single-channel floating point matrix; if you want to apply different kernels to different channels, split the image into separate color planes using split and process them individually.",kernel : シングルチャンネルの浮動小数点行列であるコンボリューションカーネル（というか相関カーネル）．
"anchor : anchor of the kernel that indicates the relative position of a filtered point within the kernel; the anchor should lie within the kernel; default value (-1,-1) means that the anchor is at the kernel center.",anchor : カーネル内でのフィルタリングされた点の相対的な位置を示すカーネルのアンカー．
delta : optional value added to the filtered pixels before storing them in dst.,delta : dstに格納する前に，フィルタリングされたピクセルに加えるオプション値．
"The function applies an arbitrary linear filter to an image. In-place operation is supported. When the aperture is partially outside the image, the function interpolates outlier pixel values according to the specified border mode.The function does actually compute correlation, not the convolution:\[\texttt{dst} (x,y) = \sum _{ \substack{0\leq x' < \texttt{kernel.cols}\\{0\leq y' < \texttt{kernel.rows}}}} \texttt{kernel} (x',y')* \texttt{src} (x+x'- \texttt{anchor.x} ,y+y'- \texttt{anchor.y} )\]That is, the kernel is not mirrored around the anchor point. If you need a real convolution, flip the kernel using flip and set the new anchor to (kernel.cols - anchor.x - 1, kernel.rows - anchor.y - 1).The function uses the DFT-based algorithm in case of sufficiently large kernels (~11 x 11 or larger) and the direct algorithm for small kernels.See alsosepFilter2D, dft, matchTemplate","この関数は，任意の線形フィルタを画像に適用します．インプレース操作がサポートされています．この関数は，畳み込みではなく，実際に相関を計算します．(x,y) = ˶ˆ꒳ˆ˵ < ˶ˆ꒳ˆ˵ < ˶ˆ꒳ˆ˵ )\♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪(x',y')* ˶ˆ꒳ˆ˵ )(x+x'- ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) つまり、カーネルはアンカーポイントを中心にミラーリングされていないのです。この関数は，十分に大きなカーネル（〜11 x 11 以上）の場合は DFT ベースのアルゴリズムを使い，小さなカーネルの場合は直接アルゴリズムを使います．参照：sosepFilter2D, dft, matchTemplate"
Applies a separable linear filter to an image.,分離可能な線形フィルタを画像に適用します．
dst : Destination image of the same size and the same number of channels as src .,dst : src と同じサイズ，同じチャンネル数の出力画像．
"ddepth : Destination image depth, see combinations",ddepth : 出力画像の深度，組み合わせを参照．
kernelX : Coefficients for filtering each row.,kernelX : 各行をフィルタリングするための係数．
kernelY : Coefficients for filtering each column.,kernelY : 各列のフィルタリングに利用される係数．
"anchor : Anchor position within the kernel. The default value \((-1,-1)\) means that the anchor is at the kernel center.","anchor : カーネル内でのアンカー位置．デフォルト値である˶((-1,-1)˶)は，カーネルの中心にアンカーを置くことを意味します．"
delta : Value added to the filtered results before storing them.,delta :フィルタリングされた結果を保存する前に加えられる値です．
"borderType : Pixel extrapolation method, see BorderTypes. BORDER_WRAP is not supported.","borderType :Pixel extrapolation method, See BorderTypes.BORDER_WRAPはサポートされていません．"
"The function applies a separable linear filter to the image. That is, first, every row of src is filtered with the 1D kernel kernelX. Then, every column of the result is filtered with the 1D kernel kernelY. The final result shifted by delta is stored in dst .See alsofilter2D, Sobel, GaussianBlur, boxFilter, blur","この関数は，画像に分離可能な線形フィルタを適用します．つまり，まず src の各行が，1次元カーネル kernelX によってフィルタリングされます．そして，結果の各列は，1次元カーネルkernelYでフィルタリングされます．delta によってシフトされた最終結果が，dst に格納されます． 参照：filter2D, Sobel, GaussianBlur, boxFilter, blur"
"Calculates the first, second, third, or mixed image derivatives using an extended Sobel operator.",拡張された Sobel 演算子を使って、画像の 1 次、2 次、3 次、または混合導関数を計算します。
dst : output image of the same size and the same number of channels as src .,dst : src と同じサイズ，同じチャンネル数の出力画像．
"ddepth : output image depth, see combinations; in the case of 8-bit input images it will result in truncated derivatives.",ddepth : 出力画像の深度，組み合わせを参照してください．8ビットの入力画像の場合は，導関数が切り捨てられます．
dx : order of the derivative x.,dx : 微分xの次数．
dy : order of the derivative y.,dy : 導関数yの次数．
"ksize : size of the extended Sobel kernel; it must be 1, 3, 5, or 7.","ksize : 拡張Sobelカーネルのサイズ; 1, 3, 5, 7のいずれかを指定します．"
"scale : optional scale factor for the computed derivative values; by default, no scaling is applied (see getDerivKernels for details).",scale : オプションである，計算された微分値のスケールファクター．
delta : optional delta value that is added to the results prior to storing them in dst.,delta : オプションで，結果をdstに格納する前に追加されるデルタ値．
"In all cases except one, the \(\texttt{ksize} \times \texttt{ksize}\) separable kernel is used to calculate the derivative. When \(\texttt{ksize = 1}\), the \(3 \times 1\) or \(1 \times 3\) kernel is used (that is, no Gaussian smoothing is done). ksize = 1 can only be used for the first or the second x- or y- derivatives.There is also the special value ksize = FILTER_SCHARR (-1) that corresponds to the \(3\times3\) Scharr filter that may give more accurate results than the \(3\times3\) Sobel. The Scharr aperture is\[\vecthreethree{-3}{0}{3}{-10}{0}{10}{-3}{0}{3}\]for the x-derivative, or transposed for the y-derivative.The function calculates an image derivative by convolving the image with the appropriate kernel:\[\texttt{dst} = \frac{\partial^{xorder+yorder} \texttt{src}}{\partial x^{xorder} \partial y^{yorder}}\]The Sobel operators combine Gaussian smoothing and differentiation, so the result is more or less resistant to the noise. Most often, the function is called with ( xorder = 1, yorder = 0, ksize = 3) or ( xorder = 0, yorder = 1, ksize = 3) to calculate the first x- or y- image derivative. The first case corresponds to a kernel of:\[\vecthreethree{-1}{0}{1}{-2}{0}{2}{-1}{0}{1}\]The second case corresponds to a kernel of:\[\vecthreethree{-1}{-2}{-1}{0}{0}{0}{1}{2}{1}\]See alsoScharr, Laplacian, sepFilter2D, filter2D, GaussianBlur, cartToPolarExamples: samples/cpp/tutorial_code/ImgTrans/Sobel_Demo.cpp.","1つのケースを除いて、微分の計算には、Separable Kernel が使われます。ksize = 1 は、1 次または 2 次の x または y の微分にのみ使用できます。また、Sobelよりも正確な結果が得られる可能性のある Scharrフィルタに対応する特別な値 ksize = FILTER_SCHARR (-1)もあります。この関数は，画像を適切なカーネルで畳み込むことにより，画像微分を計算します．\˶ˆ꒳ˆ˵ )\Sobel演算子は，ガウス平滑化と微分を組み合わせたものなので，多少なりともノイズに強い結果が得られます．多くの場合，この関数は ( xorder = 1, yorder = 0, ksize = 3) または ( xorder = 0, yorder = 1, ksize = 3) で呼び出され，xまたはyの画像1次微分を計算します．第1のケースは、次のようなカーネルに対応しています。\See alsoScharr, Laplacian, sepFilter2D, filter2D, GaussianBlur, cartToPolarExamples: samples/cpp/tutorial_code/ImgTrans/Sobel_Demo.cpp."
Calculates the first order image derivative in both x and y using a Sobel operator.,Sobel 演算子を使って、x と y の両方で画像の一次微分を計算します。
dx : output image with first-order derivative in x.,dx : xで一階微分した画像を出力。
dy : output image with first-order derivative in y.,dy : y方向に1次微分した画像を出力．
ksize : size of Sobel kernel. It must be 3.,ksize : Sobelカーネルのサイズ．3でなければなりません．
"borderType : pixel extrapolation method, see BorderTypes. Only BORDER_DEFAULT=BORDER_REFLECT_101 and BORDER_REPLICATE are supported.",borderType : ピクセル補外法，BorderTypes参照．BORDER_DEFAULT=BORDER_REFLECT_101 と BORDER_REPLICATE のみがサポートされている．
"Equivalent to calling:Sobel( src, dx, CV_16SC1, 1, 0, 3 );Sobel( src, dy, CV_16SC1, 0, 1, 3 );fragmentSee alsoSobel","以下のコマンドと同じです： Sobel( src, dx, CV_16SC1, 1, 0, 3 );Sobel( src, dy, CV_16SC1, 0, 1, 3 );fragmentSobel も参照してください．"
Calculates the first x- or y- image derivative using Scharr operator.,Scharr 演算子を用いて，画像の1次 x または y 導関数を求めます．
"ddepth : output image depth, see combinations",ddepth : 出力画像の深度，組み合わせを参照してください．
"The function computes the first x- or y- spatial image derivative using the Scharr operator. The call\[\texttt{Scharr(src, dst, ddepth, dx, dy, scale, delta, borderType)}\]is equivalent to\[\texttt{Sobel(src, dst, ddepth, dx, dy, FILTER_SCHARR, scale, delta, borderType)} .\]See alsocartToPolarExamples: samples/cpp/edge.cpp.","この関数は，Scharr演算子を用いて，XまたはYの空間的な1次微分を計算します．call\[Scharr(src, dst, ddepth, dx, dy, scale, delta, borderType)}\]は，次のものと同じです：Sobel(src, dst, ddepth, dx, dy, FILTER_SCHARR, scale, delta, borderType)} .̫⃝]See alsocartToPolarExamples: samples/cpp/edge.cpp."
Calculates the Laplacian of an image.,画像のラプラシアンを計算します。
ddepth : Desired depth of the destination image.,ddepth : 出力画像の深度を指定します．
ksize : Aperture size used to compute the second-derivative filters. See getDerivKernels for details. The size must be positive and odd.,ksize : 2次微分フィルタの計算に用いられるアパーチャサイズ．詳細は，getDerivKernels を参照してください．このサイズは，正かつ奇数でなければいけません．
"scale : Optional scale factor for the computed Laplacian values. By default, no scaling is applied. See getDerivKernels for details.",scale : オプションである，計算されたラプラシアン値のスケールファクター．デフォルトでは，スケーリングは行われません．詳しくは，getDerivKernels を参照してください．
delta : Optional delta value that is added to the results prior to storing them in dst .,delta :dst に格納する前に結果に追加されるデルタ値（オプション）．
"The function calculates the Laplacian of the source image by adding up the second x and y derivatives calculated using the Sobel operator:\[\texttt{dst} = \Delta \texttt{src} = \frac{\partial^2 \texttt{src}}{\partial x^2} + \frac{\partial^2 \texttt{src}}{\partial y^2}\]This is done when ksize > 1. When ksize == 1, the Laplacian is computed by filtering the image with the following \(3 \times 3\) aperture:\[\vecthreethree {0}{1}{0}{1}{-4}{1}{0}{1}{0}\]See alsoSobel, ScharrExamples: samples/cpp/laplace.cpp.","この関数は，Sobel演算子を用いて計算された2回目のx導関数とy導関数を足し合わせることで，入力画像のラプラシアンを計算します．+ ˶ˆ꒳ˆ˵ ) これは，ksize > 1のときに行う．ksize == 1のときは，次のようなアパーチャで画像をフィルタリングして，ラプラシアンを計算します♪See alsoSobel, ScharrExamples: samples/cpp/laplace.cpp."
Finds edges in an image using the Canny algorithm [41] .,Cannyアルゴリズム[41]を用いて，画像のエッジを検出します．
image : 8-bit input image.,image : 8ビットの入力画像．
"edges : output edge map; single channels 8-bit image, which has the same size as image .",edges : 出力されるエッジマップ．シングルチャンネルの8ビット画像で，imageと同じサイズです．
threshold1 : first threshold for the hysteresis procedure.,threshold1 : ヒステリシス処理のための最初の閾値．
threshold2 : second threshold for the hysteresis procedure.,threshold2 : ヒステリシス処理のための第二の閾値．
apertureSize : aperture size for the Sobel operator.,apertureSize : Sobel演算子のアパーチャサイズ．
"L2gradient : a flag, indicating whether a more accurate \(L_2\) norm \(=\sqrt{(dI/dx)^2 + (dI/dy)^2}\) should be used to calculate the image gradient magnitude ( L2gradient=true ), or whether the default \(L_1\) norm \(=|dI/dx|+|dI/dy|\) is enough ( L2gradient=false ).",L2gradient : フラグ．画像のグラデーションの大きさを計算するのに，より正確な ˶ˆ꒳ˆ˵ ( L2gradient=true ) を使うのか，それともデフォルトの ˶ˆ꒳ˆ˵ ( =|dI/dx|+|dI/dy|˶ ) で十分なのか ( L2gradient=false ) を示す．
"The function finds edges in the input image and marks them in the output map edges using the Canny algorithm. The smallest value between threshold1 and threshold2 is used for edge linking. The largest value is used to find initial segments of strong edges. See http://en.wikipedia.org/wiki/Canny_edge_detectorExamples: samples/cpp/edge.cpp, samples/cpp/squares.cpp, samples/cpp/tutorial_code/ImgTrans/houghlines.cpp, and samples/tapi/squares.cpp.","この関数は，Cannyアルゴリズムを用いて，入力画像中のエッジを検出し，出力マップのエッジにマーキングします．threshold1 と threshold2 の間の最小値が，エッジの連結に利用されます．また，最大の値は，強いエッジの初期セグメントを見つけるために利用されます．http://en.wikipedia.org/wiki/Canny_edge_detectorExamples: samples/cpp/edge.cpp, samples/cpp/squares.cpp, samples/cpp/tutorial_code/ImgTrans/houghlines.cpp, and samples/tapi/squares.cpp を参照してください。"
Calculates the minimal eigenvalue of gradient matrices for corner detection.,コーナー検出のために，勾配行列の最小固有値を計算します．
src : Input single-channel 8-bit or floating-point image.,src : シングルチャンネルの8ビットまたは浮動小数点型の入力画像．
dst : Image to store the minimal eigenvalues. It has the type CV_32FC1 and the same size as src .,dst : 最小固有値を保存するための画像．型は CV_32FC1 で，サイズは src と同じです．
blockSize : Neighborhood size (see the details on cornerEigenValsAndVecs ).,blockSize : 近傍領域のサイズ（ cornerEigenValsAndVecs の詳細を参照してください）．
ksize : Aperture parameter for the Sobel operator.,ksize : Sobel演算子のアパーチャパラメータ．
borderType : Pixel extrapolation method. See BorderTypes. BORDER_WRAP is not supported.,borderType :Pixel extrapolation method．BorderTypesを参照してください．BORDER_WRAPはサポートされていません．
"The function is similar to cornerEigenValsAndVecs but it calculates and stores only the minimal eigenvalue of the covariance matrix of derivatives, that is, \(\min(\lambda_1, \lambda_2)\) in terms of the formulae in the cornerEigenValsAndVecs description.","この関数は， cornerEigenValsAndVecs と似ていますが， cornerEigenValsAndVecs の説明にある式を用いて，導関数の共分散行列の最小固有値，つまり， ˶min(˶lambda_1,˶lambda_2)˶ を計算して保存します．"
Harris corner detector.,Harris corner detector.
dst : Image to store the Harris detector responses. It has the type CV_32FC1 and the same size as src .,dst : ハリス検出器の応答を格納する画像．型は CV_32FC1 で，サイズは src と同じです．
k : Harris detector free parameter. See the formula above.,k : ハリス検出器のフリーパラメータ．上の式を参照してください．
"The function runs the Harris corner detector on the image. Similarly to cornerMinEigenVal and cornerEigenValsAndVecs , for each pixel \((x, y)\) it calculates a \(2\times2\) gradient covariance matrix \(M^{(x,y)}\) over a \(\texttt{blockSize} \times \texttt{blockSize}\) neighborhood. Then, it computes the following characteristic:\[\texttt{dst} (x,y) = \mathrm{det} M^{(x,y)} - k \cdot \left ( \mathrm{tr} M^{(x,y)} \right )^2\]Corners in the image can be found as the local maxima of this response map.","この関数は，画像に対してハリスコーナー検出器を実行します．cornerMinEigenVal や cornerEigenValsAndVecs と同様に，各ピクセル ˶((x, y)˶ )に対して，勾配共分散行列˶((M^{(x,y)}˶ )を計算します．そして、次のような特性を計算します。(x,y) = ˶ˆ꒳ˆ˵ )M^{(x,y)} (x,y)- k cdot eldest ( ˶ˆ꒳ˆ˵ )M^{(x,y)}\画像の角は、この応答マップの局所的な最大値として見つけることができます。"
Calculates eigenvalues and eigenvectors of image blocks for corner detection.,コーナー検出のために，画像ブロックの固有値と固有ベクトルを計算します．
dst : Image to store the results. It has the same size as src and the type CV_32FC(6) .,dst : 結果を保存する画像．src と同じサイズで，型は CV_32FC(6) です．
blockSize : Neighborhood size (see details below).,blockSize : 近傍領域のサイズ（詳細は後述）．
"For every pixel \(p\) , the function cornerEigenValsAndVecs considers a blockSize \(\times\) blockSize neighborhood \(S(p)\) . It calculates the covariation matrix of derivatives over the neighborhood as:\[M = \begin{bmatrix} \sum _{S(p)}(dI/dx)^2 & \sum _{S(p)}dI/dx dI/dy \\ \sum _{S(p)}dI/dx dI/dy & \sum _{S(p)}(dI/dy)^2 \end{bmatrix}\]where the derivatives are computed using the Sobel operator.After that, it finds eigenvectors and eigenvalues of \(M\) and stores them in the destination image as \((\lambda_1, \lambda_2, x_1, y_1, x_2, y_2)\) where\(\lambda_1, \lambda_2\) are the non-sorted eigenvalues of \(M\)","関数 cornerEigenValsAndVecs は，各ピクセル ˶ˆ꒳ˆ˵ ) に対して， blockSize ˶ˆ꒳ˆ˵ ) 近隣領域 ˶ˆ꒳ˆ˵ ) を考慮します．この近傍領域における導関数の共分散行列を計算します。\ここでは、Sobel演算子を用いて導関数を計算します。固有ベクトルと固有値を求め、デスティネーション画像に保存します ˶((˶lambda_1, ˶lambda_2, x_1, y_1, x_2, y_2)˶) ここで、˶(˶lambda_1, ˶lambda_2˶)は、非ソートの固有値です"
"\(x_1, y_1\) are the eigenvectors corresponding to \(\lambda_1\)",\♪\\に対応する固有ベクトルは
"\(x_2, y_2\) are the eigenvectors corresponding to \(\lambda_2\)The output of the function can be used for robust edge or corner detection.See alsocornerMinEigenVal, cornerHarris, preCornerDetect","\この関数の出力は，ロバストなエッジ検出やコーナー検出に利用できます． 他にも，ocornerMinEigenVal, cornerHarris, preCornerDetect を参照してください．"
Calculates a feature map for corner detection.,コーナー検出のための特徴量マップを計算します．
src : Source single-channel 8-bit of floating-point image.,src : シングルチャンネル，8ビット浮動小数点型の入力画像．
dst : Output image that has the type CV_32F and the same size as src .,dst : 出力画像．タイプは CV_32F で，サイズは src と同じです．
ksize : Aperture size of the Sobel .,ksize : Sobel のアパーチャサイズ．
"The function calculates the complex spatial derivative-based function of the source image\[\texttt{dst} = (D_x \texttt{src} )^2 \cdot D_{yy} \texttt{src} + (D_y \texttt{src} )^2 \cdot D_{xx} \texttt{src} - 2 D_x \texttt{src} \cdot D_y \texttt{src} \cdot D_{xy} \texttt{src}\]where \(D_x\), \(D_y\) are the first image derivatives, \(D_{xx}\), \(D_{yy}\) are the second image derivatives, and \(D_{xy}\) is the mixed derivative.The corners can be found as local maximums of the functions, as shown below:Mat corners, dilated_corners;preCornerDetect(image, corners, 3);// dilation with 3x3 rectangular structuring elementdilate(corners, dilated_corners, Mat(), 1);Mat corner_mask = corners == dilated_corners;fragment","この関数は，入力画像の複素空間微分関数を計算します\\ = (D_x texttt{src} )^2 ୨୧ D_{yy}\♪♪♪♪♪♪♪♪♪♪♪♪♪+ (D_y ˶ˆ꒳ˆ˵ ) - D_{xx} ˶ˆ꒳ˆ˵ )-2 D_x ardor\D_y (D_texttt{src})\D_{xy}\ここで，\(D_x\)，\(D_y\)は1次微分，\(D_xx}˶)，┄(D_yy}˶)は2次微分，\(D_{xy}˶)は混合微分である．コーナーは，以下のように，関数の局所的な最大値として求められます： Mat corners, dilated_corners;preCornerDetect(image, corners, 3);// 3x3 の矩形構造による拡張 elementdilate(corner, dilated_corners, Mat(), 1);Mat corner_mask = corners == dilated_corners;fragment"
Refines the corner locations.,コーナーの位置を精細化します．
"image : Input single-channel, 8-bit or float image.",image : 入力されたシングルチャンネル，8ビットまたは浮動小数点型の画像．
corners : Initial coordinates of the input corners and refined coordinates provided for output.,corners : 入力コーナーの初期座標と，出力に提供される精密座標．
"winSize : Half of the side length of the search window. For example, if winSize=Size(5,5) , then a \((5*2+1) \times (5*2+1) = 11 \times 11\) search window is used.","winSize : 探索窓の辺の長さの半分．例えば，winSize=Size(5,5)であれば， ˶((5*2+1) ˶((5*2+1) = 11˶))の探索窓が使われる．"
"zeroZone : Half of the size of the dead region in the middle of the search zone over which the summation in the formula below is not done. It is used sometimes to avoid possible singularities of the autocorrelation matrix. The value of (-1,-1) indicates that there is no such a size.","zeroZone : 探索領域の中央に位置する不感帯領域の半分の大きさで、以下の式での総和が行われない領域。自己相関行列の特異点を避けるために使われることがあります。(-1,-1)の値は、そのようなサイズがないことを示す。"
"criteria : Criteria for termination of the iterative process of corner refinement. That is, the process of corner position refinement stops either after criteria.maxCount iterations or when the corner position moves by less than criteria.epsilon on some iteration.",基準:コーナーリファインメントの反復プロセスの終了基準。つまり，コーナー位置の精密化処理は， criteria.maxCount 回の繰り返しの後，あるいは，ある繰り返しにおいてコーナー位置が criteria.epsilon よりも小さい値で移動した場合に停止します．
"The function iterates to find the sub-pixel accurate location of corners or radial saddle points as described in [81], and as shown on the figure below.imageSub-pixel accurate corner locator is based on the observation that every vector from the center \(q\) to a point \(p\) located within a neighborhood of \(q\) is orthogonal to the image gradient at \(p\) subject to image and measurement noise. Consider the expression:\[\epsilon _i = {DI_{p_i}}^T \cdot (q - p_i)\]where \({DI_{p_i}}\) is an image gradient at one of the points \(p_i\) in a neighborhood of \(q\) . The value of \(q\) is to be found so that \(\epsilon_i\) is minimized. A system of equations may be set up with \(\epsilon_i\) set to zero:\[\sum _i(DI_{p_i} \cdot {DI_{p_i}}^T) \cdot q - \sum _i(DI_{p_i} \cdot {DI_{p_i}}^T \cdot p_i)\]where the gradients are summed within a neighborhood (""search window"") of \(q\) . Calling the first gradient term \(G\) and the second gradient term \(b\) gives:\[q = G^{-1} \cdot b\]The algorithm sets the center of the neighborhood window at this new center \(q\) and then iterates until the center stays within a set threshold.Examples: samples/cpp/lkdemo.cpp.",この関数は，[81]で述べられているように，サブピクセル精度でコーナーや半径方向のサドルポイントの位置を求めるために，繰り返し処理を行います． imageサブピクセル精度のコーナーロケータは，画像ノイズや計測ノイズがあったとしても，中心\(q\)からその近傍に位置する点\(p\)までのすべてのベクトルが，画像の勾配に直交するという観測に基づいています．式を考えてみましょう：\\ = {DI_{p_i}}^T \cdot (q - p_i)\]ここで、\({DI_{p_i}}\)は、\(q\)の近傍にある点の一つの画像勾配です。q\の値は、\(epsilon_i\)が最小になるように求められます。˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ )\ここで、勾配の総和は、\(q\)の近傍（サーチウィンドウ）で行われます。第1グラディエント項を「G\」、第2グラディエント項を「b\」と呼ぶと、次のようになります。
Determines strong corners on an image.,画像の強い角を判定します。
"image : Input 8-bit or floating-point 32-bit, single-channel image.",image : 入力8ビット，または浮動小数点型32ビット，シングルチャンネルの画像．
corners : Output vector of detected corners.,corners : 検出されたコーナーの出力ベクトル．
"maxCorners : Maximum number of corners to return. If there are more corners than are found, the strongest of them is returned. maxCorners <= 0 implies that no limit on the maximum is set and all detected corners are returned.",maxCorners : 返すべきコーナーの最大数．maxCorners <= 0 の場合は，最大値の制限はなく，検出された全てのコーナーが返されます．
"qualityLevel : Parameter characterizing the minimal accepted quality of image corners. The parameter value is multiplied by the best corner quality measure, which is the minimal eigenvalue (see cornerMinEigenVal ) or the Harris function response (see cornerHarris ). The corners with the quality measure less than the product are rejected. For example, if the best corner has the quality measure = 1500, and the qualityLevel=0.01 , then all the corners with the quality measure less than 15 are rejected.",qualityLevel : 画像のコーナーの品質の最小許容値を特徴づけるパラメータ。このパラメータ値には，最小固有値（cornerMinEigenVal 参照）やハリス関数応答（cornerHarris 参照）などの最適なコーナー品質指標が乗算されます．品質測定値が製品よりも小さいコーナーは拒否されます。例えば、最良のコーナーの品質尺度が1500で、qualityLevel=0.01の場合、品質尺度が15未満のコーナーはすべて拒否されます。
minDistance : Minimum possible Euclidean distance between the returned corners.,minDistance : 返されたコーナー間の可能な限りの最小ユークリッド距離．
"mask : Optional region of interest. If the image is not empty (it needs to have the type CV_8UC1 and the same size as image ), it specifies the region in which the corners are detected.",mask : オプションである，関心領域．画像が空ではない場合（CV_8UC1型で，imageと同じサイズである必要があります），コーナーが検出される領域を指定します．
blockSize : Size of an average block for computing a derivative covariation matrix over each pixel neighborhood. See cornerEigenValsAndVecs .,blockSize : 各ピクセルの近傍領域に対して微分共分散行列を計算するための，平均的なブロックのサイズ．cornerEigenValsAndVecs を参照してください．
useHarrisDetector : Parameter indicating whether to use a Harris detector (see cornerHarris) or cornerMinEigenVal.,useHarrisDetector : Harris検出器（ cornerHarris 参照）を利用するか， cornerMinEigenVal を利用するかを示すパラメータ．
k : Free parameter of the Harris detector.,k : Harris検出器のフリーパラメータ．
"The function finds the most prominent corners in the image or in the specified image region, as described in [222]Function calculates the corner quality measure at every source image pixel using the cornerMinEigenVal or cornerHarris .",この関数は，[222]に記述されているように，画像中，あるいは指定された画像領域中の最も目立つコーナーを検出します． 関数は， cornerMinEigenVal または cornerHarris を用いて，すべての入力画像ピクセルにおけるコーナー品質尺度を計算します．
Function performs a non-maximum suppression (the local maximums in 3 x 3 neighborhood are retained).,Functionは、非最大値抑制を行います（3×3近傍の局所的な最大値は保持されます）。
"The corners with the minimal eigenvalue less than \(\texttt{qualityLevel} \cdot \max_{x,y} qualityMeasureMap(x,y)\) are rejected.",最小固有値が\(˶ˆ꒳ˆ˵ )以下のコーナーは\未満の最小固有値を持つコーナーは棄却されます。
The remaining corners are sorted by the quality measure in the descending order.,残ったコーナーは、品質尺度の降順でソートされます。
"Function throws away each corner for which there is a stronger corner at a distance less than maxDistance.The function can be used to initialize a point-based tracker of an object.NoteIf the function is called with different values A and B of the parameter qualityLevel , and A > B, the vector of returned corners with qualityLevel=A will be the prefix of the output vector with qualityLevel=B .See alsocornerMinEigenVal, cornerHarris, calcOpticalFlowPyrLK, estimateRigidTransform,Examples: samples/cpp/lkdemo.cpp.","この関数は，物体のポイントベーストラッカーを初期化するために利用できます．注意もし，パラメータ qualityLevel に異なる値 A と B を指定してこの関数を呼び出し，A > B とした場合， qualityLevel=A で返されるコーナーのベクトルは， qualityLevel=B で返される出力ベクトルの接頭辞となります． 他にも，ocornerMinEigenVal, cornerHarris, calcOpticalFlowPyrLK, estimateRigidTransform などの関数を参照してください．"
Finds lines in a binary image using the standard Hough transform.,標準的なハフ変換を用いて，2 値画像中の線を検出します．
"image : 8-bit, single-channel binary source image. The image may be modified by the function.",image : 8ビット，シングルチャンネルの2値ソース画像．この画像は，この関数によって変更することができます．
"lines : Output vector of lines. Each line is represented by a 2 or 3 element vector \((\rho, \theta)\) or \((\rho, \theta, \textrm{votes})\) . \(\rho\) is the distance from the coordinate origin \((0,0)\) (top-left corner of the image). \(\theta\) is the line rotation angle in radians ( \(0 \sim \textrm{vertical line}, \pi/2 \sim \textrm{horizontal line}\) ). \(\textrm{votes}\) is the value of accumulator.",lines : 線の出力ベクトル．各線は，2要素または3要素のベクトル ˶((˶´⚰︎`˵ )) または˶((˶´⚰︎`˵ )) で表されます．\は、座標原点からの距離です。(画像の左上隅）からの距離です。\視線の回転角度（ラジアン）です。\˶ˆ꒳ˆ˵ )
rho : Distance resolution of the accumulator in pixels.,rho : アキュムレータの距離分解能（ピクセル単位）．
theta : Angle resolution of the accumulator in radians.,theta :アキュムレータの角度分解能（ラジアン単位）を表します。
threshold : Accumulator threshold parameter. Only those lines are returned that get enough votes ( \(>\texttt{threshold}\) ).,threshold : アキュムレータの閾値パラメータ．十分な投票数を得たラインのみが返されます( ˶ˆ꒳ˆ˵ )
"srn : For the multi-scale Hough transform, it is a divisor for the distance resolution rho . The coarse accumulator distance resolution is rho and the accurate accumulator resolution is rho/srn . If both srn=0 and stn=0 , the classical Hough transform is used. Otherwise, both these parameters should be positive.",srn : マルチスケールハフ変換では，距離分解能 rho の除数になります．粗い累算器の距離分解能は rho であり，正確な累算器の分解能は rho/srn である．srn=0とstn=0の場合は，古典的なハフ変換が用いられる。それ以外の場合，これらのパラメータは両方とも正でなければならない。
"stn : For the multi-scale Hough transform, it is a divisor for the distance resolution theta.",stn : マルチスケールハフ変換では，距離分解能 theta の除数となります．
"min_theta : For standard and multi-scale Hough transform, minimum angle to check for lines. Must fall between 0 and max_theta.",min_theta :標準およびマルチスケールハフ変換において，線の有無をチェックするための最小角度．0とmax_thetaの間でなければなりません。
"max_theta : For standard and multi-scale Hough transform, maximum angle to check for lines. Must fall between min_theta and CV_PI.",max_theta :標準およびマルチスケールのハフ変換において，線の有無を確認するための最大の角度を指定します．min_theta と CV_PI の間にある必要があります．
The function implements the standard or standard multi-scale Hough transform algorithm for line detection. See http://homepages.inf.ed.ac.uk/rbf/HIPR2/hough.htm for a good explanation of Hough transform.Examples: samples/cpp/tutorial_code/ImgTrans/houghlines.cpp.,この関数は，線を検出するための標準的なハフ変換アルゴリズム，あるいは標準的なマルチスケールハフ変換アルゴリズムを実装しています．ハフ変換についての詳しい説明は，http://homepages.inf.ed.ac.uk/rbf/HIPR2/hough.htm を参照してください．例： samples/cpp/tutorial_code/mgTrans/houghlines.cpp.
Finds line segments in a binary image using the probabilistic Hough transform.,確率的ハフ変換を用いて2値画像の線分を検出します。
"lines : Output vector of lines. Each line is represented by a 4-element vector \((x_1, y_1, x_2, y_2)\) , where \((x_1,y_1)\) and \((x_2, y_2)\) are the ending points of each detected line segment.","lines : 線分の出力ベクトル．各線は，4要素のベクトル ˶((x_1, y_1, x_2, y_2)˶)で表されます．ここで， ˶((x_1,y_1)˶)と˶((x_2,y_2)˶)は，検出された各線分の終点である．"
minLineLength : Minimum line length. Line segments shorter than that are rejected.,minLineLength : 最小線長。これより短い線分は拒否される。
maxLineGap : Maximum allowed gap between points on the same line to link them.,maxLineGap : 同一線上の点と点を結ぶための最大許容ギャップ．
"The function implements the probabilistic Hough transform algorithm for line detection, described in [164]See the line detection example below:#include <opencv2/imgproc.hpp>#include <opencv2/highgui.hpp>using namespace cv;using namespace std;int main(int argc, char** argv){    Mat src, dst, color_dst;    if( argc != 2 || !(src=imread(argv[1], 0)).data)        return -1;    Canny( src, dst, 50, 200, 3 );    cvtColor( dst, color_dst, COLOR_GRAY2BGR );    vector<Vec4i> lines;    HoughLinesP( dst, lines, 1, CV_PI/180, 80, 30, 10 );    for( size_t i = 0; i < lines.size(); i++ )    {        line( color_dst, Point(lines[i][0], lines[i][1]),        Point( lines[i][2], lines[i][3]), Scalar(0,0,255), 3, 8 );    }    namedWindow( ""Source"", 1 );    imshow( ""Source"", src );    namedWindow( ""Detected Lines"", 1 );    imshow( ""Detected Lines"", color_dst );    waitKey(0);    return 0;}fragmentThis is a sample picture the function parameters have been tuned for:imageAnd this is the output of the above program in case of the probabilistic Hough transform:imageSee alsoLineSegmentDetectorExamples: samples/cpp/tutorial_code/ImgTrans/houghlines.cpp.","この関数は，[164]に記述されている，線検出のための確率的ハフ変換アルゴリズムを実装しています：#include <opencv2/imgproc.hpp>#include <opencv2/highgui.hpp>using namespace cv;using namespace std;int main(int argc, char** argv){ Mat src, dst, color_dst; if( argc != 2 || !(src=imread(argv[1], 0).data) return -1; Canny( src, dst, 50, 200, 3 ); cvtColor( dst, color_dst, COLOR_GRAY2BGR ); vector<Vec4i> lines; HoughLinesP( dst, lines, 1, CV_PI/180, 80, 30, 10 ); for( size_t i = 0; i < lines.size(); i++ ) { line( color_dst, Point(lines[i][0], lines[i][1]), Point( lines[i][2], lines[i][3]), Scalar(0,0,255), 3, 8 ); } namedWindow(""Source"", 1 ); imshow(""Source"", src );    namedWindow(""Detected Lines"", 1 ); imshow(""Detected Lines"", color_dst ); waitKey(0); return 0;}fragmentこれは、関数のパラメータを調整した画像のサンプルです。imageそして、これは確率的ハフ変換の場合の上記プログラムの出力です:imageSee alsoLineSegmentDetectorExamples: samples/cpp/tutorial_code/ImgTrans/houghlines.cppを参照してください。"
Finds lines in a set of points using the standard Hough transform.,標準的なハフ変換を利用して、点の集合の中の線を見つけます。
"point : Input vector of points. Each vector must be encoded as a Point vector \((x,y)\). Type must be CV_32FC2 or CV_32SC2.","point : 点の入力ベクトル．各ベクトルは，Point vector ˶((x,y)˶)としてエンコードされなければいけません．タイプは CV_32FC2 または CV_32SC2 でなければいけません．"
"lines : Output vector of found lines. Each vector is encoded as a vector<Vec3d> \((votes, rho, theta)\). The larger the value of 'votes', the higher the reliability of the Hough line.","lines : 見つかった線の出力ベクトル．各ベクトルは， vector<Vec3d> ˶((votes, rho, theta)˶)としてエンコードされます．votes' の値が大きいほど，ハフ線の信頼性が高いことを意味します．"
lines_max : Max count of hough lines.,lines_max :ハフ線の最大数を指定します。
threshold : Accumulator threshold parameter. Only those lines are returned that get enough votes ( \(>\texttt{threshold}\) ),threshold : アキュムレータの閾値パラメータ．十分な票数を獲得したラインのみが返されます ( ˶‾᷄ -̫ ‾᷅˵ )
min_rho : Minimum Distance value of the accumulator in pixels.,min_rho : アキュムレータの最小距離をピクセル単位で表します。
max_rho : Maximum Distance value of the accumulator in pixels.,max_rho : アキュムレータの最大距離をピクセル単位で表したものです。
rho_step : Distance resolution of the accumulator in pixels.,rho_step :rho_step : アキュムレータの距離分解能をピクセル単位で表したものです。
min_theta : Minimum angle value of the accumulator in radians.,min_theta :min_theta : アキュムレータの最小角度をラジアンで表したものです。
max_theta : Maximum angle value of the accumulator in radians.,max_theta :max_theta : アキュムレータの最大角度をラジアンで表したものです。
theta_step : Angle resolution of the accumulator in radians.,theta_step : 累算器の角度分解能（ラジアン単位）．
"The function finds lines in a set of points using a modification of the Hough transform.#include <opencv2/core.hpp>#include <opencv2/imgproc.hpp>using namespace cv;using namespace std;int main(){    Mat lines;    vector<Vec3d> line3d;    vector<Point2f> point;    const static float Points[20][2] = {    { 0.0f,   369.0f }, { 10.0f,  364.0f }, { 20.0f,  358.0f }, { 30.0f,  352.0f },    { 40.0f,  346.0f }, { 50.0f,  341.0f }, { 60.0f,  335.0f }, { 70.0f,  329.0f },    { 80.0f,  323.0f }, { 90.0f,  318.0f }, { 100.0f, 312.0f }, { 110.0f, 306.0f },    { 120.0f, 300.0f }, { 130.0f, 295.0f }, { 140.0f, 289.0f }, { 150.0f, 284.0f },    { 160.0f, 277.0f }, { 170.0f, 271.0f }, { 180.0f, 266.0f }, { 190.0f, 260.0f }    };    for (int i = 0; i < 20; i++)    {        point.push_back(Point2f(Points[i][0],Points[i][1]));    }    double rhoMin = 0.0f, rhoMax = 360.0f, rhoStep = 1;    double thetaMin = 0.0f, thetaMax = CV_PI / 2.0f, thetaStep = CV_PI / 180.0f;    HoughLinesPointSet(point, lines, 20, 1,                       rhoMin, rhoMax, rhoStep,                       thetaMin, thetaMax, thetaStep);    lines.copyTo(line3d);    printf(""votes:%d, rho:%.7f, theta:%.7f\n"",(int)line3d.at(0).val[0], line3d.at(0).val[1], line3d.at(0).val[2]);}fragment","この関数は，ハフ変換の改良版を用いて，点群から直線を見つけます．hpp>using namespace cv;using namespace std;int main(){ Mat lines; vector<Vec3d> line3d; vector<Point2f> point; const static float Points[20][2] = { { 0.0f, 369.0f }, { 10.0f, 364.0f }, { 20.0f, 358.0f }, { 30.0f, 352.0f }, { 40.0f, 346.0f }, { 50.0f, 341.0f }, { 60.0f, 335.0f }, { 70.0f, 329.0f }, { 80.0f, 323.0f }, { 90.0f, 318.0f }, { 100.0f, 312.0f }, { 110.0.0f, 306.0f }, { 120.0f, 300.0f }, { 130.0f, 295.0f }, { 140.0f, 289.0f }, { 150.0f, 284.0f }, { 160.0f, 277.0f }, { 170.0f, 271.0f }, { 180.0f, 266.0f }, { 190.0f, 260.0f }。    }; for (int i = 0; i < 20; i++) { point.push_back(Point2f(Points[i][0],Points[i][1])); } double rhoMin = 0.0f, rhoMax = 360.0f, rhoStep = 1; double thetaMin = 0.0f, thetaMax = CV_PI / 2.0f, thetaStep = CV_PI / 180.0f; HoughLinesPointSet(point, lines, 20, 1, rhoMin, rhoMax, rhoStep, thetaMin, thetaMax, thetaStep); lines.copyTo(line3d); printf(""vots:%d, rho:%.7f, theta:%.7fn"",(int)line3d.at(0).val[0], line3d.at(0).val[1], line3d.at(0).val[2]);}fragment"
Finds circles in a grayscale image using the Hough transform.,ハフ変換を使ってグレースケール画像から円を見つけます。
"image : 8-bit, single-channel, grayscale input image.",image : 8ビット，シングルチャンネル，グレースケールの入力画像．
"circles : Output vector of found circles. Each vector is encoded as 3 or 4 element floating-point vector \((x, y, radius)\) or \((x, y, radius, votes)\) .","circles :見つかった円の出力ベクトル．各ベクトルは，3要素または4要素の浮動小数点型ベクトル ˶((x, y, radius)˶)または˶((x, y, radius, votes)˶)としてエンコードされます．"
"method : Detection method, see HoughModes. The available methods are HOUGH_GRADIENT and HOUGH_GRADIENT_ALT.",method : 検出方法，HoughModesを参照．使用可能なメソッドは，HOUGH_GRADIENT と HOUGH_GRADIENT_ALT です．
"dp : Inverse ratio of the accumulator resolution to the image resolution. For example, if dp=1 , the accumulator has the same resolution as the input image. If dp=2 , the accumulator has half as big width and height. For HOUGH_GRADIENT_ALT the recommended value is dp=1.5, unless some small very circles need to be detected.",dp :画像の解像度に対するアキュムレータの解像度の逆数．例えば，dp=1 の場合，アキュムレータの解像度は入力画像と同じになります．dp=2 の場合，アキュムレータの幅と高さは半分になります．HOUGH_GRADIENT_ALTでは，非常に小さな円を検出する必要がない限り，dp=1.5を推奨します．
"minDist : Minimum distance between the centers of the detected circles. If the parameter is too small, multiple neighbor circles may be falsely detected in addition to a true one. If it is too large, some circles may be missed.",minDist :検出された円の中心間の最小距離を指定します。このパラメータが小さすぎると、真の円に加えて複数の隣接する円が誤検出される可能性がある。また，大きすぎると，いくつかの円を見逃してしまう可能性があります。
"param1 : First method-specific parameter. In case of HOUGH_GRADIENT and HOUGH_GRADIENT_ALT, it is the higher threshold of the two passed to the Canny edge detector (the lower one is twice smaller). Note that HOUGH_GRADIENT_ALT uses Scharr algorithm to compute image derivatives, so the threshold value shough normally be higher, such as 300 or normally exposed and contrasty images.",param1 : 最初の手法固有のパラメータ。HOUGH_GRADIENTとHOUGH_GRADIENT_ALTの場合、Cannyエッジ検出器に渡される2つの閾値のうち、高い方の閾値です（低い方は2倍小さくなります）。なお，HOUGH_GRADIENT_ALT は，画像の微分を計算するのに Scharr アルゴリズムを利用するので，通常は，300 や，通常の露出でコントラストの高い画像のように，閾値を高くする必要があります．
"param2 : Second method-specific parameter. In case of HOUGH_GRADIENT, it is the accumulator threshold for the circle centers at the detection stage. The smaller it is, the more false circles may be detected. Circles, corresponding to the larger accumulator values, will be returned first. In the case of HOUGH_GRADIENT_ALT algorithm, this is the circle ""perfectness"" measure. The closer it to 1, the better shaped circles algorithm selects. In most cases 0.9 should be fine. If you want get better detection of small circles, you may decrease it to 0.85, 0.8 or even less. But then also try to limit the search range [minRadius, maxRadius] to avoid many false circles.","param2 : 2番目の手法固有のパラメータ．HOUGH_GRADIENTの場合は，検出段階での円中心の累積しきい値です．この値が小さいほど，誤った円が多く検出される可能性があります。大きいアキュムレータ値に対応するサークルが最初に返されます。HOUGH_GRADIENT_ALTアルゴリズムの場合、これは円の「完璧さ」の尺度です。1に近ければ近いほど、アルゴリズムは良い形の円を選択します。ほとんどの場合、0.9で十分です。小さな円をよりよく検出したい場合は、0.85や0.8、あるいはそれ以下に下げてもよいでしょう。ただし、多くの偽の円を避けるために、検索範囲[minRadius, maxRadius]を制限することもお勧めします。"
minRadius : Minimum circle radius.,minRadius : 円の最小半径。
"maxRadius : Maximum circle radius. If <= 0, uses the maximum image dimension. If < 0, HOUGH_GRADIENT returns centers without finding the radius. HOUGH_GRADIENT_ALT always computes circle radiuses.",maxRadius : 円の最大半径を指定します。<= 0の場合は、画像の最大寸法を使用します。<0の場合、HOUGH_GRADIENTは半径を求めずにセンターを返す。HOUGH_GRADIENT_ALT は常に円の半径を計算する。
"The function finds circles in a grayscale image using a modification of the Hough transform.Example: :#include <opencv2/imgproc.hpp>#include <opencv2/highgui.hpp>#include <math.h>using namespace cv;using namespace std;int main(int argc, char** argv){    Mat img, gray;    if( argc != 2 || !(img=imread(argv[1], 1)).data)        return -1;    cvtColor(img, gray, COLOR_BGR2GRAY);    // smooth it, otherwise a lot of false circles may be detected    GaussianBlur( gray, gray, Size(9, 9), 2, 2 );    vector<Vec3f> circles;    HoughCircles(gray, circles, HOUGH_GRADIENT,                 2, gray.rows/4, 200, 100 );    for( size_t i = 0; i < circles.size(); i++ )    {         Point center(cvRound(circles[i][0]), cvRound(circles[i][1]));         int radius = cvRound(circles[i][2]);         // draw the circle center         circle( img, center, 3, Scalar(0,255,0), -1, 8, 0 );         // draw the circle outline         circle( img, center, radius, Scalar(0,0,255), 3, 8, 0 );    }    namedWindow( ""circles"", 1 );    imshow( ""circles"", img );    waitKey(0);    return 0;}fragmentNoteUsually the function detects the centers of circles well. However, it may fail to find correct radii. You can assist to the function by specifying the radius range ( minRadius and maxRadius ) if you know it. Or, in the case of HOUGH_GRADIENT method you may set maxRadius to a negative number to return centers only without radius search, and find the correct radius using an additional procedure.It also helps to smooth image a bit unless it's already soft. For example, GaussianBlur() with 7x7 kernel and 1.5x1.5 sigma or similar blurring may help.See alsofitEllipse, minEnclosingCircleExamples: samples/cpp/tutorial_code/ImgTrans/houghcircles.cpp.","この関数は，ハフ変換の修正を用いて，グレースケール画像中の円を求めます．例： :#include <opencv2/imgproc.hpp>#include <opencv2/highgui.hpp>#include <math.h>using namespace cv;using namespace std;int main(int argc, char** argv){ Mat img, gray; if( argc != 2 || !(img=imread(argv[1], 1)).data) return -1; cvtColor(img, gray, COLOR_BGR2GRAY); // 滑らかにしないと，たくさんの偽の円が検出される可能性があります． GaussianBlur( gray, gray, Size(9, 9), 2, 2 ); vector<Vec3f> circles; HoughCircles(gray, circles, HOUGH_GRADIENT, 2, gray.rows/4, 200, 100 ); for( size_t i = 0; i < circles.size(); i++ ) { Point center(cvRound(circcles[i][0]), cvRound(circcles[i][1])); int radius = cvRound(circcles[i][2]); // 円の中心を描く circle( img, center, 3, Scalar(0,255,0), -1, 8, 0 );         // 円の輪郭を描く circle( img, center, radius, Scalar(0,0,255), 3, 8, 0 ); } namedWindow( ""circles"", 1 ); imshow( ""circles"", img ); waitKey(0); return 0;}fragmentNote通常、この関数は円の中心をうまく検出します。しかし、正しい半径を見つけることができない場合があります。半径の範囲（minRadiusとmaxRadius）がわかっていれば、それを指定することで、この関数を補助することができます。また、HOUGH_GRADIENTメソッドの場合、maxRadiusを負の数に設定することで、半径検索を行わずに中心のみを返し、追加の手順で正しい半径を見つけることができます。また、画像がすでに柔らかい場合を除き、画像を少し滑らかにすることもできます。他にもfitEllipse, minEnclosingCircleExamples: samples/cpp/tutorial_code/ImgTrans/houghcircles.cppを参照してください。"
Erodes an image by using a specific structuring element.,特定の構造化要素を使って画像を消去します。
"src : input image; the number of channels can be arbitrary, but the depth should be one of CV_8U, CV_16U, CV_16S, CV_32F or CV_64F.","src : 入力画像．チャンネル数は任意ですが，深さは CV_8U, CV_16U, CV_16S, CV_32F, CV_64F のいずれかでなければいけません．"
"kernel : structuring element used for erosion; if element=Mat(), a 3 x 3 rectangular structuring element is used. Kernel can be created using getStructuringElement.",kernel : 浸食に利用される構造要素．element=Mat() の場合は，3 x 3 の矩形の構造要素が利用されます．カーネルは， getStructuringElement を用いて作成できます．
"anchor : position of the anchor within the element; default value (-1, -1) means that the anchor is at the element center.","anchor : 要素内におけるアンカーの位置; デフォルト値 (-1, -1) は，アンカーが要素の中心にあることを意味します．"
iterations : number of times erosion is applied.,iterations : エロージョンを適用する回数。
borderValue : border value in case of a constant border,borderValue : ボーダーが一定の場合のボーダー値．
"The function erodes the source image using the specified structuring element that determines the shape of a pixel neighborhood over which the minimum is taken:\[\texttt{dst} (x,y) = \min _{(x',y'): \, \texttt{element} (x',y') \ne0 } \texttt{src} (x+x',y+y')\]The function supports the in-place mode. Erosion can be applied several ( iterations ) times. In case of multi-channel images, each channel is processed independently.See alsodilate, morphologyEx, getStructuringElementExamples: samples/cpp/segment_objects.cpp, and samples/cpp/tutorial_code/ImgProc/Morphology_1.cpp.","この関数は，最小値を取るピクセル近傍の形状を決定する，指定された構造化要素を用いて，入力画像を侵食します：˶˙º̬˙˶(x,y) = ˶ˆ꒳ˆ˵ (x',y'):\♪ ♪♪ (x',y') ♪ ♪ (x',y') ♪ ♪ (x',y') ♪ ♪\♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪(x+x',y+y')⋈◍＞◡̈⃝︎*。エロージョンは，複数回（イタレーション）にわたって適用することができます．他にも，odilate, morphologyEx, getStructuringElementExamples: samples/cpp/segment_objects.cpp, and samples/cpp/tutorial_code/ImgProc/Morphology_1.cppを参照してください．"
Dilates an image by using a specific structuring element.,特定の構造化要素を用いて画像を拡張します。
"kernel : structuring element used for dilation; if elemenat=Mat(), a 3 x 3 rectangular structuring element is used. Kernel can be created using getStructuringElement",kernel : 拡張に使用される構造化要素．elemenat=Mat() の場合は，3 x 3 の矩形の構造化要素が使用されます．カーネルは，getStructuringElementを用いて作成できます．
iterations : number of times dilation is applied.,iterations : 拡張を行う回数．
"borderType : pixel extrapolation method, see BorderTypes. BORDER_WRAP is not suported.",borderType : ピクセルの外挿法，BorderTypesを参照．BORDER_WRAP はサポートされていません。
"The function dilates the source image using the specified structuring element that determines the shape of a pixel neighborhood over which the maximum is taken:\[\texttt{dst} (x,y) = \max _{(x',y'): \, \texttt{element} (x',y') \ne0 } \texttt{src} (x+x',y+y')\]The function supports the in-place mode. Dilation can be applied several ( iterations ) times. In case of multi-channel images, each channel is processed independently.See alsoerode, morphologyEx, getStructuringElementExamples: samples/cpp/segment_objects.cpp, samples/cpp/squares.cpp, samples/cpp/stitching_detailed.cpp, samples/cpp/tutorial_code/ImgProc/Morphology_1.cpp, and samples/tapi/squares.cpp.","この関数は，最大値が取られるピクセル近傍の形状を決定する，指定された構造化要素を用いて，入力画像を拡張します：˶ˆ꒳ˆ˵ )(x,y) = ″max _{(x',y') ""です。\♪ ♪(x',y') ˶‾᷅˵‾᷅˵｝\♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪(x+x',y+y')⋈⋈◍＞◡̈*。ダイレーションは，複数回（イタレーション）行うことができます．See alsoerode, morphologyEx, getStructuringElementExamples: samples/cpp/segment_objects.cpp, samples/cpp/squares.cpp, samples/cpp/stitching_detailed.cpp, samples/cpp/tutorial_code/ImgProc/Morphology_1.cpp, and samples/tapi/squares.cpp."
Performs advanced morphological transformations.,高度なモルフォロジー変換を行います。
"src : Source image. The number of channels can be arbitrary. The depth should be one of CV_8U, CV_16U, CV_16S, CV_32F or CV_64F.","src : ソース画像．チャンネル数は，任意です．深度は，CV_8U, CV_16U, CV_16S, CV_32F, CV_64F のいずれかでなければいけません．"
dst : Destination image of the same size and type as source image.,dst : 入力画像と同じサイズ，同じ種類の出力画像．
"op : Type of a morphological operation, see MorphTypes",op :MorphTypesを参照してください．
kernel : Structuring element. It can be created using getStructuringElement.,kernel : 構造化要素．getStructuringElementを用いて作成することができます．
anchor : Anchor position with the kernel. Negative values mean that the anchor is at the kernel center.,anchor : カーネルに対するアンカーの位置。負の値は、アンカーがカーネルの中心にあることを意味します。
iterations : Number of times erosion and dilation are applied.,iterations :エロージョンとダイレーションが適用される回数。
borderValue : Border value in case of a constant border. The default value has a special meaning.,borderValue : ボーダーが一定の場合のボーダー値。デフォルト値は，特別な意味を持ちます．
"The function cv::morphologyEx can perform advanced morphological transformations using an erosion and dilation as basic operations.Any of the operations can be done in-place. In case of multi-channel images, each channel is processed independently.See alsodilate, erode, getStructuringElementNoteThe number of iterations is the number of times erosion or dilatation operation will be applied. For instance, an opening operation (MORPH_OPEN) with two iterations is equivalent to apply successively: erode -> erode -> dilate -> dilate (and not erode -> dilate -> erode -> dilate).Examples: samples/cpp/tutorial_code/ImgProc/Morphology_2.cpp.","関数 cv::morphologyEx は，エロージョンとダイレーションを基本操作として，高度な形態素変換を行うことができます．また，odilate, erode, getStructuringElementも参照してください．注釈反復回数とは，侵食や拡張の操作が適用される回数です．例えば、2回の繰り返しによるオープニング操作(MORPH_OPEN)は、erode -> erode -> dilate -> dilate (erode -> dilate -> erode -> dilateではない)を連続して適用することに相当します。例：samples/cpp/tutorial_code/ImgProc/Morphology_2.cpp."
Resizes an image.,画像のリサイズを行います．
"dst : output image; it has the size dsize (when it is non-zero) or the size computed from src.size(), fx, and fy; the type of dst is the same as of src.","dst : 出力画像．サイズが dsize（0以外の場合），または src.size(), fx, fy から計算されたサイズを持ちます； dst の型は，src と同じです．"
"dsize : output image size; if it equals zero, it is computed as: ",dsize : 出力画像のサイズ．これが0の場合，次のように計算されます．
"\[\texttt{dsize = Size(round(fx*src.cols), round(fy*src.rows))}\]","\dsize = Size(round(fx*src.cols), round(fy*src.rows))}\]となります．"
 Either dsize or both fx and fy must be non-zero., dsizeか、fxとfyの両方が0以外でなければならない。
"fx : scale factor along the horizontal axis; when it equals 0, it is computed as ",fx : 水平軸のスケールファクター．これが0の場合，次のように計算される．
\[\texttt{(double)dsize.width/src.cols}\],\dsize.width/src.cols}\]となります。
"fy : scale factor along the vertical axis; when it equals 0, it is computed as ",fy：縦軸方向のスケールファクターです。
\[\texttt{(double)dsize.height/src.rows}\],\FY：縦軸のスケールファクター。
"interpolation : interpolation method, see InterpolationFlags",interpolation : 補間手法，InterpolationFlags を参照してください．
"The function resize resizes the image src down to or up to the specified size. Note that the initial dst type or size are not taken into account. Instead, the size and type are derived from the src,dsize,fx, and fy. If you want to resize src so that it fits the pre-created dst, you may call the function as follows:// explicitly specify dsize=dst.size(); fx and fy will be computed from that.resize(src, dst, dst.size(), 0, 0, interpolation);fragmentIf you want to decimate the image by factor of 2 in each direction, you can call the function this way:// specify fx and fy and let the function compute the destination image size.resize(src, dst, Size(), 0.5, 0.5, interpolation);fragmentTo shrink an image, it will generally look best with INTER_AREA interpolation, whereas to enlarge an image, it will generally look best with c::INTER_CUBIC (slow) or INTER_LINEAR (faster but still looks OK).See alsowarpAffine, warpPerspective, remapExamples: samples/cpp/image_alignment.cpp, samples/cpp/train_HOG.cpp, samples/dnn/classification.cpp, samples/dnn/colorization.cpp, samples/dnn/object_detection.cpp, and samples/dnn/segmentation.cpp.","関数 resize は，画像 src を指定されたサイズに縮小あるいは拡大します．ただし，初期状態の dst のタイプやサイズは考慮されません．代わりに， src,dsize,fx,fy からサイズとタイプが導かれます．あらかじめ作成された dst に合うように src のサイズを変更したい場合は，次のようにこの関数を呼び出します： // 明示的に dsize=dst.size() を指定します．fx とfy は，that.resize(src, dst, dst.size(), 0, 0, interpolation);fragment画像を各方向に2分の1ずつ縮小したい場合は，次のように関数を呼び出します： // fx とfy を指定して，関数に出力画像サイズを計算させます．5, interpolation);fragment画像を縮小する場合は，一般的にINTER_AREA補間が最適です．一方，画像を拡大する場合は，一般的にc::INTER_CUBIC（遅い）またはINTER_LINEAR（速いが，それでも問題ない）が最適です．alsowarpAffine, warpPerspective, remapExamples: samples/cpp/image_alignment.cpp, samples/cpp/train_HOG.cpp, samples/dnn/classification.cpp, samples/dnn/colorization.cpp, samples/dnn/object_detection.cpp, and samples/dnn/segmentation.cppを参照してください。"
Applies an affine transformation to an image.,画像にアフィン変換を施します。
dst : output image that has the size dsize and the same type as src .,dst : src と同じ型で，サイズが dsize である出力画像．
M : \(2\times 3\) transformation matrix.,M : ˶‾᷄ -̫ ‾᷅˵ 変換行列
dsize : size of the output image.,dsize : 出力画像のサイズ．
flags : combination of interpolation methods (see InterpolationFlags) and the optional flag WARP_INVERSE_MAP that means that M is the inverse transformation ( \(\texttt{dst}\rightarrow\texttt{src}\) ).,flags : 補間手法（InterpolationFlags 参照）と，オプションフラグである WARP_INVERSE_MAP の組み合わせ．
"borderMode : pixel extrapolation method (see BorderTypes); when borderMode=BORDER_TRANSPARENT, it means that the pixels in the destination image corresponding to the ""outliers"" in the source image are not modified by the function.","borderMode : pixel extrapolation method（BorderTypes参照）． borderMode=BORDER_TRANSPARENTの場合，ソース画像中の ""外れ値 ""に対応するデスティネーション画像中のピクセルは，この関数によって変更されないことを意味する．"
"borderValue : value used in case of a constant border; by default, it is 0.",borderValue : ボーダーが一定の場合に利用される値．デフォルトでは，0です．
"The function warpAffine transforms the source image using the specified matrix:\[\texttt{dst} (x,y) = \texttt{src} ( \texttt{M} _{11} x + \texttt{M} _{12} y + \texttt{M} _{13}, \texttt{M} _{21} x + \texttt{M} _{22} y + \texttt{M} _{23})\]when the flag WARP_INVERSE_MAP is set. Otherwise, the transformation is first inverted with invertAffineTransform and then put in the formula above instead of M. The function cannot operate in-place.See alsowarpPerspective, resize, remap, getRectSubPix, transformExamples: samples/cpp/image_alignment.cpp.","関数 warpAffine は，指定された行列を用いて入力画像を変換します．(x,y) = ˶˙º̬˙˶ ( ˶ˆ꒳ˆ˵ )_{11} x + ˶ˆ꒳ˆ˵ )_{12} y + ˶ˆ꒳ˆ˵ )_{13}, ˶ˆ꒳ˆ˵ )_{21} x + ˶ˆ꒳ˆ˵_{22} y + ˶ˆ꒳ˆ˵ )_{23})は，フラグ WARP_INVERSE_MAP が設定されている場合に使用されます。他にも，owarpPerspective, resize, remap, getRectSubPix, transformExamples: samples/cpp/image_alignment.cpp を参照してください．"
Applies a perspective transformation to an image.,画像に透視変換を施します。
M : \(3\times 3\) transformation matrix.,M : ˶‾᷄ -̫ ‾᷅˵ 変換行列。
"flags : combination of interpolation methods (INTER_LINEAR or INTER_NEAREST) and the optional flag WARP_INVERSE_MAP, that sets M as the inverse transformation ( \(\texttt{dst}\rightarrow\texttt{src}\) ).",flags : 補間手法（INTER_LINEAR or INTER_NEAREST）と，オプションフラグ WARP_INVERSE_MAP の組み合わせで，M を逆変換（˶ˆ꒳ˆ˵）として設定します．
borderMode : pixel extrapolation method (BORDER_CONSTANT or BORDER_REPLICATE).,borderMode ：画素の外挿法（BORDER_CONSTANTまたはBORDER_REPLICATE）．
"borderValue : value used in case of a constant border; by default, it equals 0.",borderValue : ボーダーが一定の場合に使用される値で，デフォルトでは0になっています．
"The function warpPerspective transforms the source image using the specified matrix:\[\texttt{dst} (x,y) = \texttt{src} \left ( \frac{M_{11} x + M_{12} y + M_{13}}{M_{31} x + M_{32} y + M_{33}} , \frac{M_{21} x + M_{22} y + M_{23}}{M_{31} x + M_{32} y + M_{33}} \right )\]when the flag WARP_INVERSE_MAP is set. Otherwise, the transformation is first inverted with invert and then put in the formula above instead of M. The function cannot operate in-place.See alsowarpAffine, resize, remap, getRectSubPix, perspectiveTransformExamples: samples/cpp/image_alignment.cpp, and samples/dnn/text_detection.cpp.","関数 warpPerspective は，指定された行列を用いて入力画像を変換します：˶˙º̬˙˶(x,y) = ˶ˆ꒳ˆ˵ )\Left ( ˶ˆ꒳ˆ˵ )M_{21} x + M_{22} y + M_{23}}{M_{31} x + M_{32} y + M_{33}} ̶̶̶̶̶̶。\WARP_INVERSE_MAPフラグが設定されている場合は、[right )となります。他にも，owarpAffine, resize, remap, getRectSubPix, perspectiveTransformExamples: samples/cpp/image_alignment.cpp, and samples/dnn/text_detection.cpp を参照してください．"
Applies a generic geometrical transformation to an image.,汎用的な幾何学変換を画像に適用します。
dst : Destination image. It has the same size as map1 and the same type as src .,dst : 変換先の画像．map1 と同じサイズで， src と同じタイプです．
"map1 : The first map of either (x,y) points or just x values having the type CV_16SC2 , CV_32FC1, or CV_32FC2. See convertMaps for details on converting a floating point representation to fixed-point for speed.","map1 : CV_16SC2 , CV_32FC1, CV_32FC2 型の (x,y) 点または x 値のみからなる最初のマップ．高速化のために浮動小数点表現を固定小数点に変換する方法の詳細は， convertMaps を参照してください．"
"map2 : The second map of y values having the type CV_16UC1, CV_32FC1, or none (empty map if map1 is (x,y) points), respectively.","map2 : それぞれ CV_16UC1, CV_32FC1, none (map1 が (x,y) 点の場合は空のマップ)の型を持つ，2番目のy値のマップ．"
interpolation : Interpolation method (see InterpolationFlags). The methods INTER_AREA and INTER_LINEAR_EXACT are not supported by this function.,interpolation : 補間手法（ InterpolationFlags を参照してください）．INTER_AREA および INTER_LINEAR_EXACT は，この関数ではサポートされません．
"borderMode : Pixel extrapolation method (see BorderTypes). When borderMode=BORDER_TRANSPARENT, it means that the pixels in the destination image that corresponds to the ""outliers"" in the source image are not modified by the function.","borderMode :ピクセル補外法（BorderTypes参照）．borderMode=BORDER_TRANSPARENTの場合，ソース画像の ""外れ値 ""に対応するデスティネーション画像のピクセルが，この関数によって変更されないことを意味します．"
"borderValue : Value used in case of a constant border. By default, it is 0.",borderValue : ボーダーが一定の場合に使われる値．デフォルトでは，これは0です．
"The function remap transforms the source image using the specified map:\[\texttt{dst} (x,y) = \texttt{src} (map_x(x,y),map_y(x,y))\]where values of pixels with non-integer coordinates are computed using one of available interpolation methods. \(map_x\) and \(map_y\) can be encoded as separate floating-point maps in \(map_1\) and \(map_2\) respectively, or interleaved floating-point maps of \((x,y)\) in \(map_1\), or fixed-point maps created by using convertMaps. The reason you might want to convert from floating to fixed-point representations of a map is that they can yield much faster (2x) remapping operations. In the converted case, \(map_1\) contains pairs (cvFloor(x), cvFloor(y)) and \(map_2\) contains indices in a table of interpolation coefficients.This function cannot operate in-place.NoteDue to current implementation limitations the size of an input and output images should be less than 32767x32767.",関数 remap は，指定されたマップを用いて入力画像を変換します．ここで，非整数の座標を持つピクセルの値は，利用可能な補間手法の1つを用いて計算されます．\˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ˵ )マップの浮動小数点表現から固定小数点表現に変換したい理由は、リマッピング操作をより高速（2倍）にできるからです。この関数は，インプレイス処理を行うことはできません． 注意現在の実装上の制限により，入出力画像のサイズは 32767x32767 以下でなければいけません．
Converts image transformation maps from one representation to another.,画像変換マップをある表現方法から別の表現方法に変換します。
"map1 : The first input map of type CV_16SC2, CV_32FC1, or CV_32FC2 .","map1 : CV_16SC2, CV_32FC1, または CV_32FC2 型の1番目の入力マップ．"
"map2 : The second input map of type CV_16UC1, CV_32FC1, or none (empty matrix), respectively.","map2 : CV_16UC1, CV_32FC1, none（空の行列）のいずれかのタイプの2番目の入力マップ．"
dstmap1 : The first output map that has the type dstmap1type and the same size as src .,dstmap1 : src と同じサイズで，タイプが dstmap1type である1番目の出力マップ．
dstmap2 : The second output map.,dstmap2 : 2番目の出力マップ．
"dstmap1type : Type of the first output map that should be CV_16SC2, CV_32FC1, or CV_32FC2 .","dstmap1type :CV_16SC2, CV_32FC1, CV_32FC2 のいずれかのタイプの，1つ目の出力マップ．"
nninterpolation : Flag indicating whether the fixed-point maps are used for the nearest-neighbor or for a more complex interpolation.,nninterpolation :固定小数点型マップを，最近傍補間に利用するか，より複雑な補間に利用するかを示すフラグ．
"The function converts a pair of maps for remap from one representation to another. The following options ( (map1.type(), map2.type()) \(\rightarrow\) (dstmap1.type(), dstmap2.type()) ) are supported:\(\texttt{(CV_32FC1, CV_32FC1)} \rightarrow \texttt{(CV_16SC2, CV_16UC1)}\). This is the most frequently used conversion operation, in which the original floating-point maps (see remap ) are converted to a more compact and much faster fixed-point representation. The first output array contains the rounded coordinates and the second array (created only when nninterpolation=false ) contains indices in the interpolation tables.","この関数は，リマップ用のマップのペアを，ある表現から別の表現に変換します．以下のオプション（ (map1.type(), map2.type())\(dstmap1.type(), dstmap2.type()) ) がサポートされています．\\\\\\\\\\\\\\\\これは，最も頻繁に利用される変換処理で，元の浮動小数点マップ（ リマップ を参照）を，よりコンパクトで高速な固定小数点表現に変換します．1 番目の出力配列には，丸められた座標が含まれ，2 番目の出力配列（nninterpolation=false の場合のみ作成）には，補間テーブルのインデックスが含まれます．"
"\(\texttt{(CV_32FC2)} \rightarrow \texttt{(CV_16SC2, CV_16UC1)}\). The same as above but the original maps are stored in one 2-channel matrix.",\(\texttt{(CV_32FC2)}\\\\\\\\\\\\上と同じですが，元のマップは1つの2チャンネルマトリックスに格納されます．
"Reverse conversion. Obviously, the reconstructed floating-point maps will not be exactly the same as the originals.See alsoremap, undistort, initUndistortRectifyMap","逆変換．参照：oremap, undistort, initUndistortRectifyMap"
Calculates an affine matrix of 2D rotation.,2次元回転のアフィン変換行列を計算します．
center : Center of the rotation in the source image.,center : ソース画像における回転の中心．
angle : Rotation angle in degrees. Positive values mean counter-clockwise rotation (the coordinate origin is assumed to be the top-left corner).,angle : 度単位の回転角度．正の値は反時計回りの回転を意味します（座標原点は左上隅とします）．
scale : Isotropic scale factor.,scale : 等方性のスケールファクター．
"The function calculates the following matrix:\[\begin{bmatrix} \alpha & \beta & (1- \alpha ) \cdot \texttt{center.x} - \beta \cdot \texttt{center.y} \\ - \beta & \alpha & \beta \cdot \texttt{center.x} + (1- \alpha ) \cdot \texttt{center.y} \end{bmatrix}\]where\[\begin{array}{l} \alpha = \texttt{scale} \cdot \cos \texttt{angle} , \\ \beta = \texttt{scale} \cdot \sin \texttt{angle} \end{array}\]The transformation maps the rotation center to itself. If this is not the target, adjust the shift.See alsogetAffineTransform, warpAffine, transform",この関数は，次のような行列を計算します．\(1- ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ )- \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\+ (1- ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ )\end{bmatrix}\]where\[\begin{array}{l}\ardor\abruptly♪\\\\\\\\\end{array}\]この変換は、回転中心を自分自身にマッピングします。参照：getAffineTransform、warpAffine、transform
Inverts an affine transformation.,アフィン変換を反転させます。
M : Original affine transformation.,M : 元のアフィン変換。
iM : Output reverse affine transformation.,iM : 出力される逆アフィン変換。
The function computes an inverse affine transformation represented by \(2 \times 3\) matrix M:\[\begin{bmatrix} a_{11} & a_{12} & b_1 \\ a_{21} & a_{22} & b_2 \end{bmatrix}\]The result is also a \(2 \times 3\) matrix of the same type as M.,この関数は，M で表される逆アフィン変換を計算します：\[begin{bmatrix} a_{11} & a_{12} & b_1 ˶‾᷄덴‾᷅˵]結果は，M と同じ型の ˶‾᷄덴‾᷅˵ )行列です．
Calculates a perspective transform from four pairs of the corresponding points.,4組の対応する点から，透視変換を計算します．
src : Coordinates of quadrangle vertices in the source image.,src : ソース画像上の四角形の頂点の座標を表します．
dst : Coordinates of the corresponding quadrangle vertices in the destination image.,dst : 出力画像上の，対応する四角形の頂点の座標．
solveMethod : method passed to cv::solve (DecompTypes),solveMethod : cv::solve（DecompTypes）に渡されるメソッド．
"The function calculates the \(3 \times 3\) matrix of a perspective transform so that:\[\begin{bmatrix} t_i x'_i \\ t_i y'_i \\ t_i \end{bmatrix} = \texttt{map_matrix} \cdot \begin{bmatrix} x_i \\ y_i \\ 1 \end{bmatrix}\]where\[dst(i)=(x'_i,y'_i), src(i)=(x_i, y_i), i=0,1,2,3\]See alsofindHomography, warpPerspective, perspectiveTransformExamples: samples/cpp/warpPerspective_demo.cpp, and samples/dnn/text_detection.cpp.",この関数は，透視変換の行列を計算し，次のようになります．\How do you do?cpp、samples/dnn/text_detection.cpp。
Calculates an affine transform from three pairs of the corresponding points.,3組の対応点からアフィン変換を計算します。
src : Coordinates of triangle vertices in the source image.,src : 入力画像中の三角形の頂点の座標．
dst : Coordinates of the corresponding triangle vertices in the destination image.,dst : 出力画像における，対応する三角形の頂点の座標．
"The function calculates the \(2 \times 3\) matrix of an affine transform so that:\[\begin{bmatrix} x'_i \\ y'_i \end{bmatrix} = \texttt{map_matrix} \cdot \begin{bmatrix} x_i \\ y_i \\ 1 \end{bmatrix}\]where\[dst(i)=(x'_i,y'_i), src(i)=(x_i, y_i), i=0,1,2\]See alsowarpAffine, transform",この関数は，アフィン変換の行列を計算し，次のようになります．
Retrieves a pixel rectangle from an image with sub-pixel accuracy.,画像から，サブピクセル精度でピクセル矩形を抽出します．
patchSize : Size of the extracted patch.,patchSize : 抽出されたパッチのサイズ．
center : Floating point coordinates of the center of the extracted rectangle within the source image. The center must be inside the image.,center : ソース画像内の抽出された矩形の中心を示す浮動小数点座標．中心は画像内になければなりません．
patch : Extracted patch that has the size patchSize and the same number of channels as src .,patch : src と同じサイズの patchSize と同じ数のチャンネルを持つ，抽出されたパッチ．
"patchType : Depth of the extracted pixels. By default, they have the same depth as src .",patchType :抽出されたピクセルの深さ．デフォルトでは， src と同じ深さになります．
"The function getRectSubPix extracts pixels from src:\[patch(x, y) = src(x + \texttt{center.x} - ( \texttt{dst.cols} -1)*0.5, y + \texttt{center.y} - ( \texttt{dst.rows} -1)*0.5)\]where the values of the pixels at non-integer coordinates are retrieved using bilinear interpolation. Every channel of multi-channel images is processed independently. Also the image should be a single channel or three channel image. While the center of the rectangle must be inside the image, parts of the rectangle may be outside.See alsowarpAffine, warpPerspective","関数 getRectSubPix は， src からピクセルを抽出します：\[patch(x, y) = src(x + ˶‾᷄ -̫ ‾᷅˵)- ( ˶ˆ꒳ˆ˵ )[patch(x.y) = src(x + ˶ˆ꒳ˆ˵ ) ( ˶ˆ꒳ˆ˵ ), y + ˶ˆ꒳ˆ˵ )]ここで，非整数の座標におけるピクセル値は，バイリニア補間を用いて取得されます．マルチチャンネル画像の各チャンネルは，独立して処理されます．また，画像はシングルチャンネルまたは3チャンネルの画像でなければなりません。参照：owarpAffine, warpPerspective"
Remaps an image to semilog-polar coordinates space.,画像を半極座標空間に再マッピングします。
"Deprecated:This function produces same result as cv::warpPolar(src, dst, src.size(), center, maxRadius, flags+WARP_POLAR_LOG);Examples: samples/cpp/polar_transforms.cpp.","Deprecated：この関数は， cv::warpPolar(src, dst, src.size(), center, maxRadius, flags+WARP_POLAR_LOG);と同じ結果になります．"
Remaps an image to polar coordinates space.,画像を極座標空間に再マッピングします。
"Deprecated:This function produces same result as cv::warpPolar(src, dst, src.size(), center, maxRadius, flags)Examples: samples/cpp/polar_transforms.cpp.","Deprecated：この関数は， cv::warpPolar(src, dst, src.size(), center, maxRadius, flags) と同じ結果になります．"
Remaps an image to polar or semilog-polar coordinates space.,画像を極座標または半極座標空間に再マッピングします．
dst : Destination image. It will have same type as src.,dst : 出力画像．src と同じ型になります．
dsize : The destination image size (see description for valid options).,dsize : 変換後の画像サイズ（有効なオプションについては，説明を参照してください）．
center : The transformation center.,center : 変換の中心を表します．
maxRadius : The radius of the bounding circle to transform. It determines the inverse magnitude scale parameter too.,maxRadius : 変換する外接円の半径を指定します．逆方向の大きさを表すスケールパラメータもこの値になります．
"flags : A combination of interpolation methods, InterpolationFlags + WarpPolarMode.",flags :補間方法の組み合わせ、InterpolationFlags + WarpPolarMode。
Add WARP_POLAR_LINEAR to select linear polar mapping (default),WARP_POLAR_LINEAR を追加して、リニア ポーラー マッピングを選択します（デフォルト）。
Add WARP_POLAR_LOG to select semilog polar mapping,WARP_POLAR_LOG を追加し、セミログ ポーラーマッピングを選択します。
Add WARP_INVERSE_MAP for reverse mapping.,リバースマッピングを選択する場合はWARP_INVERSE_MAPを追加します。
"Polar remaps referenceTransform the source image using the following transformation:\[ dst(\rho , \phi ) = src(x,y) \]where\[ \begin{array}{l} \vec{I} = (x - center.x, \;y - center.y) \\ \phi = Kangle \cdot \texttt{angle} (\vec{I}) \\ \rho = \left\{\begin{matrix} Klin \cdot \texttt{magnitude} (\vec{I}) & default \\ Klog \cdot log_e(\texttt{magnitude} (\vec{I})) & if \; semilog \\ \end{matrix}\right. \end{array} \]and\[ \begin{array}{l} Kangle = dsize.height / 2\Pi \\ Klin = dsize.width / maxRadius \\ Klog = dsize.width / log_e(maxRadius) \\ \end{array} \]Linear vs semilog mappingPolar mapping can be linear or semi-log. Add one of WarpPolarMode to flags to specify the polar mapping mode.Linear is the default mode.The semilog mapping emulates the human ""foveal"" vision that permit very high acuity on the line of sight (central vision) in contrast to peripheral vision where acuity is minor.Option on dsize:if both values in dsize <=0 (default), the destination image will have (almost) same area of source bounding circle: ","Polar remaps reference次の変換を用いて、ソース画像を変換します。\(x - center.x, y - center.y) (phi)(♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪Klin (McDot)(with) & default ¶ Klog ¶ log_e(˶‾᷄ -̫ ‾᷅˵)(semilog ˶‾᷅᷅˵‾᷅˵\♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪\♪♪♪♪♪♪♪～Kangle = dsize.height / 2Pi ☆Klin = dsize.width / maxRadius ☆Klog = dsize.width / log_e(maxRadius) ☆end{array} ☆end{array} ☆end{array} ☆end{array}!\Linear vs semilog mappingPolar mappingにはlinearとsemi-logがあります。線形はデフォルトで、セミログマッピングは、人間の「小窩座」と呼ばれる視力を模したもので、周辺部の視力が低いのに対して、視線上の視力が非常に高いという特徴があります。"
\[\begin{array}{l} dsize.area \leftarrow (maxRadius^2 \cdot \Pi) \\ dsize.width = \texttt{cvRound}(maxRadius) \\ dsize.height = \texttt{cvRound}(maxRadius \cdot \Pi) \\ \end{array}\],\dsize.width = ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ )
"if only dsize.height <= 0, the destination image area will be proportional to the bounding circle area but scaled by Kx * Kx: ",dsize.height <= 0のみの場合、出力画像の領域は bounding circleの領域に比例しますが、Kx * Kxでスケーリングされます。
\[\begin{array}{l} dsize.height = \texttt{cvRound}(dsize.width \cdot \Pi) \\ \end{array} \],\dsize.height = ˶ˆ꒳ˆ˵ (dmsize.width ˶ˆ꒳ˆ˵) ˶ˆ꒳ˆ˵ (frank)\]
"if both values in dsize > 0, the destination image will have the given size therefore the area of the bounding circle will be scaled to dsize.Reverse mappingYou can get reverse mapping adding WARP_INVERSE_MAP to flags// direct transform        warpPolar(src, lin_polar_img, Size(),center, maxRadius, flags);                     // linear Polar        warpPolar(src, log_polar_img, Size(),center, maxRadius, flags + WARP_POLAR_LOG);    // semilog Polar        // inverse transform        warpPolar(lin_polar_img, recovered_lin_polar_img, src.size(), center, maxRadius, flags + WARP_INVERSE_MAP);        warpPolar(log_polar_img, recovered_log_polar, src.size(), center, maxRadius, flags + WARP_POLAR_LOG + WARP_INVERSE_MAP);fragmentIn addiction, to calculate the original coordinate from a polar mapped coordinate \((rho, phi)->(x, y)\):double angleRad, magnitude;        double Kangle = dst.rows / CV_2PI;        angleRad = phi / Kangle;        if (flags & WARP_POLAR_LOG)        {            double Klog = dst.cols / std::log(maxRadius);            magnitude = std::exp(rho / Klog);        }        else        {            double Klin = dst.cols / maxRadius;            magnitude = rho / Klin;        }        int x = cvRound(center.x + magnitude * cos(angleRad));        int y = cvRound(center.y + magnitude * sin(angleRad));fragmentNote","dsize＞0の場合、出力画像は指定されたサイズになりますので、外接円の面積はdsizeに合わせて拡大されます。リバースマッピング flagsにWARP_INVERSE_MAPを追加することで、リバースマッピングを行うことができます//直接変換 warpPolar(src, lin_polar_img, Size(),center, maxRadius, flags);                     // linear Polar warpPolar(src, log_polar_img, Size(),center, maxRadius, flags + WARP_POLAR_LOG); // semilog Polar // 逆変換 warpPolar(lin_polar_img, recovered_lin_polar_img, src.size(), center, maxRadius, flags + WARP_INVERSE_MAP); warpPolar(log_polar_img, recovered_log_polar, src.size(), center, maxRadius, flags + WARP_POLAR_LOG + WARP_INVERSE_MAP);fragment追加で，極マップされた座標から元の座標を計算する方法 ˶((rho, phi)->(x, y)˶):double angleRad, magnitude; double Kangle = dst.rows / CV_2PI; angleRad = phi / Kangle; if (flags & WARP_POLAR_LOG) { double Klog = dst.cols / std::log(maxRadius); magnitude = std::exp(rho / Klog); } else { double Klin = dst.cols / maxRadius; magnitude = rho / Klin; } int x = cvRound(center.x + magnitude * cos(angleRad)); int y = cvRound(center.y + magnitude * sin(angleRad));fragmentNote"
The function can not operate in-place.,この関数は，置換して動作させることはできません．
To calculate magnitude and angle in degrees cartToPolar is used internally thus angles are measured from 0 to 360 with accuracy about 0.3 degrees.,大きさと角度を度単位で計算するために，内部では cartToPolar が利用され，角度は 0 から 360 まで，約 0.3 度の精度で測定されます．
This function uses remap. Due to current implementation limitations the size of an input and output images should be less than 32767x32767.See alsocv::remapExamples: samples/cpp/polar_transforms.cpp.,この関数はリマップを使用します。alsocv::remapExamples: samples/cpp/polar_transforms.cpp を参照してください。
Calculates the integral of an image.,画像の積分を計算します．
"src : input image as \(W \times H\), 8-bit or floating-point (32f or 64f).",src : 入力画像を，8ビットまたは浮動小数点（32fまたは64f）で， ˶˙º̬˙˶に変換します．
"sum : integral image as \((W+1)\times (H+1)\) , 32-bit integer or floating-point (32f or 64f).",sum : 積分画像 image as ˶((W+1)˶)×(H+1)˶)32 ビットの整数または浮動小数点（32f または 64f）です．
"sqsum : integral image for squared pixel values; it is \((W+1)\times (H+1)\), double-precision floating-point (64f) array.",sqsum : 画素値の2乗を表すインテグラルイメージで，\((W+1)toimes (H+1)\)，倍精度浮動小数点(64f)の配列です．
tilted : integral for the image rotated by 45 degrees; it is \((W+1)\times (H+1)\) array with the same data type as sum.,tilted : 45度回転した画像の積分で，sumと同じデータ型の配列である．
"sdepth : desired depth of the integral and the tilted integral images, CV_32S, CV_32F, or CV_64F.","sdepth : 積分画像と傾斜積分画像の深度を指定します（CV_32S, CV_32F, CV_64F）．"
"sqdepth : desired depth of the integral image of squared pixel values, CV_32F or CV_64F.",sqdepth : 画素値を2乗した積分画像の深度を指定，CV_32F または CV_64F．
"The function calculates one or more integral images for the source image as follows:\[\texttt{sum} (X,Y) = \sum _{x<X,y<Y} \texttt{image} (x,y)\]\[\texttt{sqsum} (X,Y) = \sum _{x<X,y<Y} \texttt{image} (x,y)^2\]\[\texttt{tilted} (X,Y) = \sum _{y<Y,abs(x-X+1) \leq Y-y-1} \texttt{image} (x,y)\]Using these integral images, you can calculate sum, mean, and standard deviation over a specific up-right or rotated rectangular region of the image in a constant time, for example:\[\sum _{x_1 \leq x < x_2, \, y_1 \leq y < y_2} \texttt{image} (x,y) = \texttt{sum} (x_2,y_2)- \texttt{sum} (x_1,y_2)- \texttt{sum} (x_2,y_1)+ \texttt{sum} (x_1,y_1)\]It makes possible to do a fast blurring or fast block correlation with a variable window size, for example. In case of multi-channel images, sums for each channel are accumulated independently.As a practical example, the next figure shows the calculation of the integral of a straight rectangle Rect(3,3,3,2) and of a tilted rectangle Rect(5,1,2,3) . The selected pixels in the original image are shown, as well as the relative pixels in the integral images sum and tilted .integral calculation example","この関数は，入力画像に対する1つまたは複数の積分画像を次のように計算します：˶‾᷄ -̫ ‾᷅˵ ˶‾᷅˵ ˶‾᷅˵ ˶‾᷅˵ ˶ˆ꒳ˆ˵ ˶ˆ꒳ˆ˵ ˶ˆ꒳ˆ˵ ˶ˆ꒳ˆ˵ ˶ˆ꒳ˆ˵ ˶ˆ꒳ˆ˵ ˶ˆ꒳ˆ˵ ˶ˆ꒳ˆ˵ ˶ˆ꒳ˆ˵ ˶ˆ꒳ˆ˵ ˶ˆ꒳ˆ˵ ˶ˆ꒳ˆ˵ ˶ˆ꒳ˆ˵ ˶ˆ꒳ˆ˵ ˶ˆ꒳ˆ˵ ˶ˆ꒳ˆ˵ ˶ˆ꒳ˆ˵ ˶ˆ꒳ˆ˵ ˶ˆ꒳ˆ˵ ˶ˆ꒳ˆ˵ ˶ˆ꒳ˆ˵ ˶ˆ꒳ˆ˵ ˶ˆ꒳ˆ˵ ˶ˆ꒳ˆ˵ ˶ˆ꒳ˆ˵(X,Y) = ＼sum _{y<Y,abs(x-X+1) ＼Y-y-1}／ ＼texttt{image}／(x,y)⏺️]これらの積分画像を使って、例えば、画像の上下左右に回転した特定の矩形領域の総和、平均、標準偏差を一定時間で計算することができます。\イメージ(x,y) = ˶ˆ꒳ˆ˵ )(x_2,y_2)-\\(x_1,y_2)-\\(x_2,y_1)+ ⏺️⏺️⏺️⏺️⏺️⏺️。(x_1,y_1)】これにより，例えば，可変の窓サイズで高速ブラーリングや高速ブロック相関を行うことができる．マルチチャンネル画像の場合，各チャンネルの総和は独立に累積されます．次の図は，実際の例として，まっすぐな矩形Rect(3,3,3,2)と傾いた矩形Rect(5,1,2,3)の積分の計算を示しています．元画像の選択されたピクセルと，積分画像の和と傾斜の相対的なピクセルが示されています．積分計算の例"
Adds an image to the accumulator image.,画像をアキュムレータ画像に追加します．
"src : Input image of type CV_8UC(n), CV_16UC(n), CV_32FC(n) or CV_64FC(n), where n is a positive integer.","src : CV_8UC(n), CV_16UC(n), CV_32FC(n), CV_64FC(n)型の入力画像（nは正の整数）．"
"dst : Accumulator image with the same number of channels as input image, and a depth of CV_32F or CV_64F.",dst : 入力画像と同じチャンネル数で，CV_32F または CV_64F の深度を持つ累算器画像．
mask : Optional operation mask.,mask : オプションである処理マスク．
"The function adds src or some of its elements to dst :\[\texttt{dst} (x,y) \leftarrow \texttt{dst} (x,y) + \texttt{src} (x,y) \quad \text{if} \quad \texttt{mask} (x,y) \ne 0\]The function supports multi-channel images. Each channel is processed independently.The function cv::accumulate can be used, for example, to collect statistics of a scene background viewed by a still camera and for the further foreground-background segmentation.See alsoaccumulateSquare, accumulateProduct, accumulateWeighted","この関数は， src またはその一部の要素を dst に追加します．(x,y) ˶‾᷅˵‾᷅˵(x,y) + ˶‾᷄ -̫ ‾᷅˵˵(x,y)\♪♪「マスク(x,y) ˶‾᷅˵‾᷅˵ マルチチャンネル画像に対応しています。関数 cv::acumulate は，例えば，スチルカメラで撮影されたシーンの背景の統計量を収集し，さらに前景と背景のセグメンテーションを行うために利用できます．"
Adds the square of a source image to the accumulator image.,入力画像の正方形をアキュムレータ画像に追加します．
"src : Input image as 1- or 3-channel, 8-bit or 32-bit floating point.",src : 1チャンネルまたは3チャンネル，8ビットまたは32ビット浮動小数点の入力画像．
"dst : Accumulator image with the same number of channels as input image, 32-bit or 64-bit floating-point.",dst : 入力画像と同じチャンネル数の，32ビットまたは64ビット浮動小数点型の累算器画像．
"The function adds the input image src or its selected region, raised to a power of 2, to the accumulator dst :\[\texttt{dst} (x,y) \leftarrow \texttt{dst} (x,y) + \texttt{src} (x,y)^2 \quad \text{if} \quad \texttt{mask} (x,y) \ne 0\]The function supports multi-channel images. Each channel is processed independently.See alsoaccumulateSquare, accumulateProduct, accumulateWeighted","この関数は，入力画像 src またはその選択領域を2の累乗にして，累算器 dst に加えます．(x,y) ˶‾᷅˵‾᷅˵(x,y) + ˶ˆ꒳ˆ˵(x,y)^2\♪♪♪♪♪♪♪～(x,y) ˶‾᷄ -̫ ‾᷅˵ マルチチャンネル画像に対応しています。関連項目：accumulateSquare，acumulateProduct，acumulateWeighted"
Adds the per-element product of two input images to the accumulator image.,2つの入力画像の要素毎の積を，累算器画像に加算します．
"src1 : First input image, 1- or 3-channel, 8-bit or 32-bit floating point.",src1 : 1番目の入力画像．1チャンネルまたは3チャンネル，8ビットまたは32ビット浮動小数点．
src2 : Second input image of the same type and the same size as src1 .,src2 : src1 と同じ種類，同じサイズの 2 番目の入力画像．
"dst : Accumulator image with the same number of channels as input images, 32-bit or 64-bit floating-point.",dst : 入力画像と同じチャンネル数の累算器画像，32ビットまたは64ビット浮動小数点．
"The function adds the product of two images or their selected regions to the accumulator dst :\[\texttt{dst} (x,y) \leftarrow \texttt{dst} (x,y) + \texttt{src1} (x,y) \cdot \texttt{src2} (x,y) \quad \text{if} \quad \texttt{mask} (x,y) \ne 0\]The function supports multi-channel images. Each channel is processed independently.See alsoaccumulate, accumulateSquare, accumulateWeighted","この関数は，2つの画像またはその選択領域の積を，累算器 dst に加えます．(x,y) (x,y) + ˶ˆ꒳ˆ˵ )(x,y) + ˶ˆ꒳ˆ˵(x,y) ♪\\\\(x,y) ♪\\\\mask(x,y) ˶‾᷄ -̫ ‾᷅˵ マルチチャンネル画像に対応しています。関連項目：accumulate，acumulateSquare，acumulateWeighted"
Updates a running average.,移動平均を更新します．
alpha : Weight of the input image.,alpha : 入力画像の重み．
"The function calculates the weighted sum of the input image src and the accumulator dst so that dst becomes a running average of a frame sequence:\[\texttt{dst} (x,y) \leftarrow (1- \texttt{alpha} ) \cdot \texttt{dst} (x,y) + \texttt{alpha} \cdot \texttt{src} (x,y) \quad \text{if} \quad \texttt{mask} (x,y) \ne 0\]That is, alpha regulates the update speed (how fast the accumulator ""forgets"" about earlier images). The function supports multi-channel images. Each channel is processed independently.See alsoaccumulate, accumulateSquare, accumulateProduct","この関数は，入力画像 src と累算器 dst の加重和を計算し，dst がフレームシーケンスの走行平均になるようにします．(x,y) ¶leftarrow (1- ¶texttt{alpha} ) ¶cdot ¶texttt{dst} (x,y) + ¶texttt{dst} ¶leftarrow (1- ¶texttt{alpha} )(x,y) + ˶‾᷄д‾᷅˵ ˶‾᷅᷅˵ ˶‾᷄д‾᷅˵(x,y) ♪♪♪＼(^o^)|mask} (x,y)(x,y) ˶ˆ꒳ˆ˵ ) つまり，αは，更新速度（以前の画像をどれだけ早く忘れるか）を調節します．この関数は，マルチチャンネル画像をサポートします．関連項目：accumulate, accumulateSquare, accumulateProduct"
The function is used to detect translational shifts that occur between two images.,この関数は，2つの画像間で起こる並進方向のズレを検出するために利用されます．
src1 : Source floating point array (CV_32FC1 or CV_64FC1),src1 : 入力となる浮動小数点型配列（CV_32FC1 または CV_64FC1）．
src2 : Source floating point array (CV_32FC1 or CV_64FC1),src2 : 入力となる浮動小数点型の配列（CV_32FC1 または CV_64FC1）．
window : Floating point array with windowing coefficients to reduce edge effects (optional).,window : エッジ効果を低減するための，ウィンドウウィング係数を持つ浮動小数点型配列（オプション）．
"response : Signal power within the 5x5 centroid around the peak, between 0 and 1 (optional).",response : ピーク付近の5x5セントロイド内の信号強度，0から1の間で指定します（オプション）．
The operation takes advantage of the Fourier shift theorem for detecting the translational shift in the frequency domain. It can be used for fast image registration as well as motion estimation. For more information please see http://en.wikipedia.org/wiki/Phase_correlationCalculates the cross-power spectrum of two supplied source arrays. The arrays are padded if needed with getOptimalDFTSize.The function performs the following equations:First it applies a Hanning window (see http://en.wikipedia.org/wiki/Hann_function) to each image to remove possible edge effects. This window is cached until the array size changes to speed up processing time.,この操作は，周波数領域で並進シフトを検出するためのフーリエシフト定理を利用しています．これは、動きの推定だけでなく、高速な画像登録にも使用できます。詳しくは http://en.wikipedia.org/wiki/Phase_correlationCalculates the cross-power spectrum of two supplied source arrays をご覧ください。この関数は，次のような処理を行います：まず，各画像に対してハニング窓（ http://en.wikipedia.org/wiki/Hann_function を参照してください）を適用し，可能性のあるエッジ効果を取り除きます．この窓は，処理時間を短縮するために，配列のサイズが変わるまでキャッシュされます．
Next it computes the forward DFTs of each source array: ,次に，各ソース配列のForward DFTを計算します．
"\[\mathbf{G}_a = \mathcal{F}\{src_1\}, \; \mathbf{G}_b = \mathcal{F}\{src_2\}\]",\次のように計算します。
 where \(\mathcal{F}\) is the forward DFT., ここで，\(mathcal{F}\)はForward DFTである．
It then computes the cross-power spectrum of each frequency domain array: ,次に、各周波数領域のアレイのクロスパワースペクトルを計算します。
\[R = \frac{ \mathbf{G}_a \mathbf{G}_b^*}{|\mathbf{G}_a \mathbf{G}_b^*|}\],\R = ˶ˆ꒳ˆ˵ )
Next the cross-correlation is converted back into the time domain via the inverse DFT: ,次に、この相互相関を逆DFTにより時間領域に変換します。
\[r = \mathcal{F}^{-1}\{R\}\],\r = ◟꒰◍´Д｀◍꒱◞
"Finally, it computes the peak location and computes a 5x5 weighted centroid around the peak to achieve sub-pixel accuracy. ",最後に、ピークの位置を計算し、サブピクセルの精度を実現するために、ピークの周りに5x5の加重セントロイドを計算します。
"\[(\Delta x, \Delta y) = \texttt{weightedCentroid} \{\arg \max_{(x, y)}\{r\}\}\]","\♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪♪\max{(x, y)}\{r\}\]となります。"
"If non-zero, the response parameter is computed as the sum of the elements of r within the 5x5 centroid around the peak location. It is normalized to a maximum of 1 (meaning there is a single peak) and will be smaller when there are multiple peaks.See alsodft, getOptimalDFTSize, idft, mulSpectrums createHanningWindow","0でない場合，応答パラメータは，ピーク位置の周りの5x5セントロイド内のrの要素の合計として計算されます．これは，最大値が1（ピークが1つの場合）になるように正規化され，複数のピークがある場合は小さくなります． 参照：odft, getOptimalDFTSize, idft, mulSpectrums createHanningWindow"
This function computes a Hanning window coefficients in two dimensions.,この関数は，2次元のハニング窓の係数を求めます．
dst : Destination array to place Hann coefficients in,dst : ハニング窓の係数を格納する出力配列．
winSize : The window size specifications (both width and height must be > 1),winSize : ウィンドウサイズの指定（幅と高さの両方が1以上である必要があります）．
type : Created array type,type :作成された配列タイプ
"See (http://en.wikipedia.org/wiki/Hann_function) and (http://en.wikipedia.org/wiki/Window_function) for more information.An example is shown below:// create hanning window of size 100x100 and type CV_32FMat hann;createHanningWindow(hann, Size(100, 100), CV_32F);fragment","詳細は (http://en.wikipedia.org/wiki/Hann_function) と (http://en.wikipedia.org/wiki/Window_function) を参照してください．例を以下に示します： // サイズが 100x100 でタイプが CV_32FMat のハニングウィンドウを作成 hann;createHanningWindow(hann, Size(100, 100), CV_32F);fragment"
Applies a fixed-level threshold to each array element.,各配列要素に固定レベルの閾値を適用します．
"src : input array (multiple-channel, 8-bit or 32-bit floating point).",src : 入力配列（マルチチャンネル，8ビットまたは32ビット浮動小数点）．
dst : output array of the same size and type and the same number of channels as src.,dst : src と同じサイズ，同じ種類，同じチャンネル数の出力配列．
thresh : threshold value.,thresh : 閾値．
maxval : maximum value to use with the THRESH_BINARY and THRESH_BINARY_INV thresholding types.,maxval : THRESH_BINARY および THRESH_BINARY_INV 閾値設定タイプで利用する最大値．
type : thresholding type (see ThresholdTypes).,type : スレッショルドタイプ（ThresholdTypesを参照）．
"The function applies fixed-level thresholding to a multiple-channel array. The function is typically used to get a bi-level (binary) image out of a grayscale image ( compare could be also used for this purpose) or for removing a noise, that is, filtering out pixels with too small or too large values. There are several types of thresholding supported by the function. They are determined by type parameter.Also, the special values THRESH_OTSU or THRESH_TRIANGLE may be combined with one of the above values. In these cases, the function determines the optimal threshold value using the Otsu's or Triangle algorithm and uses it instead of the specified thresh.NoteCurrently, the Otsu's and Triangle methods are implemented only for 8-bit single-channel images.See alsoadaptiveThreshold, findContours, compare, min, maxExamples: samples/cpp/ffilldemo.cpp, samples/cpp/tutorial_code/ml/introduction_to_pca/introduction_to_pca.cpp, and samples/tapi/squares.cpp.","この関数は，固定レベルの閾値をマルチチャンネル配列に適用します．この関数は，グレースケール画像から2値（バイナリ）画像を得るため，あるいはノイズを除去するため（つまり，値が小さすぎたり大きすぎたりするピクセルを除外するため）に利用されます．この関数がサポートする閾値処理には，いくつかの種類があります．また，特別な値である THRESH_OTSU や THRESH_TRIANGLE を，上記の値の1つと組み合わせることもできます．また，特別な値 THRESH_OTSU や THRESH_TRIANGLE と上記の値を組み合わせることもできます．これらの場合，この関数は，Otsu's または Triangle アルゴリズムを用いて最適な閾値を決定し，それを指定された閾値の代わりに利用します．注意現在，Otsu's および Triangle 法は，8ビットシングルチャンネル画像に対してのみ実装されています．参照：adaptiveThreshold, findContours, compare, min, maxExamples: samples/cpp/ffilldemo.cpp, samples/cpp/tutorial_code/ml/introduction_to_pca/introduction_to_pca.cpp, and samples/tapi/squares.cpp."
Applies an adaptive threshold to an array.,アダプティブスレッショルドを配列に適用します。
src : Source 8-bit single-channel image.,src : 8ビットシングルチャンネルの入力画像．
dst : Destination image of the same size and the same type as src.,dst : src と同じサイズ，同じタイプの出力画像．
maxValue : Non-zero value assigned to the pixels for which the condition is satisfied,maxValue : 条件が満たされたピクセルに割り当てられる0以外の値
"adaptiveMethod : Adaptive thresholding algorithm to use, see AdaptiveThresholdTypes. The BORDER_REPLICATE | BORDER_ISOLATED is used to process boundaries.",adaptiveMethod : 使用する適応型閾値アルゴリズム，AdaptiveThresholdTypes 参照．境界の処理には、BORDER_REPLICATE | BORDER_ISOLATED が用いられる。
"thresholdType : Thresholding type that must be either THRESH_BINARY or THRESH_BINARY_INV, see ThresholdTypes.",thresholdType :THRESH_BINARYまたはTHRESH_BINARY_INVのいずれかでなければならない閾値タイプ、ThresholdTypes参照。
"blockSize : Size of a pixel neighborhood that is used to calculate a threshold value for the pixel: 3, 5, 7, and so on.",blockSize : ピクセルのしきい値を計算するために使用されるピクセル近傍のサイズ。3、5、7...といった具合です。
"C : Constant subtracted from the mean or weighted mean (see the details below). Normally, it is positive but may be zero or negative as well.",C : 平均値または加重平均値から減算される定数（詳細は後述）。通常は正の値ですが，ゼロや負の値もあり得ます．
The function transforms a grayscale image to a binary image according to the formulae:THRESH_BINARY ,この関数は，次の式に従って，グレースケール画像を2値画像に変換します：THRESH_BINARY
"\[dst(x,y) = \fork{\texttt{maxValue}}{if \(src(x,y) > T(x,y)\)}{0}{otherwise}\]","\dst(x,y) = ˶‾᷄ -̫ ‾᷅˵}}{if ˶‾᷅ src(x,y) > T(x,y)₎₎}{0}{otherwise}]."
THRESH_BINARY_INV ,thresh_binary_inv
"\[dst(x,y) = \fork{0}{if \(src(x,y) > T(x,y)\)}{\texttt{maxValue}}{otherwise}\]","\dst(x,y) = ˶‾᷄ -̫ ‾᷅˵{0}{if ˶‾᷄ src(x,y) > T(x,y)୨୧}{0}{otherwise}\ [dst(x,y) = ˶‾᷄ -̫ ‾᷅˵{0{maxValue}}{0}{otherwise}\] 。"
" where \(T(x,y)\) is a threshold calculated individually for each pixel (see adaptiveMethod parameter).The function can process the image in-place.See alsothreshold, blur, GaussianBlur", この関数は，画像をその場で処理することができます． 関連項目： しきい値，ブラー，ガウスブラー
Blurs an image and downsamples it.,画像をぼかし，ダウンサンプリングします．
dst : output image; it has the specified size and the same type as src.,dst : 出力画像．指定されたサイズで， src と同じ種類の画像です．
dstsize : size of the output image.,dstsize : 出力画像のサイズ．
"borderType : Pixel extrapolation method, see BorderTypes (BORDER_CONSTANT isn't supported)","borderType :Pixel extrapolation method, See BorderTypes (BORDER_CONSTANT is not supported)"
"By default, size of the output image is computed as Size((src.cols+1)/2, (src.rows+1)/2), but in any case, the following conditions should be satisfied:\[\begin{array}{l} | \texttt{dstsize.width} *2-src.cols| \leq 2 \\ | \texttt{dstsize.height} *2-src.rows| \leq 2 \end{array}\]The function performs the downsampling step of the Gaussian pyramid construction. First, it convolves the source image with the kernel:\[\frac{1}{256} \begin{bmatrix} 1 & 4 & 6 & 4 & 1 \\ 4 & 16 & 24 & 16 & 4 \\ 6 & 24 & 36 & 24 & 6 \\ 4 & 16 & 24 & 16 & 4 \\ 1 & 4 & 6 & 4 & 1 \end{bmatrix}\]Then, it downsamples the image by rejecting even rows and columns.Examples: samples/cpp/squares.cpp, samples/cpp/tutorial_code/ImgProc/Pyramids/Pyramids.cpp, and samples/tapi/squares.cpp.","デフォルトでは，出力画像のサイズは Size((src.cols+1)/2, (src.rows+1)/2) として計算されますが，どのような場合でも，以下の条件を満たす必要があります：。| texttt{dstsize.width}.*2-src.cols| ˶‾᷄ -̫ ‾᷅˵ | ˶‾᷄ -̫ ‾᷅˵ | ˶‾᷄ -̫ ‾᷅˵*2-src.rows| ˶ˆ꒳ˆ˵ )]この関数は，ガウス型ピラミッドを構築する際のダウンサンプリング処理を行います．まず，入力画像をカーネルで畳み込みます：\[frac{1}{256}.\beegin{bmatrix} (1 & 4 & 6 & 4 & 4)1 & 4 & 6 & 4 & 1 ≒ 6 & 24 & 36 & 24 & 6 ≒ 4 & 16 & 24 & 16 & 4 ≒ 1 & 4 & 6 & 4 & 1 ≒ end{bmatrix}]その後、偶数行、偶数列を排除してダウンサンプリングを行います。例：samples/cpp/squares.cpp、samples/cpp/tutorial_code/ImgProc/Pyramids/Pyramids.cpp、samples/tapi/squares.cpp。"
Upsamples an image and then blurs it.,画像をアップサンプリングし、それをぼかします。
dst : output image. It has the specified size and the same type as src .,dst : 出力画像．src と同じタイプで，指定されたサイズの画像です．
"borderType : Pixel extrapolation method, see BorderTypes (only BORDER_DEFAULT is supported)","borderType :Pixel extrapolation method, See BorderTypes (BORDER_DEFAULT のみがサポートされます)"
"By default, size of the output image is computed as Size(src.cols\*2, (src.rows\*2), but in any case, the following conditions should be satisfied:\[\begin{array}{l} | \texttt{dstsize.width} -src.cols*2| \leq ( \texttt{dstsize.width} \mod 2) \\ | \texttt{dstsize.height} -src.rows*2| \leq ( \texttt{dstsize.height} \mod 2) \end{array}\]The function performs the upsampling step of the Gaussian pyramid construction, though it can actually be used to construct the Laplacian pyramid. First, it upsamples the source image by injecting even zero rows and columns and then convolves the result with the same kernel as in pyrDown multiplied by 4.Examples: samples/cpp/squares.cpp, samples/cpp/tutorial_code/ImgProc/Pyramids/Pyramids.cpp, and samples/tapi/squares.cpp.","デフォルトでは，出力画像のサイズは Size(src.cols\*2, (src.rows\*2)) として計算されますが，どのような場合でも，以下の条件を満たす必要があります：˶‾᷄ -̫ ‾᷅˵ ˶‾᷅˵ ₎₎₎₎₎₎₎₎₎₎₎₎。| ˶ˆ꒳ˆ˵ ( ˶ˆ꒳ˆ˵ ) | ˶ˆ꒳ˆ˵ ( ˶ˆ꒳ˆ˵ )height} ¶mod 2) end{array}\]この関数は，ガウス型ピラミッドを構成する際のアップサンプリングを行うものですが，実際にはラプラシアン型ピラミッドを構成するのにも使えます．例： samples/cpp/squares.cpp, samples/cpp/tutorial_code/ImgProc/Pyramids/Pyramids.cpp, and samples/tapi/squares.cpp."
Calculates a histogram of a set of arrays.,配列の集合のヒストグラムを計算します．
"images : Source arrays. They all should have the same depth, CV_8U, CV_16U or CV_32F , and the same size. Each of them can have an arbitrary number of channels.","images :入力配列．これらはすべて同じビット深度，CV_8U, CV_16U または CV_32F を持ち，同じサイズでなければいけません．それぞれの配列は，任意のチャンネル数を持つことができます．"
nimages : Number of source images.,nimages :ソース画像の数．
"channels : List of the dims channels used to compute the histogram. The first array channels are numerated from 0 to images[0].channels()-1 , the second array channels are counted from images[0].channels() to images[0].channels() + images[1].channels()-1, and so on.",channels : ヒストグラムの計算に利用される深さチャンネルのリスト．1番目の配列のチャンネルは，0 から images[0].channels()-1 までの数字，2番目の配列のチャンネルは，images[0].channels() から images[0].channels() + images[1].channels()-1 までの数字，というように数えられます．
"mask : Optional mask. If the matrix is not empty, it must be an 8-bit array of the same size as images[i] . The non-zero mask elements mark the array elements counted in the histogram.",mask : オプションのマスク．行列が空ではない場合は， images[i] と同じサイズの8ビット配列でなければいけません．非0のマスク要素は，ヒストグラムでカウントされる配列要素を示します．
"hist : Output histogram, which is a dense or sparse dims -dimensional array.",hist :密な，あるいは疎な dims -次元の配列である出力ヒストグラム．
dims : Histogram dimensionality that must be positive and not greater than CV_MAX_DIMS (equal to 32 in the current OpenCV version).,dims : 正の値で，CV_MAX_DIMS（現在のOpenCVでは32に等しい）よりも大きくてはいけないヒストグラムの次元数．
histSize : Array of histogram sizes in each dimension.,histSize : 各次元におけるヒストグラムのサイズを表す配列．
"ranges : Array of the dims arrays of the histogram bin boundaries in each dimension. When the histogram is uniform ( uniform =true), then for each dimension i it is enough to specify the lower (inclusive) boundary \(L_0\) of the 0-th histogram bin and the upper (exclusive) boundary \(U_{\texttt{histSize}[i]-1}\) for the last histogram bin histSize[i]-1 . That is, in case of a uniform histogram each of ranges[i] is an array of 2 elements. When the histogram is not uniform ( uniform=false ), then each of ranges[i] contains histSize[i]+1 elements: \(L_0, U_0=L_1, U_1=L_2, ..., U_{\texttt{histSize[i]}-2}=L_{\texttt{histSize[i]}-1}, U_{\texttt{histSize[i]}-1}\) . The array elements, that are not between \(L_0\) and \(U_{\texttt{histSize[i]}-1}\) , are not counted in the histogram.","ranges :各次元におけるヒストグラムのビンの境界を表す dims 配列の配列．ヒストグラムが一様（ uniform =true ）である場合，各次元 i において，0 番目のヒストグラムビンの下側（包含）の境界˶（L_0\）と，最後のヒストグラムビン histSize[i]-1 の上側（包含）の境界˶（U_{texttt{histSize}[i]-1}˶）を指定すれば十分です．つまり，一様なヒストグラムの場合， ranges[i]はそれぞれ2個の要素からなる配列です．また，ヒストグラムが一様でない場合（uniform=false）， ranges[i]の各要素は， histSize[i]+1 の要素を含みます．\L_0, U_0=L_1, U_1=L_2, ..., U_{\tt{histSize[i]}-2}=L_{\tt{histSize[i]}-1}, U_{\tt{histSize[i]}-1}\) .配列の要素のうち，\(L_0\)と\(U_{\{histSize[i]}-1})の間にないものは，ヒストグラムにカウントされません。"
uniform : Flag indicating whether the histogram is uniform or not (see above).,uniform :ヒストグラムが一様であるかどうかを示すフラグです（上述）．
"accumulate : Accumulation flag. If it is set, the histogram is not cleared in the beginning when it is allocated. This feature enables you to compute a single histogram from several sets of arrays, or to update the histogram in time.",accumulate :累積フラグ．これがセットされていると，ヒストグラムが割り当てられたときに，最初からクリアされません．この機能により，複数の配列セットから1つのヒストグラムを計算したり，ヒストグラムを時間的に更新したりすることができます．
"The function cv::calcHist calculates the histogram of one or more arrays. The elements of a tuple used to increment a histogram bin are taken from the corresponding input arrays at the same location. The sample below shows how to compute a 2D Hue-Saturation histogram for a color image. :#include <opencv2/imgproc.hpp>#include <opencv2/highgui.hpp>using namespace cv;int main( int argc, char** argv ){    Mat src, hsv;    if( argc != 2 || !(src=imread(argv[1], 1)).data )        return -1;    cvtColor(src, hsv, COLOR_BGR2HSV);    // Quantize the hue to 30 levels    // and the saturation to 32 levels    int hbins = 30, sbins = 32;    int histSize[] = {hbins, sbins};    // hue varies from 0 to 179, see cvtColor    float hranges[] = { 0, 180 };    // saturation varies from 0 (black-gray-white) to    // 255 (pure spectrum color)    float sranges[] = { 0, 256 };    const float* ranges[] = { hranges, sranges };    MatND hist;    // we compute the histogram from the 0-th and 1-st channels    int channels[] = {0, 1};    calcHist( &hsv, 1, channels, Mat(), // do not use mask             hist, 2, histSize, ranges,             true, // the histogram is uniform             false );    double maxVal=0;    minMaxLoc(hist, 0, &maxVal, 0, 0);    int scale = 10;    Mat histImg = Mat::zeros(sbins*scale, hbins*10, CV_8UC3);    for( int h = 0; h < hbins; h++ )        for( int s = 0; s < sbins; s++ )        {            float binVal = hist.at<float>(h, s);            int intensity = cvRound(binVal*255/maxVal);            rectangle( histImg, Point(h*scale, s*scale),                        Point( (h+1)*scale - 1, (s+1)*scale - 1),                        Scalar::all(intensity),                        -1 );        }    namedWindow( ""Source"", 1 );    imshow( ""Source"", src );    namedWindow( ""H-S Histogram"", 1 );    imshow( ""H-S Histogram"", histImg );    waitKey();}fragmentExamples: samples/cpp/camshiftdemo.cpp, and samples/cpp/demhist.cpp.","関数 cv::calcHist は，1 つまたは複数の配列のヒストグラムを計算します．ヒストグラムのビンを増加させるために用いられるタプルの要素は，対応する入力配列の同じ位置から取得されます．以下のサンプルは，カラー画像の2次元色相-彩度ヒストグラムを計算する方法を示しています． :#include <opencv2/imgproc.hpp>#include <opencv2/highgui.hpp>using namespace cv;int main( int argc, char** argv ){ Mat src, hsv; if( argc != 2 || !(src=imread(argv[1], 1)).data ) return -1; cvtColor(src, hsv, COLOR_BGR2HSV); // 色相を 30 レベルに，彩度を 32 レベルに量子化します int hbins = 30, sbins = 32; int histSize[] = {hbins, sbins};    // cvtColor を参照してください） float hranges[] = { 0, 180 }; // 彩度は，0（黒-灰色-白）から // 255（純粋なスペクトルカラー）まで変化します．    const float* ranges[] = { hranges, sranges }; MatND hist; // 0 番目と 1 番目のチャンネルからヒストグラムを計算します int channels[] = {0, 1};    calcHist( &hsv, 1, channels, Mat(), // マスクを使用しない hist, 2, histSize, ranges, true, // ヒストグラムは一様です false ); double maxVal=0; minMaxLoc(hist, 0, &maxVal, 0, 0); int scale = 10; Mat histImg = Mat::zeros(sbins*scale, hbins*10, CV_8UC3); for( int h = 0; h < hbins; h++ ) for( int s = 0; s < sbins; s++ ) { float binVal = hist.at<float>(h, s); int intensity = cvRound(binVal*255/maxVal); rectangle( histImg, Point(h*scale, s*scale), Point( (h+1)*scale - 1, (s+1)*scale - 1), Scalar::all(intensity), -1 ); } namedWindow(""Source"", 1 ); imshow(""Source"", src ); namedWindow(""H-S Histogram"", 1 ); imshow(""H-S Histogram"", histImg ); waitKey();}fragmentExamples: samples/cpp/camshiftdemo.cppとsamples/cpp/demhist.cppです。"
Calculates the back projection of a histogram.,ヒストグラムのバックプロジェクションを計算します．
"channels : The list of channels used to compute the back projection. The number of channels must match the histogram dimensionality. The first array channels are numerated from 0 to images[0].channels()-1 , the second array channels are counted from images[0].channels() to images[0].channels() + images[1].channels()-1, and so on.",channels : バックプロジェクションの計算に使われるチャンネルのリスト．チャンネルの数は，ヒストグラムの次元数と一致しなければいけません．配列の1番目のチャンネルは，0 から images[0].channels()-1 までの数字，配列の2番目のチャンネルは， images[0].channels() から images[0].channels() + images[1].channels()-1 までの数字，というように数えられます．
hist : Input histogram that can be dense or sparse.,hist : 入力ヒストグラム．
backProject : Destination back projection array that is a single-channel array of the same size and depth as images[0] .,backProject : バックプロジェクションの出力配列．これは， images[0] と同じサイズ，同じ深さのシングルチャンネル配列です．
ranges : Array of arrays of the histogram bin boundaries in each dimension. See calcHist .,ranges :各次元におけるヒストグラムのビンの境界を表す配列の配列．calcHist を参照してください．
scale : Optional scale factor for the output back projection.,scale : オプションである，出力バックプロジェクションのスケールファクタ．
"The function cv::calcBackProject calculates the back project of the histogram. That is, similarly to calcHist , at each location (x, y) the function collects the values from the selected channels in the input images and finds the corresponding histogram bin. But instead of incrementing it, the function reads the bin value, scales it by scale , and stores in backProject(x,y) . In terms of statistics, the function computes probability of each element value in respect with the empirical probability distribution represented by the histogram. See how, for example, you can find and track a bright-colored object in a scene:Before tracking, show the object to the camera so that it covers almost the whole frame. Calculate a hue histogram. The histogram may have strong maximums, corresponding to the dominant colors in the object.","関数 cv::calcBackProject は，ヒストグラムのバックプロジェクションを計算します．つまり， calcHist と同様に，各位置 (x, y) において，入力画像の選択されたチャンネルの値を収集し，それに対応するヒストグラムのビンを求めます．しかし，この関数は，ビンの値をインクリメントするのではなく，ビンの値を読み込み，それを scale でスケーリングして backProject(x,y) に格納します．統計学の観点から言えば，この関数は，ヒストグラムで表される経験的な確率分布に関する各要素の値の確率を計算します．例えば，シーンの中の明るい色の物体を見つけて追跡する方法を見てみましょう．追跡する前に，物体がフレームのほぼ全体を覆うようにカメラに見せます．色相ヒストグラムを算出する。このヒストグラムには、物体の主要な色に対応する強い最大値が含まれています。"
"When tracking, calculate a back projection of a hue plane of each input video frame using that pre-computed histogram. Threshold the back projection to suppress weak colors. It may also make sense to suppress pixels with non-sufficient color saturation and too dark or too bright pixels.",トラッキングの際には、事前に計算したヒストグラムを用いて、入力ビデオフレームの色相平面のバックプロジェクションを計算します。弱い色を抑制するために、バックプロジェクションの閾値を設定します。また、彩度が不十分な画素や、暗すぎる画素、明るすぎる画素を抑制することも有効です。
"Find connected components in the resulting picture and choose, for example, the largest component.This is an approximate algorithm of the CamShift color object tracker.See alsocalcHist, compareHistExamples: samples/cpp/camshiftdemo.cpp.","これはCamShiftカラーオブジェクトトラッカーの近似アルゴリズムです。socalcHist, compareHistExamples: samples/cpp/camshiftdemo.cpp."
Compares two histograms.,2つのヒストグラムを比較します。
H1 : First compared histogram.,H1 : 比較された最初のヒストグラム．
H2 : Second compared histogram of the same size as H1 .,H2 : H1 と同じサイズの2番目の比較ヒストグラム．
"method : Comparison method, see HistCompMethods",method : 比較手法，HistCompMethods を参照してください．
"The function cv::compareHist compares two dense or two sparse histograms using the specified method.The function returns \(d(H_1, H_2)\) .While the function works well with 1-, 2-, 3-dimensional dense histograms, it may not be suitable for high-dimensional sparse histograms. In such histograms, because of aliasing and sampling problems, the coordinates of non-zero histogram bins can slightly shift. To compare such histograms or more general sparse configurations of weighted points, consider using the EMD function.",関数 cv::compareHist は，指定された手法を用いて2つの密なヒストグラム，あるいは2つの疎なヒストグラムを比較します．この関数は，1次元，2次元，3次元の密なヒストグラムではうまく動作しますが，高次元の疎なヒストグラムには適していないかもしれません．このようなヒストグラムでは，エイリアシングやサンプリングの問題のために，非ゼロのヒストグラムビンの座標がわずかにずれることがあります．このようなヒストグラムや，より一般的な重み付けされた点の疎な構成を比較するには，EMD関数の使用を検討します．
Equalizes the histogram of a grayscale image.,グレースケール画像のヒストグラムを均等化します．
src : Source 8-bit single channel image.,src : 入力となる8ビットシングルチャンネル画像．
The function equalizes the histogram of the input image using the following algorithm:Calculate the histogram \(H\) for src .,この関数は，以下のアルゴリズムを用いて，入力画像のヒストグラムを均等化します： src のヒストグラムを計算します．
Normalize the histogram so that the sum of histogram bins is 255.,ヒストグラムのビンの合計が 255 になるように，ヒストグラムを正規化します．
Compute the integral of the histogram: ,ヒストグラムの積分を計算します．
\[H'_i = \sum _{0 \le j < i} H(j)\],\H'_i = sum _{0 ‾ j < i} H(j)⏺️」となります。
"Transform the image using \(H'\) as a look-up table: \(\texttt{dst}(x,y) = H'(\texttt{src}(x,y))\)The algorithm normalizes the brightness and increases the contrast of the image.Examples: samples/cpp/facedetect.cpp.",H(j)をルックアップテーブルにして、画像を変換します。\例：samples/cpp/facedetect.cpp.
"Computes the ""minimal work"" distance between two weighted point configurations.",2つの重み付けされた点構成の間の「最小作業」距離を計算します。
"signature1 : First signature, a \(\texttt{size1}\times \texttt{dims}+1\) floating-point matrix. Each row stores the point weight followed by the point coordinates. The matrix is allowed to have a single column (weights only) if the user-defined cost matrix is used. The weights must be non-negative and have at least one non-zero value.",signature1 : 最初のシグネチャーは、浮動小数点型の行列です。各行には，点の重みに続いて，点の座標が格納されています．ユーザ定義のコスト行列を用いる場合，行列は1列（重みのみ）でよいことになっています．重みは，非負で，少なくとも1つの非ゼロの値を持たなければならない．
"signature2 : Second signature of the same format as signature1 , though the number of rows may be different. The total weights may be different. In this case an extra ""dummy"" point is added to either signature1 or signature2. The weights must be non-negative and have at least one non-zero value.","signature2 : signature1と同じフォーマットの2つ目の署名（ただし，行数は異なる場合がある）．重みの合計は異なるかもしれない。この場合、追加の ""ダミー ""ポイントがsignature1またはsignature2のいずれかに追加される。重みは非負であり、少なくとも1つの非ゼロの値を持たなければならない。"
distType : Used metric. See DistanceTypes.,distType :使用されるメトリック。DistanceTypesを参照してください。
"cost : User-defined \(\texttt{size1}\times \texttt{size2}\) cost matrix. Also, if a cost matrix is used, lower boundary lowerBound cannot be calculated because it needs a metric function.",cost : ユーザー定義のコスト行列．また、コストマトリックスを使用した場合、lower boundary lowerBoundは、メトリック関数が必要なため計算できない。
"lowerBound : Optional input/output parameter: lower boundary of a distance between the two signatures that is a distance between mass centers. The lower boundary may not be calculated if the user-defined cost matrix is used, the total weights of point configurations are not equal, or if the signatures consist of weights only (the signature matrices have a single column). You must** initialize *lowerBound . If the calculated distance between mass centers is greater or equal to *lowerBound (it means that the signatures are far enough), the function does not calculate EMD. In any case *lowerBound is set to the calculated distance between mass centers on return. Thus, if you want to calculate both distance between mass centers and EMD, *lowerBound should be set to 0.",lowerBound : Optional input/output parameter: 質量中心間の距離である2つのシグネチャー間の距離の下界。ユーザー定義のコスト行列が使用されている場合や，点構成の総重量が等しくない場合，あるいはシグネチャが重量のみで構成されている場合（シグネチャ行列は1列）には，lowerBoundは計算されないことがある．また，*lowerBound を初期化する必要があります。計算された質量中心間の距離が *lowerBound 以上の場合（シグネチャが十分に離れていることを意味します），この関数は EMD を計算しません．どのような場合でも，*lowerBound は，リターン時に計算されたマスセンター間の距離に設定されます。したがって、マスセンター間の距離とEMDの両方を計算したい場合は、*lowerBoundを0に設定する必要があります。
"flow : Resultant \(\texttt{size1} \times \texttt{size2}\) flow matrix: \(\texttt{flow}_{i,j}\) is a flow from \(i\) -th point of signature1 to \(j\) -th point of signature2 .",flow :Resultant ≪Resultant≫ flow matrix:\は，signature1のi番目の点からsignature2のj番目の点までのフローです．
"The function computes the earth mover distance and/or a lower boundary of the distance between the two weighted point configurations. One of the applications described in [207], [208] is multi-dimensional histogram comparison for image retrieval. EMD is a transportation problem that is solved using some modification of a simplex algorithm, thus the complexity is exponential in the worst case, though, on average it is much faster. In the case of a real metric the lower boundary can be calculated even faster (using linear-time algorithm) and it can be used to determine roughly whether the two signatures are far enough so that they cannot relate to the same object.",この関数は，地球移動距離と，2つの重み付けされた点群間の距離の下限値を計算します．207]，[208]に記載されている応用例の1つに，画像検索のための多次元ヒストグラム比較があります．EMDは、シンプレックスアルゴリズムの修正を用いて解決される輸送問題であるため、複雑さは最悪のケースで指数関数的になりますが、平均的にははるかに高速です。実際のメトリックの場合、下側の境界は（線形時間アルゴリズムを使用して）さらに高速に計算でき、2つのシグネチャが同じオブジェクトに関連しないほど離れているかどうかを大まかに判断するのに使用できます。
Performs a marker-based image segmentation using the watershed algorithm.,watershed アルゴリズムを用いて、マーカーベースの画像セグメンテーションを行います。
image : Input 8-bit 3-channel image.,image : 入力 8 ビット，3 チャンネルの画像．
markers : Input/output 32-bit single-channel image (map) of markers. It should have the same size as image .,markers : マーカーの32ビットシングルチャンネル画像（マップ）を入力／出力します．image と同じサイズである必要があります．
"The function implements one of the variants of watershed, non-parametric marker-based segmentation algorithm, described in [169] .Before passing the image to the function, you have to roughly outline the desired regions in the image markers with positive (>0) indices. So, every region is represented as one or more connected components with the pixel values 1, 2, 3, and so on. Such markers can be retrieved from a binary mask using findContours and drawContours (see the watershed.cpp demo). The markers are ""seeds"" of the future image regions. All the other pixels in markers , whose relation to the outlined regions is not known and should be defined by the algorithm, should be set to 0's. In the function output, each pixel in markers is set to a value of the ""seed"" components or to -1 at boundaries between the regions.NoteAny two neighbor connected components are not necessarily separated by a watershed boundary (-1's pixels); for example, they can touch each other in the initial marker image passed to the function.See alsofindContoursExamples: samples/cpp/watershed.cpp.",この関数は，[169]で述べられている，ノンパラメトリックなマーカーベースのセグメンテーションアルゴリズムである watershed のバリエーションの1つを実装しています． 画像をこの関数に渡す前に，画像中の希望する領域を正（>0）のインデックスを持つマーカーで大まかに表しておく必要があります．つまり，すべての領域は，ピクセル値が1，2，3...となっている1つ以上の連結成分として表現されます．このようなマーカーは，findContoursやdrawContoursを使ってバイナリマスクから取り出すことができます（watershed.cppのデモを参照）．これらのマーカーは，将来の画像領域の「種」となります．マーカー内の他のすべてのピクセルは，アウトライン化された領域との関係がわからず，アルゴリズムによって定義されるべきものなので，0に設定されるべきです．注意隣り合う2つの連結成分は，必ずしも流域の境界（-1のピクセル）で区切られているわけではありません．例えば，この関数に渡される最初のマーカー画像の中で，お互いに接していても構いません．
Performs initial step of meanshift segmentation of an image.,画像に対して，Mean-Shift Segmentation の最初のステップを実行します．
"src : The source 8-bit, 3-channel image.",src : 8ビット，3チャンネルの入力画像．
dst : The destination image of the same format and the same size as the source.,dst : 入力画像と同じフォーマット，同じサイズの出力画像．
sp : The spatial window radius.,sp :空間窓の半径
sr : The color window radius.,sr : カラーウィンドウの半径を指定します。
maxLevel : Maximum level of the pyramid for the segmentation.,maxLevel : セグメンテーションのためのピラミッドの最大レベルです。
termcrit : Termination criteria: when to stop meanshift iterations.,termcrit : 終了基準：Meanshiftの反復をいつ止めるかを指定します．
"The function implements the filtering stage of meanshift segmentation, that is, the output of the function is the filtered ""posterized"" image with color gradients and fine-grain texture flattened. At every pixel (X,Y) of the input image (or down-sized input image, see below) the function executes meanshift iterations, that is, the pixel (X,Y) neighborhood in the joint space-color hyperspace is considered:\[(x,y): X- \texttt{sp} \le x \le X+ \texttt{sp} , Y- \texttt{sp} \le y \le Y+ \texttt{sp} , ||(R,G,B)-(r,g,b)|| \le \texttt{sr}\]where (R,G,B) and (r,g,b) are the vectors of color components at (X,Y) and (x,y), respectively (though, the algorithm does not depend on the color space used, so any 3-component color space can be used instead). Over the neighborhood the average spatial value (X',Y') and average color vector (R',G',B') are found and they act as the neighborhood center on the next iteration:\[(X,Y)~(X',Y'), (R,G,B)~(R',G',B').\]After the iterations over, the color components of the initial pixel (that is, the pixel from where the iterations started) are set to the final value (average color at the last iteration):\[I(X,Y) <- (R*,G*,B*)\]When maxLevel > 0, the gaussian pyramid of maxLevel+1 levels is built, and the above procedure is run on the smallest layer first. After that, the results are propagated to the larger layer and the iterations are run again only on those pixels where the layer colors differ by more than sr from the lower-resolution layer of the pyramid. That makes boundaries of color regions sharper. Note that the results will be actually different from the ones obtained by running the meanshift procedure on the whole original image (i.e. when maxLevel==0).","この関数は，Meanshiftセグメンテーションのフィルタリングステージを実装しています．つまり，この関数の出力は，色のグラデーションと細かいテクスチャが平坦になるようにフィルタリングされた「ポスタライズド」画像です．この関数は，入力画像（あるいは，ダウンサイズされた入力画像，下記参照）の各ピクセル（X,Y）に対して，Meanshiftの反復処理を行います．つまり，色のハイパースペースである結合空間におけるピクセル（X,Y）の近傍領域を考慮します．X- ˶ˆ꒳ˆ˵ X+ ˶ˆ꒳ˆ˵ Y- ˶ˆ꒳ˆ˵ Y+ ˶ˆ꒳ˆ˵ ||(R,G,B)-(r,g,b)|| ˶ˆ꒳ˆ˵]ここで、(R,G,B)と(r,g,b)は、色空間のベクトルです。b）は、それぞれ(X,Y)と(x,y)における色成分のベクトルである（ただし、このアルゴリズムは使用する色空間に依存しないため、任意の3成分色空間を使用することができる）。近傍では、平均空間値(X',Y')と平均色ベクトル(R',G',B')が求められ、次の反復では、これらが近傍の中心として機能します：\[(X,Y)~(X',Y'),(R,G,B)~(R',G',B').I(X,Y) <- (R*,G*,B*)}\\\\\\その後、その結果をより大きな層に伝搬させ、ピラミッドの低解像度層と層の色がsr以上異なる画素のみを対象に、再度繰り返し実行します。これにより、色の領域の境界がより鮮明になります。なお、元の画像全体に対してMeanshift処理を実行した場合（つまりmaxLevel==0の場合）とは、実際には異なる結果となります。"
Runs the GrabCut algorithm.,GrabCutアルゴリズムを実行します．
img : Input 8-bit 3-channel image.,img : 入力8ビット3チャンネル画像．
mask : Input/output 8-bit single-channel mask. The mask is initialized by the function when mode is set to GC_INIT_WITH_RECT. Its elements may have one of the GrabCutClasses.,mask : 入力/出力 8ビットシングルチャンネルマスク．このマスクは，modeがGC_INIT_WITH_RECTに設定されているときに，この関数によって初期化されます．その要素は，GrabCutClassesのいずれかを持ちます．
"rect : ROI containing a segmented object. The pixels outside of the ROI are marked as ""obvious background"". The parameter is only used when mode==GC_INIT_WITH_RECT .","rect :セグメント化されたオブジェクトを含むROI．ROIの外側のピクセルは ""明らかな背景 ""としてマークされます。このパラメータはmode==GC_INIT_WITH_RECTのときのみ使用されます。"
bgdModel : Temporary array for the background model. Do not modify it while you are processing the same image.,bgdModel : 背景モデルの一時的な配列．同じ画像を処理している間は、変更しないでください。
fgdModel : Temporary arrays for the foreground model. Do not modify it while you are processing the same image.,fgdModel : フォアグラウンドモデル用の一時的な配列．同じ画像を処理している間は、変更しないでください。
iterCount : Number of iterations the algorithm should make before returning the result. Note that the result can be refined with further calls with mode==GC_INIT_WITH_MASK or mode==GC_EVAL .,iterCount :アルゴリズムが結果を返すまでに行うべき反復の回数．なお，mode==GC_INIT_WITH_MASK または mode==GC_EVAL を指定してさらに呼び出すと，結果が洗練されます．
mode : Operation mode that could be one of the GrabCutModes,mode :GrabCutModesのうちの1つである操作モード
The function implements the GrabCut image segmentation algorithm.Examples: samples/cpp/grabcut.cpp.,この関数は，GrabCut画像分割アルゴリズムを実装しています．例：samples/cpp/grabcut.cpp.
Calculates the distance to the closest zero pixel for each pixel of the source image.,入力画像の各ピクセルについて，最も近い0ピクセルまでの距離を計算します．
"src : 8-bit, single-channel (binary) source image.",src : 8ビット，シングルチャンネル（2値）の入力画像．
"dst : Output image with calculated distances. It is a 8-bit or 32-bit floating-point, single-channel image of the same size as src.",dst : 距離が計算された出力画像．src と同じサイズの，8ビットまたは32ビット浮動小数点型，シングルチャンネルの画像です．
labels : Output 2D array of labels (the discrete Voronoi diagram). It has the type CV_32SC1 and the same size as src.,labels : ラベル（離散的なボロノイ図）の2次元配列を出力．これは，タイプが CV_32SC1 で， src と同じサイズです．
"distanceType : Type of distance, see DistanceTypes",distanceType :距離の種類， DistanceTypes を参照してください．
"maskSize : Size of the distance transform mask, see DistanceTransformMasks. DIST_MASK_PRECISE is not supported by this variant. In case of the DIST_L1 or DIST_C distance type, the parameter is forced to 3 because a \(3\times 3\) mask gives the same result as \(5\times 5\) or any larger aperture.",maskSize : 距離変換マスクのサイズ， DistanceTransformMasks を参照してください．DIST_MASK_PRECISE は，このバリアントではサポートされていません．DIST_L1やDIST_Cの距離タイプの場合、パラメータは強制的に3に設定される。これは、\(3times 3\)のマスクは、\(5times 5\)やそれ以上のアパーチャと同じ結果になるからである。
"labelType : Type of the label array to build, see DistanceTransformLabelTypes.",labelType :構築するラベル配列の種類，DistanceTransformLabelTypesを参照してください．
"The function cv::distanceTransform calculates the approximate or precise distance from every binary image pixel to the nearest zero pixel. For zero image pixels, the distance will obviously be zero.When maskSize == DIST_MASK_PRECISE and distanceType == DIST_L2 , the function runs the algorithm described in [72] . This algorithm is parallelized with the TBB library.In other cases, the algorithm [28] is used. This means that for a pixel the function finds the shortest path to the nearest zero pixel consisting of basic shifts: horizontal, vertical, diagonal, or knight's move (the latest is available for a \(5\times 5\) mask). The overall distance is calculated as a sum of these basic distances. Since the distance function should be symmetric, all of the horizontal and vertical shifts must have the same cost (denoted as a ), all the diagonal shifts must have the same cost (denoted as b), and all knight's moves must have the same cost (denoted as c). For the DIST_C and DIST_L1 types, the distance is calculated precisely, whereas for DIST_L2 (Euclidean distance) the distance can be calculated only with a relative error (a \(5\times 5\) mask gives more accurate results). For a,b, and c, OpenCV uses the values suggested in the original paper:DIST_L1: a = 1, b = 2","関数 cv::distanceTransform は，2値画像の各ピクセルから，最も近い0ピクセルまでのおおよその，あるいは正確な距離を計算します．maskSize == DIST_MASK_PRECISE かつ distanceType == DIST_L2 の場合，この関数は [72] で述べられているアルゴリズムを実行します．このアルゴリズムは，TBB ライブラリによって並列化されています．その他の場合は，アルゴリズム [28] が利用されます．つまり，この関数は，あるピクセルに対して，水平，垂直，斜め，騎士の動きという基本的なシフトからなる，最も近い0ピクセルへの最短経路を求めます（最新のものは，\\のマスクで利用可能です）．全体の距離は、これらの基本的な距離の合計として計算されます。距離関数は対称でなければならないので、水平方向と垂直方向のシフトはすべて同じコスト（aと表記）でなければならず、対角線方向のシフトはすべて同じコスト（bと表記）でなければならず、ナイトの動きはすべて同じコスト（cと表記）でなければなりません。DIST_C と DIST_L1 では、距離は正確に計算されますが、DIST_L2 (Euclidean distance) では、距離は相対的な誤差でしか計算できません (mask を使うとより正確な結果が得られます)。a,b,c に対して，OpenCV は原著論文で提案された値を使用します： DIST_L1: a = 1, b = 2"
DIST_L2:,"DIST_L2: a = 1, b = 2:"
"3 x 3: a=0.955, b=1.3693","3×3: a=0.955, b=1.3693"
"5 x 5: a=1, b=1.4, c=2.1969","5×5： a=1, b=1.4, c=2.1969"
"DIST_C: a = 1, b = 1Typically, for a fast, coarse distance estimation DIST_L2, a \(3\times 3\) mask is used. For a more accurate distance estimation DIST_L2, a \(5\times 5\) mask or the precise algorithm is used. Note that both the precise and the approximate algorithms are linear on the number of pixels.This variant of the function does not only compute the minimum distance for each pixel \((x, y)\) but also identifies the nearest connected component consisting of zero pixels (labelType==DIST_LABEL_CCOMP) or the nearest zero pixel (labelType==DIST_LABEL_PIXEL). Index of the component/pixel is stored in labels(x, y). When labelType==DIST_LABEL_CCOMP, the function automatically finds connected components of zero pixels in the input image and marks them with distinct labels. When labelType==DIST_LABEL_PIXEL, the function scans through the input image and marks all the zero pixels with distinct labels.In this mode, the complexity is still linear. That is, the function provides a very fast way to compute the Voronoi diagram for a binary image. Currently, the second variant can use only the approximate distance transform algorithm, i.e. maskSize=DIST_MASK_PRECISE is not supported yet.Examples: samples/cpp/distrans.cpp.","DIST_C: a = 1, b = 1通常、高速で粗い距離推定DIST_L2には、˶‾᷄ -̫ ‾᷅˵のマスクを使用する。より正確な距離推定DIST_L2のためには、\(5\)マスクや精密アルゴリズムを用いる。この関数は，各ピクセルの最小距離\((x, y)\)を計算するだけでなく，0個のピクセルで構成される最近接連結成分(labelType==DIST_LABEL_CCOMP)や最近接0ピクセル(labelType==DIST_LABEL_PIXEL)を特定します．labels(x, y)には，成分・画素のインデックスが格納されます．labelType==DIST_LABEL_CCOMP の場合，この関数は，入力画像中のゼロピクセルの連結成分を自動的に見つけ，それらに個別のラベルを付けます．labelType==DIST_LABEL_PIXEL の場合，この関数は入力画像を走査し，すべての0ピクセルを個別のラベルでマークします．このモードでは，複雑さは線形です．つまり，この関数は，2値画像のボロノイ図を非常に高速に計算する方法を提供します．現在のところ，2番目のモードでは，近似距離変換アルゴリズムしか利用できません．つまり， maskSize=DIST_MASK_PRECISE はまだサポートされていません．"
Fills a connected component with the given color.,与えられた色で連結成分を塗りつぶします．
"image : Input/output 1- or 3-channel, 8-bit, or floating-point image. It is modified by the function unless the FLOODFILL_MASK_ONLY flag is set in the second variant of the function. See the details below.",image : 1-または3-チャンネル，8ビット，または浮動小数点型の画像を入出力します．関数の第2バリアントでFLOODFILL_MASK_ONLYフラグが設定されていない限り，この関数によって変更されます．詳細は以下の通りです。
"mask : Operation mask that should be a single-channel 8-bit image, 2 pixels wider and 2 pixels taller than image. Since this is both an input and output parameter, you must take responsibility of initializing it. Flood-filling cannot go across non-zero pixels in the input mask. For example, an edge detector output can be used as a mask to stop filling at edges. On output, pixels in the mask corresponding to filled pixels in the image are set to 1 or to the a value specified in flags as described below. Additionally, the function fills the border of the mask with ones to simplify internal processing. It is therefore possible to use the same mask in multiple calls to the function to make sure the filled areas do not overlap.",mask : シングルチャンネルの8ビット画像で，imageよりも幅が2ピクセル，高さが2ピクセル大きいオペレーションマスクです．これは入力でもあり出力でもあるので，ユーザが責任を持って初期化しなければいけません．フラッドフィリングは、入力マスクのゼロでないピクセルを越えて行うことはできません。例えば、エッジ検出器の出力をマスクとして使用すると、エッジでのフィリングを止めることができます。出力時には，画像中の塗りつぶされたピクセルに対応するマスク内のピクセルが，後述する flags で指定された値，あるいは 1 に設定されます．さらにこの関数は，内部処理を簡単にするために，マスクの境界を 1 で埋めます．したがって，塗り潰した部分が重ならないようにするために，この関数を複数回呼び出す際には，同じマスクを使うことができます．
seedPoint : Starting point.,seedPoint : 開始点。
newVal : New value of the repainted domain pixels.,newVal : 塗り替えられた領域のピクセルの新しい値です。
"loDiff : Maximal lower brightness/color difference between the currently observed pixel and one of its neighbors belonging to the component, or a seed pixel being added to the component.",loDiff : 現在観測されているピクセルと、コンポーネントに属する隣接ピクセルの1つ、またはコンポーネントに追加されるシードピクセルとの、最大の下側輝度/色差。
"upDiff : Maximal upper brightness/color difference between the currently observed pixel and one of its neighbors belonging to the component, or a seed pixel being added to the component.",upDiff : 現在観測されているピクセルと、コンポーネントに属する隣接ピクセルのうちの1つ、またはコンポーネントに追加されるシードピクセルとの間の最大の上側輝度/色差
rect : Optional output parameter set by the function to the minimum bounding rectangle of the repainted domain.,rect : オプションの出力パラメータで，塗り替えられた領域の最小外接矩形を指定します．
"flags : Operation flags. The first 8 bits contain a connectivity value. The default value of 4 means that only the four nearest neighbor pixels (those that share an edge) are considered. A connectivity value of 8 means that the eight nearest neighbor pixels (those that share a corner) will be considered. The next 8 bits (8-16) contain a value between 1 and 255 with which to fill the mask (the default value is 1). For example, 4 | ( 255 << 8 ) will consider 4 nearest neighbours and fill the mask with a value of 255. The following additional options occupy higher bits and therefore may be further combined with the connectivity and mask fill values using bit-wise or (|), see FloodFillFlags.",flags :操作フラグ．最初の8ビットには，接続性の値が含まれます．デフォルト値の4は，4つの最近接ピクセル（エッジを共有するピクセル）のみを考慮することを意味します．接続性の値が8の場合は、8つの最近接ピクセル（コーナーを共有するピクセル）が考慮されることを意味します。次の8ビット（8-16）には、マスクを埋めるための1～255の値が入ります（デフォルト値は1）。例えば、「4｜（255 << 8）」と入力すると、4つの最近接ピクセルが考慮され、マスクは255の値で埋められます。以下の追加オプションは上位ビットを占有するため，ビット単位の和や（｜）を用いて，接続性やマスクの塗りつぶし値とさらに組み合わせることができます（FloodFillFlags を参照してください）．
"The function cv::floodFill fills a connected component starting from the seed point with the specified color. The connectivity is determined by the color/brightness closeness of the neighbor pixels. The pixel at \((x,y)\) is considered to belong to the repainted domain if:in case of a grayscale image and floating range ",関数 cv::floodFill は，種点から始まる連結成分を，指定された色で塗りつぶします．この連結性は，隣接するピクセルの色と明るさの近さによって決まります．このピクセルは，以下の場合に，塗り替え領域に属するとみなされます：グレースケール画像で，浮動小数点数の場合
"\[\texttt{src} (x',y')- \texttt{loDiff} \leq \texttt{src} (x,y) \leq \texttt{src} (x',y')+ \texttt{upDiff}\]","\(x',y')-(x',y')-(x',y')(x',y')-\\\\\\(x,y)には、\\\が含まれます。(x',y')+ ˶‾᷄ -̫ ‾᷅˵"
in case of a grayscale image and fixed range ,グレースケール画像で範囲が決まっている場合は
"\[\texttt{src} ( \texttt{seedPoint} .x, \texttt{seedPoint} .y)- \texttt{loDiff} \leq \texttt{src} (x,y) \leq \texttt{src} ( \texttt{seedPoint} .x, \texttt{seedPoint} .y)+ \texttt{upDiff}\]","\♪♪～( ˶ˆ꒳ˆ˵ )\\\\(x,y) ˶‾᷄ -̫ ‾᷅˵˵( ˶ˆ꒳ˆ˵ ) + ˶ˆ꒳ˆ˵ )"
in case of a color image and floating range ,カラー画像でフローティングレンジの場合
"\[\texttt{src} (x',y')_r- \texttt{loDiff} _r \leq \texttt{src} (x,y)_r \leq \texttt{src} (x',y')_r+ \texttt{upDiff} _r,\]","\\\\(x',y')_r- ˶‾᷄↪Lo_Diff↩‾᷅˵_r ardor(x,y)_r ˶‾᷄ -̫ ‾᷅˵(x',y')_r+ ˶‾᷄ -̫ ‾᷅˵˵_r,\]"
 , 
"\[\texttt{src} (x',y')_g- \texttt{loDiff} _g \leq \texttt{src} (x,y)_g \leq \texttt{src} (x',y')_g+ \texttt{upDiff} _g\]","\♪♪～(x',y')_g - ˶‾᷄ -̫ ‾᷅˵˵_g ˶‾᷄ -̫ ‾᷅˵(x,y)_g \\(x',y')_g+ ˶‾᷄ -̫ ‾᷅˵˵_g\]"
 and , そして
"\[\texttt{src} (x',y')_b- \texttt{loDiff} _b \leq \texttt{src} (x,y)_b \leq \texttt{src} (x',y')_b+ \texttt{upDiff} _b\]","\\\\(x',y')_b- ˶‾᷄↪Lo_Diff↩‾᷅˵_b wrestler's Mr.(x,y)_b ˶‾᷄ -̫ ‾᷅˵(x',y')_b+ ˶‾᷄ -̫ ‾᷅˵˵_b\]"
in case of a color image and fixed range ,カラー画像で範囲が固定されている場合
"\[\texttt{src} ( \texttt{seedPoint} .x, \texttt{seedPoint} .y)_r- \texttt{loDiff} _r \leq \texttt{src} (x,y)_r \leq \texttt{src} ( \texttt{seedPoint} .x, \texttt{seedPoint} .y)_r+ \texttt{upDiff} _r,\]","\♪♪～( ˶ˆ꒳ˆ˵ )_r ardor(x,y)_r \\\( ˶˙º̬˙˶ )_r+ ˶˙º̬˙˶ )_r,\]"
"\[\texttt{src} ( \texttt{seedPoint} .x, \texttt{seedPoint} .y)_g- \texttt{loDiff} _g \leq \texttt{src} (x,y)_g \leq \texttt{src} ( \texttt{seedPoint} .x, \texttt{seedPoint} .y)_g+ \texttt{upDiff} _g\]","\♪♪♪♪♪～( ˶ˆ꒳ˆ˵ ) _g- ˶loDiff˶_g ̶̶̶̶̶̶̶̶̶̶̶̶̶̶̶̶̶̶̶̶̶̶ 600(x,y)_g W\\\( ˶ˆ꒳ˆ˵ ) _g+ ˶ˆ꒳ˆ˵ )_g\]"
"\[\texttt{src} ( \texttt{seedPoint} .x, \texttt{seedPoint} .y)_b- \texttt{loDiff} _b \leq \texttt{src} (x,y)_b \leq \texttt{src} ( \texttt{seedPoint} .x, \texttt{seedPoint} .y)_b+ \texttt{upDiff} _b\]where \(src(x',y')\) is the value of one of pixel neighbors that is already known to belong to the component. That is, to be added to the connected component, a color/brightness of the pixel should be close enough to:Color/brightness of one of its neighbors that already belong to the connected component in case of a floating range.","\\\\( ˶ˆ꒳ˆ˵ ) _b- ˶ˆ꒳ˆ˵_b ˶ˆ꒳ˆ˵ )(x,y)_b ˶‾᷄ -̫ ‾᷅˵˵( ˶˙º̬˙˶ )_b+ ˶˙º̬˙˶ )ここで，\(src(x',y')\)は，すでにコンポーネントに属することがわかっているピクセルネイバーの1つの値です．つまり，連結成分に追加されるためには，ピクセルの色や明るさが，次のように十分に近い必要があります：浮動小数点数の場合は，既に連結成分に属している隣のピクセルの色や明るさ．"
"Color/brightness of the seed point in case of a fixed range.Use these functions to either mark a connected component with the specified color in-place, or build a mask and then extract the contour, or copy the region to another image, and so on.NoteSince the mask is larger than the filled image, a pixel \((x, y)\) in image corresponds to the pixel \((x+1, y+1)\) in the mask .See alsofindContours","これらの関数を利用して，指定した色で連結成分をその場でマークしたり，マスクを作成してから輪郭を抽出したり，その領域を別の画像にコピーしたりすることができます．注意：マスクは塗りつぶした画像よりも大きいので，画像中のピクセル ˶((x, y)˶)は，マスク中のピクセル ˶((x+1, y+1)˶)に相当します．"
Performs linear blending of two images:,2つの画像の線形ブレンドを行います．
"src1 : It has a type of CV_8UC(n) or CV_32FC(n), where n is a positive integer.",src1 ：タイプが CV_8UC(n) または CV_32FC(n) （ここで n は正の整数）．
src2 : It has the same type and size as src1.,src2 : src1 と同じ型，同じサイズ．
weights1 : It has a type of CV_32FC1 and the same size with src1.,weights1 ：タイプが CV_32FC1 で，サイズは src1 と同じです．
weights2 : It has a type of CV_32FC1 and the same size with src1.,weights2 : src1 と同じサイズで，タイプが CV_32FC1 のもの．
dst : It is created if it does not have the same size and type with src1.,dst : src1 と同じサイズとタイプを持たない場合に作成されます。
"\[ \texttt{dst}(i,j) = \texttt{weights1}(i,j)*\texttt{src1}(i,j) + \texttt{weights2}(i,j)*\texttt{src2}(i,j) \]",\˶ˆ꒳ˆ˵ ( ˶ˆ꒳ˆ˵ ) = ˶ˆ꒳ˆ˵ ( ˶ˆ꒳ˆ˵ ) + ˶ˆ꒳ˆ˵ ( ˶ˆ꒳ˆ˵ )
Converts an image from one color space to another.,画像を，ある色空間から別の色空間に変換します．
"src : input image: 8-bit unsigned, 16-bit unsigned ( CV_16UC... ), or single-precision floating-point.",src : 入力画像．8ビット符号なし，16ビット符号なし（CV_16UC...），または単精度浮動小数点．
dst : output image of the same size and depth as src.,dst : src と同じサイズ，同じ深度の出力画像．
code : color space conversion code (see ColorConversionCodes).,code : 色空間変換コード（ColorConversionCodes 参照）．
"dstCn : number of channels in the destination image; if the parameter is 0, the number of the channels is derived automatically from src and code.",dstCn : 出力画像のチャンネル数．パラメータが0の場合，チャンネル数は src と code から自動的に求められます．
"The function converts an input image from one color space to another. In case of a transformation to-from RGB color space, the order of the channels should be specified explicitly (RGB or BGR). Note that the default color format in OpenCV is often referred to as RGB but it is actually BGR (the bytes are reversed). So the first byte in a standard (24-bit) color image will be an 8-bit Blue component, the second byte will be Green, and the third byte will be Red. The fourth, fifth, and sixth bytes would then be the second pixel (Blue, then Green, then Red), and so on.The conventional ranges for R, G, and B channel values are:0 to 255 for CV_8U images",この関数は，入力画像をある色空間から別の色空間に変換します．RGB色空間からの変換の場合は，チャンネルの順番を明示的に指定する必要があります（RGBまたはBGR）．OpenCVのデフォルトの色形式は，しばしばRGBと呼ばれますが，実際にはBGR（バイトが逆）であることに注意してください．つまり，標準的な（24ビット）カラー画像の1バイト目は，8ビットの青成分，2バイト目は緑，3バイト目は赤になります．R、G、Bのチャンネル値の一般的な範囲は次のとおりです：CV_8U画像では0〜255
0 to 65535 for CV_16U images,CV_8U画像では0〜255、CV_16U画像では0〜65535
"0 to 1 for CV_32F imagesIn case of linear transformations, the range does not matter. But in case of a non-linear transformation, an input RGB image should be normalized to the proper value range to get the correct results, for example, for RGB \(\rightarrow\) L*u*v* transformation. For example, if you have a 32-bit floating-point image directly converted from an 8-bit image without any scaling, then it will have the 0..255 value range instead of 0..1 assumed by the function. So, before calling cvtColor , you need first to scale the image down:img *= 1./255;cvtColor(img, img, COLOR_BGR2Luv);fragmentIf you use cvtColor with 8-bit images, the conversion will have some information lost. For many applications, this will not be noticeable but it is recommended to use 32-bit images in applications that need the full range of colors or that convert an image before an operation and then convert back.If conversion adds the alpha channel, its value will set to the maximum of corresponding channel range: 255 for CV_8U, 65535 for CV_16U, 1 for CV_32F.See alsoColor conversionsExamples: samples/cpp/camshiftdemo.cpp, samples/cpp/edge.cpp, samples/cpp/facedetect.cpp, samples/cpp/ffilldemo.cpp, samples/cpp/lkdemo.cpp, samples/cpp/train_HOG.cpp, samples/cpp/tutorial_code/ImgTrans/houghcircles.cpp, samples/cpp/tutorial_code/ImgTrans/houghlines.cpp, samples/cpp/tutorial_code/ImgTrans/Sobel_Demo.cpp, samples/cpp/tutorial_code/ml/introduction_to_pca/introduction_to_pca.cpp, samples/cpp/watershed.cpp, samples/dnn/colorization.cpp, samples/dnn/text_detection.cpp, and samples/tapi/hog.cpp.","R、G、Bのチャンネル値の範囲は、CV_8U映像では0〜255、CV_16U映像では0〜65535、CV_32F映像では0〜1です。しかし、非線形変換の場合、正しい結果を得るためには、入力RGB画像を適切な値の範囲に正規化する必要があります。例えば、RGB ˶ˆ꒳ˆ˵ L*u*v* 変換の場合です。例えば，8 ビット画像から直接変換された 32 ビット浮動小数点画像をスケーリングせずに入力した場合，この関数が想定している 0 ～ 1 ではなく，0 ～ 255 の値域を持つことになります．ですから， cvtColor を呼び出す前に，まず画像を縮小する必要があります： img *= 1./255;cvtColor(img, img, COLOR_BGR2Luv);fragment8 ビット画像に対して cvtColor を使用した場合，変換によって失われる情報があります．変換によってアルファチャンネルが追加された場合，その値は対応するチャンネル範囲の最大値に設定されます：CV_8Uでは255，CV_16Uでは65535，CV_32Fでは1です．参照：色の変換例サンプル：samples/cpp/camshiftdemo.cpp，samples/cpp/edge.cpp, samples/cpp/facedetect.cpp, samples/cpp/ffilldemo.cpp, samples/cpp/lkdemo.cpp, samples/cpp/train_HOG.cpp, samples/cpp/tutorial_code/ImgTrans/houghcircles.cpp, samples/cpp/tutorial_code/ImgTrans/houghlines.cpp、samples/cpp/tutorial_code/ImgTrans/Sobel_Demo.cpp、samples/cpp/tutorial_code/ml/introduction_to_pca/introduction_to_pca.cpp、samples/cpp/watershed.cpp、samples/dnn/colorization.cpp、samples/dnn/text_detection.cpp、samples/tapi/hog.cppがあります。"
Converts an image from one color space to another where the source image is stored in two planes.,ソース画像が2つのプレーンに格納されている場合に，画像をある色空間から別の色空間に変換します．
src1 : 8-bit image (CV_8U) of the Y plane.,src1 : Y平面の8ビット画像（CV_8U）．
src2 : image containing interleaved U/V plane.,src2 : インターリーブされた U/V プレーンを含む画像．
dst : output image.,dst : 出力画像．
code : Specifies the type of conversion. It can take any of the following values:,code :変換の種類を指定します．以下のいずれかの値をとります．
COLOR_YUV2BGR_NV12,カラー_YUV2BGR_NV12
COLOR_YUV2RGB_NV12,カラー_YUV2RGB_NV12
COLOR_YUV2BGRA_NV12,カラー_YUV2BGR_NV12
COLOR_YUV2RGBA_NV12,カラー_ユビ2rgba_nv12
COLOR_YUV2BGR_NV21,カラー_YUV2BGR_NV21
COLOR_YUV2RGB_NV21,カラー_YUV2RGB_NV21
COLOR_YUV2BGRA_NV21,カラー_YUV2BGR_NV21
COLOR_YUV2RGBA_NV21,カラー_ユビ2rgba_nv21
This function only supports YUV420 to RGB conversion as of now.,この関数は、現在のところ、YUV420からRGBへの変換のみに対応しています。
main function for all demosaicing processes,全てのデモザイク処理を行うメイン関数
src : input image: 8-bit unsigned or 16-bit unsigned.,src : 入力画像．8ビット符号なし，または，16ビット符号なし．
code : Color space conversion code (see the description below).,code :色空間変換コード（後述の説明を参照してください）．
The function can do the following transformations:Demosaicing using bilinear interpolation,この関数は，以下の変換を行うことができます： バイリニア補間を用いたデモザイク処理
"COLOR_BayerBG2BGR , COLOR_BayerGB2BGR , COLOR_BayerRG2BGR , COLOR_BayerGR2BGR","COLOR_BayerBG2BGR , COLOR_BayerGB2BGR , COLOR_BayerRG2BGR , COLOR_BayerGR2BGR"
"COLOR_BayerBG2GRAY , COLOR_BayerGB2GRAY , COLOR_BayerRG2GRAY , COLOR_BayerGR2GRAY",COLOR_BayerBG2GRAY、COLOR_BayerGB2GRAY、COLOR_BayerRG2GRAY、COLOR_BayerGR2GRAY
Demosaicing using Variable Number of Gradients.,可変グラデーション数を用いたデモザイキング。
"COLOR_BayerBG2BGR_VNG , COLOR_BayerGB2BGR_VNG , COLOR_BayerRG2BGR_VNG , COLOR_BayerGR2BGR_VNG","COLOR_BayerBG2BGR_VNG , COLOR_BayerGB2BGR_VNG , COLOR_BayerRG2BGR_VNG , COLOR_BayerGR2BGR_VNG"
Edge-Aware Demosaicing.,エッジアウェア・デモザイキング。
"COLOR_BayerBG2BGR_EA , COLOR_BayerGB2BGR_EA , COLOR_BayerRG2BGR_EA , COLOR_BayerGR2BGR_EA","COLOR_BayerBG2BGR_EA , COLOR_BayerGB2BGR_EA , COLOR_BayerRG2BGR_EA , COLOR_BayerGR2BGR_EA"
Demosaicing with alpha channel,アルファチャンネルによるデモザイキング
"COLOR_BayerBG2BGRA , COLOR_BayerGB2BGRA , COLOR_BayerRG2BGRA , COLOR_BayerGR2BGRASee alsocvtColor","COLOR_BayerBG2BGRA , COLOR_BayerGB2BGRA , COLOR_BayerRG2BGRA , COLOR_BayerGR2BGRASee alsocvtColor"
Calculates all of the moments up to the third order of a polygon or rasterized shape.,ポリゴンやラスタライズされた形状の3次までのモーメントをすべて計算します。
"array : Raster image (single-channel, 8-bit or floating-point 2D array) or an array ( \(1 \times N\) or \(N \times 1\) ) of 2D points (Point or Point2f ).",array : ラスタ画像（シングルチャンネル，8ビット，または，浮動小数点型の2次元配列），または，2次元点（Point または Point2f ）の配列（ ˶‾᷄ -̫ ‾᷅˵ ）．
"binaryImage : If it is true, all non-zero image pixels are treated as 1's. The parameter is used for images only.",binaryImage : trueの場合，0ではない画像のピクセルはすべて1として扱われます．このパラメータは，画像に対してのみ利用されます．
"The function computes moments, up to the 3rd order, of a vector shape or a rasterized shape. The results are returned in the structure cv::Moments.NoteOnly applicable to contour moments calculations from Python bindings: Note that the numpy type for the input array should be either np.int32 or np.float32.See alsocontourArea, arcLength",この関数は，ベクトル形状やラスタライズされた形状の，3次までのモーメントを求めます．その結果は，構造体 cv::Moments として返されます．注意Pythonバインディングによる輪郭のモーメント計算にのみ適用されます．入力配列の numpy 型は，np.int32 か np.float32 であることに注意してください．
Compares a template against overlapped image regions.,重ね合わせた画像領域に対してテンプレートを比較します。
image : Image where the search is running. It must be 8-bit or 32-bit floating-point.,image : 探索を行う画像．8ビットまたは32ビット浮動小数点でなければなりません．
templ : Searched template. It must be not greater than the source image and have the same data type.,templ : 検索されたテンプレート．元画像より大きくない、同じデータ型であることが必要です。
"result : Map of comparison results. It must be single-channel 32-bit floating-point. If image is \(W \times H\) and templ is \(w \times h\) , then result is \((W-w+1) \times (H-h+1)\) .",result : 比較結果のマップ．シングルチャンネルの32ビット浮動小数点である必要があります．image が ˶ˆ꒳ˆ˵ ) templ が ˶ˆ꒳ˆ˵ ) の場合、result は ˶ˆ꒳ˆ˵ ) です。
"method : Parameter specifying the comparison method, see TemplateMatchModes",method : 比較方法を指定するパラメータ．TemplateMatchModes を参照．
"mask : Optional mask. It must have the same size as templ. It must either have the same number of channels as template or only one channel, which is then used for all template and image channels. If the data type is CV_8U, the mask is interpreted as a binary mask, meaning only elements where mask is nonzero are used and are kept unchanged independent of the actual mask value (weight equals 1). For data tpye CV_32F, the mask values are used as weights. The exact formulas are documented in TemplateMatchModes.",mask : オプションのマスク．templ と同じサイズであること．また，テンプレートと同じ数のチャンネルを持つか，あるいは1つのチャンネルのみを持つ必要があり，そのチャンネルがテンプレートと画像のすべてのチャンネルに使われます．データ型が CV_8U の場合，マスクはバイナリマスクとして解釈されます．つまり，マスクが0ではない要素のみが利用され，実際のマスク値に依存せずに変更されません（重みは1になります）．CV_32Fまでのデータでは，マスクの値が重みとして使われます．正確な計算式は，TemplateMatchModes に記載されています．
"The function slides through image , compares the overlapped patches of size \(w \times h\) against templ using the specified method and stores the comparison results in result . TemplateMatchModes describes the formulae for the available comparison methods ( \(I\) denotes image, \(T\) template, \(R\) result, \(M\) the optional mask ). The summation is done over template and/or the image patch: \(x' = 0...w-1, y' = 0...h-1\)After the function finishes the comparison, the best matches can be found as global minimums (when TM_SQDIFF was used) or maximums (when TM_CCORR or TM_CCOEFF was used) using the minMaxLoc function. In case of a color image, template summation in the numerator and each sum in the denominator is done over all of the channels and separate mean values are used for each channel. That is, the function can take a color template and a color image. The result will still be a single-channel image, which is easier to analyze.Examples: samples/cpp/tutorial_code/Histograms_Matching/MatchTemplate_Demo.cpp.",この関数は， image をスライドさせながら，指定された方法で， templ に対してサイズ \ (w times h\) のオーバーラップしたパッチを比較し，その比較結果を result に格納します．TemplateMatchModes は，利用可能な比較手法の式を記述したものです ( ˶‾᷄ -̫ ‾᷅˵ )。総和は、テンプレートや画像パッチに対して行われます。\関数が比較を終えた後、minMaxLoc関数を使って、グローバルな最小値（TM_SQDIFFを使った場合）や最大値（TM_CCORRやTM_CCOEFFを使った場合）として、ベストマッチを見つけることができます。カラー画像の場合、分子のテンプレート和と分母の各和は、すべてのチャンネルに対して行われ、各チャンネルに別々の平均値が使用されます。つまり，この関数は，カラーテンプレートとカラー画像を受け取ることができます．例： samples/cpp/tutorial_code/Histograms_Matching/MatchTemplate_Demo.cpp.
computes the connected components labeled image of boolean image,ブーリアン画像の連結成分ラベル付き画像を計算します．
image : the 8-bit single-channel image to be labeled,image : ラベル付けされる8ビットシングルチャンネル画像．
labels : destination labeled image,labels : ラベル付けされた出力画像
connectivity : 8 or 4 for 8-way or 4-way connectivity respectively,"connectivity : 8-way, 4-way の接続性を表す 8 または 4．"
ltype : output image label type. Currently CV_32S and CV_16U are supported.,ltype : 出力画像のラベルタイプ．現在は，CV_32S と CV_16U がサポートされています．
ccltype : connected components algorithm type (see the ConnectedComponentsAlgorithmsTypes).,ccltype : 連結成分アルゴリズムタイプ（ConnectedComponentsAlgorithmsTypesを参照してください）．
"image with 4 or 8 way connectivity - returns N, the total number of labels [0, N-1] where 0 represents the background label. ltype specifies the output label image type, an important consideration based on the total number of labels or alternatively the total number of pixels in the source image. ccltype specifies the connected components labeling algorithm to use, currently Grana (BBDT) and Wu's (SAUF) [276] algorithms are supported, see the ConnectedComponentsAlgorithmsTypes for details. Note that SAUF algorithm forces a row major ordering of labels while BBDT does not. This function uses parallel version of both Grana and Wu's algorithms if at least one allowed parallel framework is enabled and if the rows of the image are at least twice the number returned by getNumberOfCPUs.Examples: samples/cpp/connected_components.cpp.",ltype は，出力ラベルの画像タイプを指定します．これは，ラベルの総数，あるいは，ソース画像の総ピクセル数に基づく重要な考慮事項です． ccltype は，使用する連結成分ラベリングアルゴリズムを指定します．現在，Grana (BBDT) と Wu (SAUF) [276] のアルゴリズムがサポートされており，詳細は ConnectedComponentsAlgorithmsTypes を参照してください．なお、SAUFアルゴリズムでは、BBDTがそうでないのに対し、ラベルの行の主要な順序が強制されます。この関数は，GranaおよびWuの両アルゴリズムの並列版を使用します．ただし，少なくとも1つの許可された並列フレームワークが有効であり，イメージの行数がgetNumberOfCPUsで返される数の少なくとも2倍である場合に限ります．
computes the connected components labeled image of boolean image and also produces a statistics output for each label,boolean画像の連結成分ラベル付き画像を計算し，各ラベルの統計情報を出力します．
"stats : statistics output for each label, including the background label. Statistics are accessed via stats(label, COLUMN) where COLUMN is one of ConnectedComponentsTypes, selecting the statistic. The data type is CV_32S.","stats : 背景ラベルも含めた各ラベルの統計出力．統計量には stats(label, COLUMN) でアクセスします．ここで COLUMN は ConnectedComponentsTypes の1つであり，統計量を選択します．データタイプは，CV_32Sです．"
"centroids : centroid output for each label, including the background label. Centroids are accessed via centroids(label, 0) for x and centroids(label, 1) for y. The data type CV_64F.","centroids : 背景ラベルを含む，各ラベルのセントロイド出力．セントロイドへのアクセスは，xに対しては centroids(label, 0)，yに対しては centroids(label, 1)です．データ型は CV_64F です．"
ccltype : connected components algorithm type (see ConnectedComponentsAlgorithmsTypes).,ccltype : 連結成分アルゴリズムタイプ（ ConnectedComponentsAlgorithmsTypes を参照してください）．
"image with 4 or 8 way connectivity - returns N, the total number of labels [0, N-1] where 0 represents the background label. ltype specifies the output label image type, an important consideration based on the total number of labels or alternatively the total number of pixels in the source image. ccltype specifies the connected components labeling algorithm to use, currently Grana's (BBDT) and Wu's (SAUF) [276] algorithms are supported, see the ConnectedComponentsAlgorithmsTypes for details. Note that SAUF algorithm forces a row major ordering of labels while BBDT does not. This function uses parallel version of both Grana and Wu's algorithms (statistics included) if at least one allowed parallel framework is enabled and if the rows of the image are at least twice the number returned by getNumberOfCPUs.",ltype は，出力ラベルの画像タイプを指定します．これは，ラベルの総数，あるいは，ソース画像の総ピクセル数に基づく重要な考慮事項です． ccltype は，使用する連結成分ラベリングアルゴリズムを指定します．現在，Grana (BBDT) と Wu (SAUF) [276] のアルゴリズムがサポートされており，詳細は ConnectedComponentsAlgorithmsTypes を参照してください．なお，SAUFアルゴリズムはラベルの行の主要な順序を強制しますが，BBDTはそうではありません．この関数は，少なくとも1つの許可された並列フレームワークが有効であり，画像の行数がgetNumberOfCPUsで返される数の少なくとも2倍であれば，GranaとWuの両方のアルゴリズムの並列バージョン（統計情報を含む）を使用します．
Finds contours in a binary image.,2値画像の輪郭を検出します．
"image : Source, an 8-bit single-channel image. Non-zero pixels are treated as 1's. Zero pixels remain 0's, so the image is treated as binary . You can use compare, inRange, threshold , adaptiveThreshold, Canny, and others to create a binary image out of a grayscale or color one. If mode equals to RETR_CCOMP or RETR_FLOODFILL, the input can also be a 32-bit integer image of labels (CV_32SC1).","image : 入力，8ビットシングルチャンネル画像．0ではないピクセルは1として扱われます．ゼロのピクセルは0のままなので，画像はバイナリとして扱われます．compare, inRange, threshold, adaptiveThreshold, Canny などを使って、グレースケールやカラーの画像からバイナリ画像を作ることができます。mode が RETR_CCOMP または RETR_FLOODFILL の場合，入力は32ビット整数のラベル画像 (CV_32SC1) にすることもできます．"
contours : Detected contours. Each contour is stored as a vector of points (e.g. std::vector<std::vector<cv::Point> >).,contours : 検出された輪郭．各輪郭は，点のベクトルとして格納されます（例えば，std::vector<std::vector<cv::Point> >）．
"hierarchy : Optional output vector (e.g. std::vector<cv::Vec4i>), containing information about the image topology. It has as many elements as the number of contours. For each i-th contour contours[i], the elements hierarchy[i][0] , hierarchy[i][1] , hierarchy[i][2] , and hierarchy[i][3] are set to 0-based indices in contours of the next and previous contours at the same hierarchical level, the first child contour and the parent contour, respectively. If for the contour i there are no next, previous, parent, or nested contours, the corresponding elements of hierarchy[i] will be negative.","hierarchy : オプションの出力ベクトル（例：std::vector<cv::Vec4i>）で，画像のトポロジーに関する情報が格納されています．輪郭の数と同じ数の要素を持ちます．i番目の各輪郭contours[i]に対して，要素hierarchy[i][0] , hierarchy[i][1] , hierarchy[i][2] , hierarchy[i][3]は，同じ階層レベルの次の輪郭と前の輪郭，最初の子輪郭と親輪郭の輪郭における0ベースのインデックスにそれぞれ設定される．輪郭iに対して、次の輪郭、前の輪郭、親の輪郭、入れ子の輪郭が存在しない場合、hierarchy[i]の対応する要素は負の値となる。"
"mode : Contour retrieval mode, see RetrievalModes",モード :輪郭の検索モード，検索モード参照．
"method : Contour approximation method, see ContourApproximationModes",method : 輪郭の近似法，ContourApproximationModes 参照．
offset : Optional offset by which every contour point is shifted. This is useful if the contours are extracted from the image ROI and then they should be analyzed in the whole image context.,offset : オプションで，各輪郭点を移動させるためのオフセットを指定します．これは，画像ROIから輪郭を抽出し，それを画像全体で分析したい場合に役立ちます．
"The function retrieves contours from the binary image using the algorithm [233] . The contours are a useful tool for shape analysis and object detection and recognition. See squares.cpp in the OpenCV sample directory.NoteSince opencv 3.2 source image is not modified by this function.Examples: modules/shape/samples/shape_example.cpp, samples/cpp/contours2.cpp, samples/cpp/fitellipse.cpp, samples/cpp/segment_objects.cpp, samples/cpp/squares.cpp, samples/cpp/tutorial_code/ml/introduction_to_pca/introduction_to_pca.cpp, samples/cpp/watershed.cpp, and samples/tapi/squares.cpp.","この関数は，アルゴリズム [233]を用いて，2値画像から輪郭を抽出します．輪郭は，形状分析や物体の検出・認識に役立つツールです．OpenCVのサンプルディレクトリにあるsquares.cppを参照してください．注意 opencv 3.2以降では，この関数によってソース画像が変更されることはありません．例： modules/shape/samples/shape_example.cpp, samples/cpp/contours2.cpp, samples/cpp/fitellipse.cpp、samples/cpp/segment_objects.cpp、samples/cpp/squares.cpp、samples/cpp/tutorial_code/ml/introduction_to_pca/introduction_to_pca.cpp、samples/cpp/watershed.cpp、samples/tapi/squares.cppがあります。"
Approximates a polygonal curve(s) with the specified precision.,多角形の曲線を指定された精度で近似します。
curve : Input vector of a 2D point stored in std::vector or Mat,curve : std::vector または Mat に格納された2次元点の入力ベクトル．
approxCurve : Result of the approximation. The type should match the type of the input curve.,approxCurve :approxCurve : 近似された曲線の結果．その型は，入力曲線の型と一致しなければいけません．
epsilon : Parameter specifying the approximation accuracy. This is the maximum distance between the original curve and its approximation.,epsilon : 近似精度を指定するパラメータ．これは，元の曲線とその近似曲線の間の最大距離を表します．
"closed : If true, the approximated curve is closed (its first and last vertices are connected). Otherwise, it is not closed.",closed : trueの場合、近似曲線が閉じている（最初の頂点と最後の頂点がつながっている）ことを示します。そうでない場合は，閉じていません．
"The function cv::approxPolyDP approximates a curve or a polygon with another curve/polygon with less vertices so that the distance between them is less or equal to the specified precision. It uses the Douglas-Peucker algorithm http://en.wikipedia.org/wiki/Ramer-Douglas-Peucker_algorithmExamples: samples/cpp/contours2.cpp, samples/cpp/squares.cpp, and samples/tapi/squares.cpp.","関数 cv::approxPolyDP は，曲線や多角形を，より少ない頂点を持つ別の曲線や多角形で近似し，それらの間の距離が指定された精度以下になるようにします．Douglas-Peucker アルゴリズムを利用します． http://en.wikipedia.org/wiki/Ramer-Douglas-Peucker_algorithmExamples: samples/cpp/contours2.cpp, samples/cpp/squares.cpp, and samples/tapi/squares.cpp."
Calculates a contour perimeter or a curve length.,輪郭の外周や曲線の長さを計算します。
"curve : Input vector of 2D points, stored in std::vector or Mat.",curve : std::vector または Mat に格納された，2次元点の入力ベクトル．
closed : Flag indicating whether the curve is closed or not.,closed : 曲線が閉じているかどうかを示すフラグ．
"The function computes a curve length or a closed contour perimeter.Examples: samples/cpp/squares.cpp, and samples/tapi/squares.cpp.","例： samples/cpp/squares.cpp, samples/tapi/squares.cpp."
Calculates the up-right bounding rectangle of a point set or non-zero pixels of gray-scale image.,点セットまたはグレースケール画像の非ゼロピクセルの右上境界矩形を計算します。
"array : Input gray-scale image or 2D point set, stored in std::vector or Mat.",array : std::vector または Mat に格納された，入力されるグレースケール画像または2次元ポイントセット．
The function calculates and returns the minimal up-right bounding rectangle for the specified point set or non-zero pixels of gray-scale image.,この関数は，指定された点集合，あるいはグレースケール画像の非0ピクセルに対する最小の右上がりの外接矩形を計算して返します．
Calculates a contour area.,輪郭領域を計算します．
"contour : Input vector of 2D points (contour vertices), stored in std::vector or Mat.",contour : 2次元点（輪郭の頂点）の入力ベクトル，std::vector または Mat に格納されます．
"oriented : Oriented area flag. If it is true, the function returns a signed area value, depending on the contour orientation (clockwise or counter-clockwise). Using this feature you can determine orientation of a contour by taking the sign of an area. By default, the parameter is false, which means that the absolute value is returned.",oriented :Oriented area フラグ．これが真の場合，この関数は，輪郭の向き（時計回りか反時計回りか）に応じた符号付きの面積値を返します．この機能を利用すると，面積の符号を取ることで，輪郭の向きを決定することができます．デフォルトでは，このパラメータはfalseであり，絶対値が返されます．
"The function computes a contour area. Similarly to moments , the area is computed using the Green formula. Thus, the returned area and the number of non-zero pixels, if you draw the contour using drawContours or fillPoly , can be different. Also, the function will most certainly give a wrong results for contours with self-intersections.Example:vector<Point> contour;contour.push_back(Point2f(0, 0));contour.push_back(Point2f(10, 0));contour.push_back(Point2f(10, 10));contour.push_back(Point2f(5, 4));double area0 = contourArea(contour);vector<Point> approx;approxPolyDP(contour, approx, 5, true);double area1 = contourArea(approx);cout << ""area0 ="" << area0 << endl <<        ""area1 ="" << area1 << endl <<        ""approx poly vertices"" << approx.size() << endl;fragmentExamples: samples/cpp/segment_objects.cpp, samples/cpp/squares.cpp, samples/cpp/tutorial_code/ml/introduction_to_pca/introduction_to_pca.cpp, and samples/tapi/squares.cpp.","この関数は，輪郭の面積を計算します．モーメントと同様に，面積は Green の式を用いて計算されます。したがって，drawContours や fillPoly を用いて輪郭を描いた場合，返される面積と，0ではないピクセル数は異なります．例：vector<Point> contour;contour.push_back(Point2f(0, 0));contour.push_back(Point2f(10, 0));contour.push_back(Point2f(10, 10));contour.push_back(Point2f(10, 10));contour.push_back(Point2f(5, 4));double area0 = contourArea(contour);vector<Point> approx;approxPolyDP(contour, approx, 5, true);double area1 = contourArea(approx);cout << ""area0 ="" << area0 << endl << ""area1 ="" << area1 << endl << ""approx poly vertices"" << approx.size() << endl;fragmentExamples: samples/cpp/segment_objects.cpp, samples/cpp/squares.cpp, samples/cpp/tutorial_code/ml/introduction_to_pca/introduction_to_pca.cpp, and samples/tapi/squares.cpp."
Finds a rotated rectangle of the minimum area enclosing the input 2D point set.,入力された2D点セットを囲む最小面積の回転した長方形を見つけます。
"points : Input vector of 2D points, stored in std::vector<> or Mat",points :std::vector<> または Mat に格納された，2次元点の入力ベクトル．
The function calculates and returns the minimum-area bounding rectangle (possibly rotated) for a specified point set. Developer should keep in mind that the returned RotatedRect can contain negative indices when data is close to the containing Mat element boundary.Examples: samples/cpp/minarea.cpp.,この関数は，指定された点群を囲む最小面積の外接矩形（回転している場合もあります）を計算して返します．データが含まれる Mat 要素の境界に近い場合，返される RotatedRect には負のインデックスが含まれる可能性があることに，開発者は留意する必要があります．例： samples/cpp/minarea.cpp．
Finds the four vertices of a rotated rect. Useful to draw the rotated rectangle.,回転した矩形の 4 つの頂点を求めます．回転した矩形の描画に役立ちます．
box : The input rotated rectangle. It may be the output of,box : 入力される回転した矩形．の出力である場合もあります．
points : The output array of four vertices of rectangles.,points :矩形の4つの頂点を表す出力配列．
"The function finds the four vertices of a rotated rectangle. This function is useful to draw the rectangle. In C++, instead of using this function, you can directly use RotatedRect::points method. Please visit the tutorial on Creating Bounding rotated boxes and ellipses for contours for more information.",この関数は，回転した矩形の 4 つの頂点を求めます．この関数は，矩形の描画に役立ちます．C++ では，この関数を使う代わりに， RotatedRect::points メソッドを直接使うことができます．詳しくは、チュートリアルの「輪郭用の回転ボックスと楕円の作成」をご覧ください。
Finds a circle of the minimum area enclosing a 2D point set.,2次元点群を囲む最小面積の円を求めます。
center : Output center of the circle.,center : 円の中心を出力します。
radius : Output radius of the circle.,radius : 円の半径を出力します．
The function finds the minimal enclosing circle of a 2D point set using an iterative algorithm.Examples: samples/cpp/minarea.cpp.,この関数は，反復アルゴリズムを用いて，2 次元点集合を囲む最小の円を求めます．例： samples/cpp/minarea.cpp.
Finds a triangle of minimum area enclosing a 2D point set and returns its area.,2次元点群を囲む最小面積の三角形を求め、その面積を返す。
"points : Input vector of 2D points with depth CV_32S or CV_32F, stored in std::vector<> or Mat",points :CV_32S または CV_32F の深さを持つ2次元点の入力ベクトル，std::vector<> または Mat に格納されます．
triangle : Output vector of three 2D points defining the vertices of the triangle. The depth of the OutputArray must be CV_32F.,triangle : 三角形の頂点を定義する3つの2次元点の出力ベクトル．OutputArray の深さは CV_32F でなければいけません．
The function finds a triangle of minimum area enclosing the given set of 2D points and returns its area. The output for a given 2D point set is shown in the image below. 2D points are depicted in red* and the enclosing triangle in yellow.Sample output of the minimum enclosing triangle functionThe implementation of the algorithm is based on O'Rourke's [188] and Klee and Laskowski's [129] papers. O'Rourke provides a \(\theta(n)\) algorithm for finding the minimal enclosing triangle of a 2D convex polygon with n vertices. Since the minEnclosingTriangle function takes a 2D point set as input an additional preprocessing step of computing the convex hull of the 2D point set is required. The complexity of the convexHull function is \(O(n log(n))\) which is higher than \(\theta(n)\). Thus the overall complexity of the function is \(O(n log(n))\).Examples: samples/cpp/minarea.cpp.,この関数は，与えられた2次元点群を囲む最小面積の三角形を求め，その面積を返します．与えられた2次元点群に対する出力は，以下の図のようになります．このアルゴリズムの実装は，O'Rourke[188]およびKlee and Laskowski[129]の論文に基づいています．O'Rourke氏は、n個の頂点を持つ2次元凸多角形の最小囲み三角形を求めるために、\(theta(n)\)アルゴリズムを提供しています。minEnclosingTriangle関数は入力として2Dポイントセットを取るので、2Dポイントセットの凸包を計算する追加の前処理ステップが必要です。convexHull関数の複雑さは、\(theta(n)\)よりも高い、\(O(n log(n))\です。）例： samples/cpp/minarea.cpp.
Compares two shapes.,2つの形状を比較します．
contour1 : First contour or grayscale image.,contour1 ：1つ目の輪郭，またはグレースケール画像．
contour2 : Second contour or grayscale image.,contour2 : 2 番目の輪郭またはグレースケール画像．
"method : Comparison method, see ShapeMatchModes",method : 比較手法，「ShapeMatchModes」を参照．
parameter : Method-specific parameter (not supported now).,parameter : メソッド固有のパラメータ（現在はサポートされていません）。
The function compares two shapes. All three implemented methods use the Hu invariants (see HuMoments),この関数は，2 つの形状を比較します．実装されている 3 つのメソッドは，いずれも Hu 不変量を利用しています（HuMoments 参照）．
Finds the convex hull of a point set.,点集合の凸包を求める関数．
"points : Input 2D point set, stored in std::vector or Mat.",points :std::vector または Mat に格納された，入力2次元点群．
"hull : Output convex hull. It is either an integer vector of indices or vector of points. In the first case, the hull elements are 0-based indices of the convex hull points in the original array (since the set of convex hull points is a subset of the original point set). In the second case, hull elements are the convex hull points themselves.",hull : 凸包の出力．これは，インデックスを表す整数ベクトルか，点のベクトルです．最初のケースでは，船体要素は，元の配列における凸包点の0ベースのインデックスになります（凸包点の集合は，元の点集合の部分集合なので）．2 番目のケースでは， hull 要素は凸包点そのものです．
"clockwise : Orientation flag. If it is true, the output convex hull is oriented clockwise. Otherwise, it is oriented counter-clockwise. The assumed coordinate system has its X axis pointing to the right, and its Y axis pointing upwards.",clockwise : 向きのフラグ．このフラグが真の場合，出力される凸包は時計回りに配向されます．そうでない場合は，反時計回りになります．想定される座標系は、X軸が右を向き、Y軸が上を向いている。
"returnPoints : Operation flag. In case of a matrix, when the flag is true, the function returns convex hull points. Otherwise, it returns indices of the convex hull points. When the output array is std::vector, the flag is ignored, and the output depends on the type of the vector: std::vector<int> implies returnPoints=false, std::vector<Point> implies returnPoints=true.",returnPoints :操作フラグ．行列の場合，このフラグが真であれば，この関数は凸包の点を返します．それ以外の場合は，凸包点のインデックスを返します．出力配列が std::vector の場合，このフラグは無視され，出力はベクトルの種類に依存します： std::vector<int> は returnPoints=false を意味し， std::vector<Point> は returnPoints=true を意味します．
"The function cv::convexHull finds the convex hull of a 2D point set using the Sklansky's algorithm [224] that has O(N logN) complexity in the current implementation.Notepoints and hull should be different arrays, inplace processing isn't supported.Check the corresponding tutorial for more details.useful links:https://www.learnopencv.com/convex-hull-using-opencv-in-python-and-c/Examples: samples/cpp/convexhull.cpp.",関数 cv::convexHull は，Sklansky のアルゴリズム [224] を用いて 2 次元点群の凸包を求めます．
Finds the convexity defects of a contour.,輪郭の凸型欠陥を求めます．
contour : Input contour.,contour : 入力輪郭。
convexhull : Convex hull obtained using convexHull that should contain indices of the contour points that make the hull.,convexhull : convexHullを用いて得られた凸包で，凸包を構成する輪郭点のインデックスを含みます．
"convexityDefects : The output vector of convexity defects. In C++ and the new Python/Java interface each convexity defect is represented as 4-element integer vector (a.k.a. Vec4i): (start_index, end_index, farthest_pt_index, fixpt_depth), where indices are 0-based indices in the original contour of the convexity defect beginning, end and the farthest point, and fixpt_depth is fixed-point approximation (with 8 fractional bits) of the distance between the farthest contour point and the hull. That is, to get the floating-point value of the depth will be fixpt_depth/256.0.","convexityDefects :凸性欠陥の出力ベクトル．C++および新しいPython/Javaインターフェースでは，各凸状欠損は4要素の整数ベクトル（a.k.a. Vec4i）で表されます．(start_index, end_index, farthest_pt_index, fixpt_depth), ここで index は，凸状欠損の始点，終点，最遠点の元の輪郭における0ベースのインデックスであり，fixpt_depth は，最遠の輪郭点とハルとの間の距離の固定小数点近似値（8ビットの小数ビット）です．つまり、深さの浮動小数点値を求めるには、fixpt_depth/256.0となります。"
The figure below displays convexity defects of a hand contour:image,下の図は、手の輪郭の凸部の欠陥を表示しています：image
Tests a contour convexity.,輪郭の凸性をテストします．
"contour : Input vector of 2D points, stored in std::vector<> or Mat",contour : 2次元点の入力ベクトル，std::vector<> または Mat に格納されます．
"The function tests whether the input contour is convex or not. The contour must be simple, that is, without self-intersections. Otherwise, the function output is undefined.Examples: samples/cpp/intersectExample.cpp, samples/cpp/squares.cpp, and samples/tapi/squares.cpp.",この関数は，入力された輪郭が凸であるかどうかをテストします．輪郭は単純でなければならず，つまり，自己交差があってはいけません．例：samples/cpp/intersectExample.cpp，samples/cpp/squares.cpp，samples/tapi/squares.cpp．
Finds intersection of two convex polygons.,2つの凸多角形の交点を求める関数です。
p1 : First polygon,p1 : 第一多角形
p2 : Second polygon,p2 : 二番目の多角形
p12 : Output polygon describing the intersecting area,p12 :交差部を表す出力ポリゴン
"handleNested : When true, an intersection is found if one of the polygons is fully enclosed in the other. When false, no intersection is found. If the polygons share a side or the vertex of one polygon lies on an edge of the other, they are not considered nested and an intersection will be found regardless of the value of handleNested.",handleNested : true の場合、片方の多角形がもう片方の多角形に完全に囲まれている場合に交点を発見します。falseの場合、交点は見つからない。一方のポリゴンの頂点が他方のポリゴンの辺上にある場合は、handleNestedの値に関わらず、入れ子になっているとはみなされず、交点が検出されます。
NoteintersectConvexConvex doesn't confirm that both polygons are convex and will return invalid results if they aren't.Examples: samples/cpp/intersectExample.cpp.,注意intersectConvexConvexは、両方のポリゴンが凸であるかどうかを確認せず、凸でない場合は無効な結果を返します。
Fits an ellipse around a set of 2D points.,2次元の点の周りに楕円を描く関数です。
"points : Input 2D point set, stored in std::vector<> or Mat",points :std::vector<> または Mat に格納された，入力2次元点集合．
"The function calculates the ellipse that fits (in a least-squares sense) a set of 2D points best of all. It returns the rotated rectangle in which the ellipse is inscribed. The first algorithm described by [78] is used. Developer should keep in mind that it is possible that the returned ellipse/rotatedRect data contains negative indices, due to the data points being close to the border of the containing Mat element.Examples: samples/cpp/fitellipse.cpp.",この関数は，2次元点の集合に最もよくフィットする（最小二乗の意味で）楕円を計算します．そして，その楕円が内接する，回転した矩形を返します．ここでは，[78]で述べられている最初のアルゴリズムを使用しています．開発者は，データポイントが含まれる Mat 要素の境界に近いために，返される楕円/回転矩形データに負のインデックスが含まれる可能性があることに留意する必要があります．例： samples/cpp/fitellipse.cpp.
"The function calculates the ellipse that fits a set of 2D points. It returns the rotated rectangle in which the ellipse is inscribed. The Approximate Mean Square (AMS) proposed by [240] is used.For an ellipse, this basis set is \( \chi= \left(x^2, x y, y^2, x, y, 1\right) \), which is a set of six free coefficients \( A^T=\left\{A_{\text{xx}},A_{\text{xy}},A_{\text{yy}},A_x,A_y,A_0\right\} \). However, to specify an ellipse, all that is needed is five numbers; the major and minor axes lengths \( (a,b) \), the position \( (x_0,y_0) \), and the orientation \( \theta \). This is because the basis set includes lines, quadratics, parabolic and hyperbolic functions as well as elliptical functions as possible fits. If the fit is found to be a parabolic or hyperbolic function then the standard fitEllipse method is used. The AMS method restricts the fit to parabolic, hyperbolic and elliptical curves by imposing the condition that \( A^T ( D_x^T D_x + D_y^T D_y) A = 1 \) where the matrices \( Dx \) and \( Dy \) are the partial derivatives of the design matrix \( D \) with respect to x and y. The matrices are formed row by row applying the following to each of the points in the set:\begin{align*} D(i,:)&=\left\{x_i^2, x_i y_i, y_i^2, x_i, y_i, 1\right\} & D_x(i,:)&=\left\{2 x_i,y_i,0,1,0,0\right\} & D_y(i,:)&=\left\{0,x_i,2 y_i,0,1,0\right\} \end{align*}The AMS method minimizes the cost function\begin{equation*} \epsilon ^2=\frac{ A^T D^T D A }{ A^T (D_x^T D_x + D_y^T D_y) A^T } \end{equation*}The minimum cost is found by solving the generalized eigenvalue problem.\begin{equation*} D^T D A = \lambda \left( D_x^T D_x + D_y^T D_y\right) A \end{equation*}Examples: samples/cpp/fitellipse.cpp.","この関数は，2 次元点の集合にフィットする楕円を計算します．そして，その楕円が内接する回転した矩形を返します．ここでは，[240]で提案された Approximate Mean Square (AMS) が用いられます．楕円の場合、この基底セットは、6つの自由係数の集合である \chi= \left(x^2, x y, y^2, x, y, 1\right) \( A^T=\left\{A_{text{xx}},A_{text{xy}},A_{text{yy}},A_x,A_y,A_0\right} ˶ ) です。しかし、楕円を指定するには、長軸と短軸の長さ\( (a,b) )、位置\( (x_0,y_0) )、方向\( ˶ˆ꒳ˆ˵ ) の5つの数字があればよいのです。これは、基底セットには、直線、二次曲線、放物線、双曲線のほか、楕円形の関数もフィットする可能性があるからです。フィットが放物線または双曲線関数であることが判明した場合、標準のfitEllipseメソッドが使用されます。AMS法では、フィットを放物線、双曲線、楕円曲線に限定するために、「A^T ( D_x^T D_x + D_y^T D_y) A = 1 ˶ˆ꒳ˆ˵」という条件を課しています。このマトリクスは、セット内の各ポイントに以下のように適用して、行ごとに形成されます。D(i,:)&=Begin{x_i^2, x_i y_i, y_i^2, x_i, y_i, 1\\} & D_x(i,:)&=Begin{2 x_i,y_i,0,1,0,0\\} & D_y(i,:)&=Begin{0,x_i,2 y_i,0,1,0,0嵓｝\end{align*}AMS法は、コスト関数を最小化します。\A^T D^T D A }{ A^T (D_x^T D_x + D_y^T D_y) A^T } E\end{equation*}最小コストは、一般化された固有値問題を解くことで求められます。D^T D A = ˶ˆ꒳ˆ˵ )"
"The function calculates the ellipse that fits a set of 2D points. It returns the rotated rectangle in which the ellipse is inscribed. The Direct least square (Direct) method by [79] is used.For an ellipse, this basis set is \( \chi= \left(x^2, x y, y^2, x, y, 1\right) \), which is a set of six free coefficients \( A^T=\left\{A_{\text{xx}},A_{\text{xy}},A_{\text{yy}},A_x,A_y,A_0\right\} \). However, to specify an ellipse, all that is needed is five numbers; the major and minor axes lengths \( (a,b) \), the position \( (x_0,y_0) \), and the orientation \( \theta \). This is because the basis set includes lines, quadratics, parabolic and hyperbolic functions as well as elliptical functions as possible fits. The Direct method confines the fit to ellipses by ensuring that \( 4 A_{xx} A_{yy}- A_{xy}^2 > 0 \). The condition imposed is that \( 4 A_{xx} A_{yy}- A_{xy}^2=1 \) which satisfies the inequality and as the coefficients can be arbitrarily scaled is not overly restrictive.\begin{equation*} \epsilon ^2= A^T D^T D A \quad \text{with} \quad A^T C A =1 \quad \text{and} \quad C=\left(\begin{matrix} 0 & 0 & 2 & 0 & 0 & 0 \\ 0 & -1 & 0 & 0 & 0 & 0 \\ 2 & 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 & 0 \end{matrix} \right) \end{equation*}The minimum cost is found by solving the generalized eigenvalue problem.\begin{equation*} D^T D A = \lambda \left( C\right) A \end{equation*}The system produces only one positive eigenvalue \( \lambda\) which is chosen as the solution with its eigenvector \(\mathbf{u}\). These are used to find the coefficients\begin{equation*} A = \sqrt{\frac{1}{\mathbf{u}^T C \mathbf{u}}} \mathbf{u} \end{equation*}The scaling factor guarantees that \(A^T C A =1\).Examples: samples/cpp/fitellipse.cpp.","この関数は，2 次元の点の集合にフィットする楕円を計算します．また，楕円が内接する回転した矩形を返します．ここでは，[79]のDirect least square（Direct）法を用いています．楕円の場合、この基底セットは、6つの自由係数の集合である \chi= \left(x^2, x y, y^2, x, y, 1\right) \( A^T=\left\{A_{text{xx}},A_{text{xy}},A_{text{yy}},A_x,A_y,A_0\right) ⁾⁾となります。しかし、楕円を指定するには、長軸と短軸の長さ\( (a,b) )、位置\( (x_0,y_0) )、方向\( ˶ˆ꒳ˆ˵ ) の5つの数字があればよいのです。これは、基底セットには、直線、二次曲線、放物線、双曲線のほか、楕円形の関数もフィットする可能性があるからです。Direct法では、フィッティングの対象を楕円に限定するために、\( 4 A_{xx} A_{yy}- A_{xy}^2 > 0 ˶ )を確保します。この条件は、不等式を満足し、かつ係数を任意にスケーリングできることから、過度に制限するものではありません。\A^T D^T D A ˶‾᷄ -̫ ‾᷅˵\A^T C A =1\♪♪♪♪♪～0＆0＆2＆0＆0＆0 0＆1＆0＆0＆0 2＆0＆0＆0＆0＆0＆0 0＆0＆0＆0＆0＆0＆0＆0＆0＆0＆0＆0＆0＆0＆0＆0＆0＆0＆0＆0＆0＆0＆0＆0＆0＆0＆0＆0＆0＆0＆0エンド{matrix}です。D^T D A = \lambda \left( C\right) A ୨୧{equation*}このシステムでは、正の固有値は1つしかなく、その固有ベクトル\(mathbf{u}\)を解としています。これらを用いて、係数を求めます。A = ˶ˆ꒳ˆ˵ )\A^T C A =1」が保証されています。例： samples/cpp/fitellipse.cpp."
Fits a line to a 2D or 3D point set.,2Dまたは3Dのポイントセットに直線をフィットさせます。
"points : Input vector of 2D or 3D points, stored in std::vector<> or Mat.",points :std::vector<> または Mat に格納された，2次元または3次元の点の入力ベクトル．
"line : Output line parameters. In case of 2D fitting, it should be a vector of 4 elements (like Vec4f) - (vx, vy, x0, y0), where (vx, vy) is a normalized vector collinear to the line and (x0, y0) is a point on the line. In case of 3D fitting, it should be a vector of 6 elements (like Vec6f) - (vx, vy, vz, x0, y0, z0), where (vx, vy, vz) is a normalized vector collinear to the line and (x0, y0, z0) is a point on the line.","line : 出力される線のパラメータ．ここで，(vx, vy)は直線に平行な正規化されたベクトル，(x0, y0)は直線上の点を表します．3Dフィッティングの場合は，6要素のベクトル（Vec6fなど） - (vx, vy, vz, x0, y0, z0), ここで (vx, vy, vz) は直線に平行な正規化されたベクトル， (x0, y0, z0) は直線上の点です．"
"distType : Distance used by the M-estimator, see DistanceTypes",distType :M-estimatorで利用される距離，参照： DistanceTypes
"param : Numerical parameter ( C ) for some types of distances. If it is 0, an optimal value is chosen.",param :param : いくつかのタイプの距離を表す数値パラメータ（C）．これが0の場合，最適な値が選択されます．
reps : Sufficient accuracy for the radius (distance between the coordinate origin and the line).,reps :半径（座標原点から直線までの距離）に十分な精度を指定します。
aeps : Sufficient accuracy for the angle. 0.01 would be a good default value for reps and aeps.,aeps :角度に対して十分な精度を持ちます。reps と aeps のデフォルト値は 0.01 が良いでしょう。
"The function fitLine fits a line to a 2D or 3D point set by minimizing \(\sum_i \rho(r_i)\) where \(r_i\) is a distance between the \(i^{th}\) point, the line and \(\rho(r)\) is a distance function, one of the following:DIST_L2 ",関数 fitLine は、2D または 3D の点群に直線をフィットさせるために、\(r_i\)を最小化します。 ここで、\(r_i\)は、点と直線との距離であり、\(rho(r)\)は、距離関数であり、次のいずれかです。
\[\rho (r) = r^2/2 \quad \text{(the simplest and the fastest least-squares method)}\],\rho (r) = r^2/2 \\{(the simplest and the fast least-squares method)}\]である。
DIST_L1 ,DIST_L1
\[\rho (r) = r\],\rho (r) = r\
DIST_L12 ,DIST_L12
\[\rho (r) = 2 \cdot ( \sqrt{1 + \frac{r^2}{2}} - 1)\],\Rho (r) = 2 ˶ˆ꒳ˆ˵ ( ˶ˆ꒳ˆ˵ )
DIST_FAIR ,DIST_FAIR
\[\rho \left (r \right ) = C^2 \cdot \left ( \frac{r}{C} - \log{\left(1 + \frac{r}{C}\right)} \right ) \quad \text{where} \quad C=1.3998\],\DIST_FAIR] DIST_FAIR- (1 + ˶‾᷄д‾᷅˵)\♪♪♪♪♪♪♪）\Here's nothing!\C=1.3998\]の
DIST_WELSCH ,DIST_WELSCH
\[\rho \left (r \right ) = \frac{C^2}{2} \cdot \left ( 1 - \exp{\left(-\left(\frac{r}{C}\right)^2\right)} \right ) \quad \text{where} \quad C=2.9846\],\♪♪♪♪♪♪～\♪♪♪♪♪♪♪～\♪♪～C=2.9846\\］
DIST_HUBER ,DIST_HUBER
\[\rho (r) = \fork{r^2/2}{if \(r < C\)}{C \cdot (r-C/2)}{otherwise} \quad \text{where} \quad C=1.345\]The algorithm is based on the M-estimator ( http://en.wikipedia.org/wiki/M-estimator ) technique that iteratively fits the line using the weighted least-squares algorithm. After each iteration the weights \(w_i\) are adjusted to be inversely proportional to \(\rho(r_i)\) .,\Rho (r) = R^2/2}{if uriosity(r < C\)}{C cdot (r-C/2)}{otherwise}\\\\\このアルゴリズムは、M-estimator( http://en.wikipedia.org/wiki/M-estimator )という手法に基づいており、重み付き最小二乗法を用いて繰り返し直線をフィットさせます。反復後の重みは、反比例するように調整されています(w_i\)。
Performs a point-in-contour test.,point-in-contourテストを実行します。
pt : Point tested against the contour.,pt :輪郭に対してテストされた点。
"measureDist : If true, the function estimates the signed distance from the point to the nearest contour edge. Otherwise, the function only checks if the point is inside a contour or not.",measureDist : true の場合，この関数は，点から最も近い輪郭のエッジまでの符号付き距離を推定します．そうでない場合，この関数は，点が輪郭内にあるかどうかのみをチェックします．
"The function determines whether the point is inside a contour, outside, or lies on an edge (or coincides with a vertex). It returns positive (inside), negative (outside), or zero (on an edge) value, correspondingly. When measureDist=false , the return value is +1, -1, and 0, respectively. Otherwise, the return value is a signed distance between the point and the nearest contour edge.See below a sample output of the function where each image pixel is tested against the contour:sample output",この関数は，点が輪郭の内側にあるのか，外側にあるのか，あるいは辺上にあるのか（あるいは頂点と一致するのか）を判定します．それぞれ、正の値（内側）、負の値（外側）、ゼロ（エッジ上）が返されます。measureDist=false の場合、戻り値はそれぞれ +1、-1、0 です。それ以外の場合，戻り値は，点と最も近い輪郭のエッジとの符号付き距離になります．この関数の出力例を以下に示します．
Finds out if there is any intersection between two rotated rectangles.,回転させた2つの矩形の間に交点があるかどうかを調べます．
rect1 : First rectangle,rect1 : 1 番目の矩形
rect2 : Second rectangle,rect2 : 2 番目の矩形
intersectingRegion : The output array of the vertices of the intersecting region. It returns at most 8 vertices. Stored as std::vector<cv::Point2f> or cv::Mat as Mx1 of type CV_32FC2.,intersectingRegion : 交差する領域の頂点を示す出力配列．最大で8個の頂点が返される。std::vector<cv::Point2f> または cv::Mat の CV_32FC2 型の Mx1 として格納されます．
If there is then the vertices of the intersecting region are returned as well.Below are some examples of intersection configurations. The hatched pattern indicates the intersecting region and the red vertices are returned by the function.intersection examples,これがある場合は，交差する領域の頂点も同様に返されます．ハッチングされたパターンは交差領域を示し，赤の頂点はこの関数によって返されます．intersection の例
Applies a GNU Octave/MATLAB equivalent colormap on a given image.,GNU Octave/MATLAB と同等のカラーマップを，与えられた画像に適用します．
"src : The source image, grayscale or colored of type CV_8UC1 or CV_8UC3.",src : CV_8UC1 または CV_8UC3 型のグレースケールまたはカラーの入力画像．
dst : The result is the colormapped source image. Note: Mat::create is called on dst.,dst : カラーマップされた元画像が結果として得られます．注意： Mat::create は dst で呼び出されます．
"colormap : The colormap to apply, see ColormapTypes",colormap : 適用されるcolormap，ColormapTypesを参照してください．
Examples: samples/cpp/falsecolor.cpp.,例： samples/cpp/falecolor.cpp．
Draws a arrow segment pointing from the first point to the second one.,1つ目の点から2つ目の点を指す矢印セグメントを描画します．
img : Image.,img : 画像です。
pt1 : The point the arrow starts from.,pt1 : 矢印が始まる点。
pt2 : The point the arrow points to.,pt2 : 矢印が指し示す点です。
color : Line color.,color : 線の色。
thickness : Line thickness.,thickness : 線の太さ。
line_type : Type of the line. See LineTypes,line_type :線の種類。関連項目： LineTypes
shift : Number of fractional bits in the point coordinates.,shift : 点座標の小数点以下のビット数です。
tipLength : The length of the arrow tip in relation to the arrow length,tipLength : 矢の長さに対する矢の先端の長さ．
The function cv::arrowedLine draws an arrow between pt1 and pt2 points in the image. See also line.,関数 cv::arrowedLine は，画像上の点 pt1 と pt2 の間に矢印を描きます．line も参照してください．
"Draws a simple, thick, or filled up-right rectangle.",単純な，太い，あるいは塗りつぶされた右上がりの矩形を描画します．
pt1 : Vertex of the rectangle.,pt1 : 矩形の頂点．
pt2 : Vertex of the rectangle opposite to pt1 .,pt2 : 矩形の頂点で、pt1と反対側の頂点です。
color : Rectangle color or brightness (grayscale image).,color : 矩形の色または明るさ（グレースケール画像）。
"thickness : Thickness of lines that make up the rectangle. Negative values, like FILLED, mean that the function has to draw a filled rectangle.",thickness : 矩形を構成する線の太さを指定します。FILLEDのような負の値は，この関数が塗りつぶされた矩形を描画することを意味します．
lineType : Type of the line. See LineTypes,lineType :線の種類。LineTypes を参照してください．
"The function cv::rectangle draws a rectangle outline or a filled rectangle whose two opposite corners are pt1 and pt2.Examples: samples/cpp/camshiftdemo.cpp, samples/cpp/demhist.cpp, samples/cpp/facedetect.cpp, samples/cpp/falsecolor.cpp, samples/cpp/grabcut.cpp, samples/cpp/intersectExample.cpp, samples/cpp/peopledetect.cpp, samples/cpp/train_HOG.cpp, samples/cpp/tutorial_code/Histograms_Matching/MatchTemplate_Demo.cpp, samples/cpp/tutorial_code/ImgProc/basic_drawing/Drawing_1.cpp, samples/cpp/tutorial_code/ImgProc/basic_drawing/Drawing_2.cpp, samples/dnn/object_detection.cpp, and samples/tapi/hog.cpp.","例： samples/cpp/camshiftdemo.cpp, samples/cpp/demhist.cpp, samples/cpp/facedetect.cpp, samples/cpp/falecolor.cpp, samples/cpp/grabcut.cpp, samples/cpp/intersectExample.cpp, samples/cpp/peopledetect.cpp, samples/cpp/train_HOG.cpp, samples/cpp/tutorial_code/Histograms_Matching/MatchTemplate_Demo.cpp, samples/cpp/tutorial_code/ImgProc/basic_drawing/Drawing_1.cpp, samples/cpp/tutorial_code/ImgProc/basic_drawing/Drawing_2.cpp, samples/dnn/object_detection.cpp, and samples/tapi/hog.cpp."
Draws a circle.,円を描きます。
img : Image where the circle is drawn.,img : 円が描画される画像。
center : Center of the circle.,center : 円の中心を示します。
radius : Radius of the circle.,radius : 円の半径です。
color : Circle color.,color : 円の色を指定します。
"thickness : Thickness of the circle outline, if positive. Negative values, like FILLED, mean that a filled circle is to be drawn.",thickness : 円の輪郭の太さ（正の値の場合）。FILLEDのような負の値は、塗りつぶされた円が描画されることを意味します。
lineType : Type of the circle boundary. See LineTypes,lineType :円の境界線のタイプです。関連項目： LineTypes
shift : Number of fractional bits in the coordinates of the center and in the radius value.,shift : 中心の座標と半径の値に含まれる小数ビットの数．
"The function cv::circle draws a simple or filled circle with a given center and radius.Examples: samples/cpp/convexhull.cpp, samples/cpp/falsecolor.cpp, samples/cpp/kmeans.cpp, samples/cpp/lkdemo.cpp, samples/cpp/minarea.cpp, samples/cpp/tutorial_code/ImgProc/basic_drawing/Drawing_1.cpp, samples/cpp/tutorial_code/ImgProc/basic_drawing/Drawing_2.cpp, samples/cpp/tutorial_code/ImgTrans/houghcircles.cpp, and samples/dnn/openpose.cpp.","例： samples/cpp/convexhull.cpp, samples/cpp/falecolor.cpp, samples/cpp/kmeans.cpp, samples/cpp/lkdemo.cpp, samples/cpp/minarea.cpp, samples/cpp/tutorial_code/ImgProc/basic_drawing/Drawing_1.cpp, samples/cpp/tutorial_code/ImgProc/basic_drawing/Drawing_2.cpp, samples/cpp/tutorial_code/ImgTrans/houghcircles.cpp, and samples/dnn/openpose.cpp."
Draws a simple or thick elliptic arc or fills an ellipse sector.,単純または太い楕円の円弧を描いたり、楕円のセクタを埋めたりします。
center : Center of the ellipse.,center : 楕円の中心。
axes : Half of the size of the ellipse main axes.,axes : 楕円の主軸の大きさの半分。
angle : Ellipse rotation angle in degrees.,angle : 楕円の回転角度を度単位で指定します。
startAngle : Starting angle of the elliptic arc in degrees.,startAngle : 楕円弧の開始角度を度単位で指定します。
endAngle : Ending angle of the elliptic arc in degrees.,endAngle : 楕円の終了角度を度数で表したものです。
color : Ellipse color.,color : 楕円の色を指定します。
"thickness : Thickness of the ellipse arc outline, if positive. Otherwise, this indicates that a filled ellipse sector is to be drawn.",thickness : 楕円の輪郭の厚み（正の値の場合）。それ以外の場合は、塗りつぶした楕円のセクターを描くことを示す。
lineType : Type of the ellipse boundary. See LineTypes,lineType :楕円の境界線のタイプ。LineTypesを参照してください。
shift : Number of fractional bits in the coordinates of the center and values of axes.,shift : 中心の座標と軸の値に含まれる，小数ビットの数．
"The function cv::ellipse with more parameters draws an ellipse outline, a filled ellipse, an elliptic arc, or a filled ellipse sector. The drawing code uses general parametric form. A piecewise-linear curve is used to approximate the elliptic arc boundary. If you need more control of the ellipse rendering, you can retrieve the curve using ellipse2Poly and then render it with polylines or fill it with fillPoly. If you use the first variant of the function and want to draw the whole ellipse, not an arc, pass startAngle=0 and endAngle=360. If startAngle is greater than endAngle, they are swapped. The figure below explains the meaning of the parameters to draw the blue arc.ellipse.svg",関数 cv::ellipse は，より多くのパラメータを指定することで，楕円の輪郭，塗りつぶした楕円，楕円の円弧，塗りつぶした楕円のセクタを描画します．描画コードは，一般的なパラメトリック形式を利用しています．楕円弧の境界を近似するために、ピースウィズ線形曲線が使用されます。楕円の描画をより細かく制御する必要がある場合は、ellipse2Poly を使用して曲線を取得し、それをポリラインで描画したり、fillPoly で塗りつぶしたりすることができます。この関数の最初のバージョンを使用して、円弧ではなく楕円全体を描画したい場合は、startAngle=0 と endAngle=360 を渡します。startAngle が endAngle よりも大きい場合は、両者が入れ替わります。下図は、青い円弧.ellipse.svgを描くためのパラメータの意味を説明したものです。
"Parameters of Elliptic ArcExamples: fld_lines.cpp, samples/cpp/camshiftdemo.cpp, samples/cpp/contours2.cpp, samples/cpp/falsecolor.cpp, samples/cpp/fitellipse.cpp, samples/cpp/tutorial_code/ImgProc/basic_drawing/Drawing_1.cpp, and samples/cpp/tutorial_code/ImgProc/basic_drawing/Drawing_2.cpp.","楕円弧のパラメータExamples: fld_lines.cpp, samples/cpp/camshiftdemo.cpp, samples/cpp/contours2.cpp, samples/cpp/falecolor.cpp, samples/cpp/fitellipse.cpp, samples/cpp/tutorial_code/ImgProc/basic_drawing/Drawing_1.cpp, and samples/cpp/tutorial_code/ImgProc/basic_drawing/Drawing_2.cpp."
box : Alternative ellipse representation via RotatedRect. This means that the function draws an ellipse inscribed in the rotated rectangle.,box : RotatedRectによる楕円の代替表現。つまり，この関数は，回転した矩形に内接する楕円を描画します。
Draws a marker on a predefined position in an image.,画像内の定義済みの位置にマーカーを描画します。
position : The point where the crosshair is positioned.,position : 十字線の位置を指定します。
"markerType : The specific type of marker you want to use, see MarkerTypes",markerType :MarkerTypesを参照してください．
"line_type : Type of the line, See LineTypes",line_type :線の種類、参照：LineTypes
markerSize : The length of the marker axis [default = 20 pixels],markerSize : マーカー軸の長さ [デフォルト = 20 ピクセル]．
"The function cv::drawMarker draws a marker on a given position in the image. For the moment several marker types are supported, see MarkerTypes for more information.Examples: samples/cpp/polar_transforms.cpp.",関数 cv::drawMarker は，画像中の指定された位置にマーカーを描画します．現時点では，複数のマーカータイプがサポートされています．詳細は MarkerTypes を参照してください．例： samples/cpp/polar_transforms.cpp.
Fills a convex polygon.,凸型の多角形を塗りつぶします。
points : Polygon vertices.,points :ポリゴンの頂点
color : Polygon color.,color : ポリゴンの色
lineType : Type of the polygon boundaries. See LineTypes,lineType :ポリゴンの境界線の種類LineTypes参照
shift : Number of fractional bits in the vertex coordinates.,shift : 頂点座標に含まれる小数ビットの数．
"The function cv::fillConvexPoly draws a filled convex polygon. This function is much faster than the function fillPoly . It can fill not only convex polygons but any monotonic polygon without self-intersections, that is, a polygon whose contour intersects every horizontal line (scan line) twice at the most (though, its top-most and/or the bottom edge could be horizontal).",関数 cv::fillConvexPoly は，塗りつぶした凸多角形を描画します．この関数は，関数 fillPoly よりもはるかに高速です．これは，凸多角形だけでなく，自己交差のない単調な多角形，つまり，輪郭がすべての水平線（走査線）と最大で2回交差する多角形を塗りつぶすことができます（ただし，最上端や最下端が水平である可能性もあります）．
"Examples: samples/cpp/create_mask.cpp, samples/cpp/intersectExample.cpp, samples/cpp/tutorial_code/ImgProc/basic_drawing/Drawing_1.cpp, and samples/cpp/tutorial_code/ImgProc/basic_drawing/Drawing_2.cpp.",例：samples/cpp/create_mask.cpp、samples/cpp/intersectExample.cpp、samples/cpp/tutorial_code/ImgProc/basic_drawing/Drawing_1.cpp、samples/cpp/tutorial_code/ImgProc/basic_drawing/Drawing_2.cpp。
Fills the area bounded by one or more polygons.,1つまたは複数のポリゴンで囲まれた領域を塗りつぶします。
pts : Array of polygons where each polygon is represented as an array of points.,pts :多角形の配列で，各多角形は点の配列として表されます．
offset : Optional offset of all points of the contours.,offset : オプションである，輪郭の全点のオフセット．
"The function cv::fillPoly fills an area bounded by several polygonal contours. The function can fill complex areas, for example, areas with holes, contours with self-intersections (some of their parts), and so forth.",関数 cv::fillPoly は，複数の多角形の輪郭で囲まれた領域を塗りつぶします．この関数は，複雑な領域を塗りつぶすことができます．例えば，穴のある領域や，自己交差（部分的に）している輪郭などです．
"Examples: fld_lines.cpp, samples/cpp/create_mask.cpp, samples/cpp/intersectExample.cpp, samples/cpp/squares.cpp, samples/cpp/tutorial_code/ImgProc/basic_drawing/Drawing_2.cpp, samples/dnn/text_detection.cpp, and samples/tapi/squares.cpp.",例：fld_lines.cpp、samples/cpp/create_mask.cpp、samples/cpp/intersectExample.cpp、samples/cpp/squares.cpp、samples/cpp/tutorial_code/ImgProc/basic_drawing/Drawing_2.cpp、samples/dnn/text_detection.cpp、samples/tapi/squares.cpp。
Draws several polygonal curves.,いくつかの多角形の曲線を描画します。
pts : Array of polygonal curves.,pts :多角形の曲線の配列．
"isClosed : Flag indicating whether the drawn polylines are closed or not. If they are closed, the function draws a line from the last vertex of each curve to its first vertex.",isClosed : 描画されたポリラインが閉じているかどうかを示すフラグ。閉じている場合は、各曲線の最後の頂点からその最初の頂点まで線を引きます。
color : Polyline color.,color : ポリラインの色。
thickness : Thickness of the polyline edges.,thickness : ポリラインのエッジの太さを表します．
lineType : Type of the line segments. See LineTypes,lineType :線分の種類。LineTypes を参照してください．
The function cv::polylines draws one or more polygonal curves.,関数 cv::polylines は，1つあるいは複数の多角形の曲線を描画します．
Draws contours outlines or filled contours.,輪郭のアウトラインや塗りつぶした輪郭を描画します．
image : Destination image.,image : 出力画像．
contours : All the input contours. Each contour is stored as a point vector.,contours : すべての入力輪郭．各輪郭は，点状のベクトルとして格納されます．
"contourIdx : Parameter indicating a contour to draw. If it is negative, all the contours are drawn.",contourIdx : 描画する輪郭を示すパラメータ．負の値の場合は，すべての輪郭が描画されます．
color : Color of the contours.,color : 輪郭線の色。
"thickness : Thickness of lines the contours are drawn with. If it is negative (for example, thickness=FILLED ), the contour interiors are drawn.",thickness : 輪郭が描かれる線の太さ。負の値（例えば、thickness=FILLED ）の場合、輪郭の内部が描画されます。
lineType : Line connectivity. See LineTypes,lineType :線の接続性。LineTypes参照
hierarchy : Optional information about hierarchy. It is only needed if you want to draw only some of the contours (see maxLevel ).,hierarchy : 階層に関するオプション情報です。輪郭の一部だけを描画する場合にのみ必要です（maxLevel参照）。
"maxLevel : Maximal level for drawn contours. If it is 0, only the specified contour is drawn. If it is 1, the function draws the contour(s) and all the nested contours. If it is 2, the function draws the contours, all the nested contours, all the nested-to-nested contours, and so on. This parameter is only taken into account when there is hierarchy available.",maxLevel : 描画される輪郭の最大レベル。これが0の場合，指定された輪郭のみが描画されます．1 の場合は、輪郭と入れ子になっているすべての輪郭が描画されます。2の場合，輪郭，すべての入れ子になった輪郭，すべての入れ子から入れ子になった輪郭，...が描画されます。このパラメータは，利用可能な階層がある場合にのみ考慮されます．
"offset : Optional contour shift parameter. Shift all the drawn contours by the specified \(\texttt{offset}=(dx,dy)\) .","offset : オプションの輪郭シフトパラメータ．描画されたすべての輪郭を，指定された分だけシフトします（\{offset}=(dx,dy)\）．"
"The function draws contour outlines in the image if \(\texttt{thickness} \ge 0\) or fills the area bounded by the contours if \(\texttt{thickness}<0\) . The example below shows how to retrieve connected components from the binary image and label them: :#include ""opencv2/imgproc.hpp""#include ""opencv2/highgui.hpp""using namespace cv;using namespace std;int main( int argc, char** argv ){    Mat src;    // the first command-line parameter must be a filename of the binary    // (black-n-white) image    if( argc != 2 || !(src=imread(argv[1], 0)).data)        return -1;    Mat dst = Mat::zeros(src.rows, src.cols, CV_8UC3);    src = src > 1;    namedWindow( ""Source"", 1 );    imshow( ""Source"", src );    vector<vector<Point> > contours;    vector<Vec4i> hierarchy;    findContours( src, contours, hierarchy,        RETR_CCOMP, CHAIN_APPROX_SIMPLE );    // iterate through all the top-level contours,    // draw each connected component with its own random color    int idx = 0;    for( ; idx >= 0; idx = hierarchy[idx][0] )    {        Scalar color( rand()&255, rand()&255, rand()&255 );        drawContours( dst, contours, idx, color, FILLED, 8, hierarchy );    }    namedWindow( ""Components"", 1 );    imshow( ""Components"", dst );    waitKey(0);}fragmentNoteWhen thickness=FILLED, the function is designed to handle connected components with holes correctly even when no hierarchy date is provided. This is done by analyzing all the outlines together using even-odd rule. This may give incorrect results if you have a joint collection of separately retrieved contours. In order to solve this problem, you need to call drawContours separately for each sub-group of contours, or iterate over the collection using contourIdx parameter.Examples: samples/cpp/contours2.cpp, samples/cpp/segment_objects.cpp, samples/cpp/tutorial_code/ml/introduction_to_pca/introduction_to_pca.cpp, and samples/cpp/watershed.cpp.","この関数は，画像に輪郭を描きます（\\）．以下の例では，2値画像から連結成分を取り出し，ラベルを付ける方法を示しています： :#include ""opencv2/imgproc.hpp ""#include ""opencv2/highgui.hpp ""using namespace cv;using namespace std;int main( int argc, char** argv ){ Mat src; // 最初のコマンドラインパラメータは，バイナリ（白黒）画像のファイル名でなければいけません // if( argc != 2 || !(src=imread(argv[1], 0).data) return -1; Mat dst = Mat::zeros(src.rows, src.cols, CV_8UC3); src = src > 1; namedWindow( ""Source"", 1 ); imshow( ""Source"", src ); vector<vector<Point> > contours; vector<Vec4i> hierarchy;    findContours( src, contours, hierarchy, RETR_CCOMP, CHAIN_APPROX_SIMPLE ); // すべてのトップレベルの輪郭を繰り返し処理して、 // 接続された各コンポーネントをそれぞれのランダムな色で描画します int idx = 0; for( ; idx >= 0;idx = hierarchy[idx][0] ) { Scalar color( rand()&255, rand()&255, rand()&255 ); drawContours( dst, contours, idx, color, FILLED, 8, hierarchy ); } namedWindow( ""Components"", 1 );    imshow( ""Components"", dst ); waitKey(0);}fragmentNote thickness=FILLEDにすると、階層の日付が指定されていなくても、穴の開いた連結部品を正しく扱えるようになっています。これは、偶奇則を用いてすべてのアウトラインをまとめて解析することで実現しています。これは、別々に検索された輪郭の共同コレクションがある場合、誤った結果を与える可能性があります。この問題を解決するには、輪郭のサブグループごとに別々に drawContours を呼び出すか、contourIdx パラメータを使ってコレクションを反復処理する必要があります。例: samples/cpp/contours2.cpp, samples/cpp/segment_objects.cpp, samples/cpp/tutorial_code/ml/introduction_to_pca/introduction_to_pca.cpp, samples/cpp/watershed.cpp."
Clips the line against the image rectangle.,画像の矩形に対して線をクリップします。
"imgSize : Image size. The image rectangle is Rect(0, 0, imgSize.width, imgSize.height) .","imgSize : 画像サイズ．画像の矩形は Rect(0, 0, imgSize.width, imgSize.height) です．"
pt1 : First line point.,pt1 : 第一線のポイント。
pt2 : Second line point.,pt2 : 2本目の線の点．
"The function cv::clipLine calculates a part of the line segment that is entirely within the specified rectangle. it returns false if the line segment is completely outside the rectangle. Otherwise, it returns true .",関数 cv::clipLine は，線分の一部が指定された矩形内に収まるように計算します．線分が完全に矩形の外側にある場合は false を返します．そうでない場合は，真を返します．
Approximates an elliptic arc with a polyline.,楕円形の円弧をポリラインで近似します．
center : Center of the arc.,center : 円弧の中心。
axes : Half of the size of the ellipse main axes. See ellipse for details.,axes : 楕円の主軸の大きさの半分。詳しくは楕円を参照してください。
angle : Rotation angle of the ellipse in degrees. See ellipse for details.,angle : 楕円の回転角度を度単位で指定します。詳細は楕円を参照してください。
arcStart : Starting angle of the elliptic arc in degrees.,arcStart : 楕円弧の開始角度（度）。
arcEnd : Ending angle of the elliptic arc in degrees.,arcEnd : 楕円の終了角度を度単位で表したもの。
delta : Angle between the subsequent polyline vertices. It defines the approximation accuracy.,delta :後続のポリラインの頂点間の角度です。近似精度を定義します。
pts : Output vector of polyline vertices.,pts :ポリラインの頂点の出力ベクトル．
"The function ellipse2Poly computes the vertices of a polyline that approximates the specified elliptic arc. It is used by ellipse. If arcStart is greater than arcEnd, they are swapped.",関数 ellipse2Poly は，指定された楕円弧に近似するポリラインの頂点を計算します．この関数は，ellipse で使用されます．arcStart が arcEnd よりも大きい場合，それらは入れ替えられます。
Draws a text string.,テキスト文字列を描画します。
text : Text string to be drawn.,text : 描画される文字列です。
org : Bottom-left corner of the text string in the image.,org : 画像内の文字列の左下隅。
"fontFace : Font type, see HersheyFonts.",fontFace : フォントタイプ，HersheyFonts参照．
fontScale : Font scale factor that is multiplied by the font-specific base size.,fontScale : フォント固有のベースサイズに乗算されるフォントスケール係数。
color : Text color.,color : 文字色。
thickness : Thickness of the lines used to draw a text.,thickness : テキストの描画に使用される線の太さ。
lineType : Line type. See LineTypes,lineType :線の種類。関連項目： LineTypes
"bottomLeftOrigin : When true, the image data origin is at the bottom-left corner. Otherwise, it is at the top-left corner.",bottomLeftOrigin : trueの場合、画像データの原点が左下隅になります。そうでない場合は，左上隅になります．
"The function cv::putText renders the specified text string in the image. Symbols that cannot be rendered using the specified font are replaced by question marks. See getTextSize for a text rendering code example.Examples: samples/cpp/falsecolor.cpp, samples/cpp/fitellipse.cpp, samples/cpp/intersectExample.cpp, samples/cpp/peopledetect.cpp, samples/cpp/tutorial_code/ImgProc/basic_drawing/Drawing_2.cpp, samples/cpp/tutorial_code/ImgProc/Smoothing/Smoothing.cpp, samples/cpp/warpPerspective_demo.cpp, samples/dnn/classification.cpp, samples/dnn/object_detection.cpp, samples/dnn/segmentation.cpp, samples/dnn/text_detection.cpp, and samples/tapi/hog.cpp.","関数 cv::putText は，指定された文字列を画像内にレンダリングします．指定されたフォントでは描画できない記号は，クエスチョンマークに置き換えられます．テキストレンダリングのコード例は，getTextSize を参照してください．例： samples/cpp/falecolor.cpp, samples/cpp/fitellipse.cpp, samples/cpp/intersectExample.cpp, samples/cpp/peopledetect.cpp, samples/cpp/tutorial_code/ImgProc/basic_drawing/Drawing_2.cpp、samples/cpp/tutorial_code/ImgProc/Smoothing/Smoothing.cpp、samples/cpp/warpPerspective_demo.cpp、samples/dnn/classification.cpp、samples/dnn/object_detection.cpp、samples/dnn/segmentation.cpp、samples/dnn/text_detection.cpp、samples/tapi/hog.cppです。"
Calculates the width and height of a text string.,テキスト文字列の幅と高さを計算します。
text : Input text string.,text : 入力テキスト文字列．
"fontFace : Font to use, see HersheyFonts.",fontFace : 使用するフォント，HersheyFontsを参照．
thickness : Thickness of lines used to render the text. See putText for details.,thickness : テキストの描画に使われる線の太さ。詳細は putText を参照してください。
baseLine : [out],baseLine : [out].
"The function cv::getTextSize calculates and returns the size of a box that contains the specified text. That is, the following code renders some text, the tight box surrounding it, and the baseline: :String text = ""Funny text inside the box"";int fontFace = FONT_HERSHEY_SCRIPT_SIMPLEX;double fontScale = 2;int thickness = 3;Mat img(600, 800, CV_8UC3, Scalar::all(0));int baseline=0;Size textSize = getTextSize(text, fontFace,                            fontScale, thickness, &baseline);baseline += thickness;// center the textPoint textOrg((img.cols - textSize.width)/2,              (img.rows + textSize.height)/2);// draw the boxrectangle(img, textOrg + Point(0, baseline),          textOrg + Point(textSize.width, -textSize.height),          Scalar(0,0,255));// ... and the baseline firstline(img, textOrg + Point(0, thickness),     textOrg + Point(textSize.width, thickness),     Scalar(0, 0, 255));// then put the text itselfputText(img, text, textOrg, fontFace, fontScale,        Scalar::all(255), thickness, 8);fragmentSee alsoputTextExamples: samples/cpp/fitellipse.cpp, samples/cpp/tutorial_code/ImgProc/basic_drawing/Drawing_2.cpp, and samples/dnn/object_detection.cpp.","関数 cv::getTextSize は，指定されたテキストを含むボックスのサイズを計算して返します．つまり，次のコードは，いくつかのテキストと，それを囲むタイトなボックス，そしてベースラインをレンダリングします： :String text = ""Funny text inside the box"";int fontFace = FONT_HERSHEY_SCRIPT_SIMPLEX;double fontScale = 2;int thickness = 3;Mat img(600, 800, CV_8UC3, Scalar::all(0));int baseline=0;Size textSize = getTextSize(text, fontFace, fontScale, thickness, &baseline);baseline += thickness;// textPoint を中央に配置 textOrg((img.cols - textSize.width)/2, (img.rows + textSize.height)/2);// boxrectangle(img, textOrg + Point(0, baseline), textOrg + Point(textSize.width, -textSize.height), Scalar(0,0,255));// ...そしてベースライン firstline(img, textOrg + Point(0, thickness), textOrg + Point(textSize.width, thickness), Scalar(0, 0, 255));// そしてテキスト自体を配置putText(img, text, textOrg, fontFace, fontScale, Scalar::all(255), thickness, 8);fragmentSee alsoputTextExamples: samples/cpp/fitellipse.cpp, samples/cpp/tutorial_code/ImgProc/basic_drawing/Drawing_2.cpp, and samples/dnn/object_detection.cpp."
Calculates the font-specific size to use to achieve a given height in pixels.,与えられた高さをピクセル単位で実現するために使用するフォント固有のサイズを計算します。
"fontFace : Font to use, see cv::HersheyFonts.",fontFace : 使用するフォント， cv::HersheyFonts を参照してください．
pixelHeight : Pixel height to compute the fontScale for,pixelHeight : fontScaleを計算する際のピクセルの高さ．
thickness : Thickness of lines used to render the text.See putText for details.,thickness : テキストの描画に利用される線の太さ．
See alsocv::putText,詳しくは putText を参照してください．
Creates a smart pointer to a cv::CLAHE class and initializes it.,cv::CLAHE クラスへのスマートポインタを作成し，それを初期化します．
clipLimit : Threshold for contrast limiting.,clipLimit : コントラストを制限するための閾値．
tileGridSize : Size of grid for histogram equalization. Input image will be divided into equally sized rectangular tiles. tileGridSize defines the number of tiles in row and column.,tileGridSize : ヒストグラム・イコライゼーションのためのグリッドのサイズ．tileGridSize は，行と列のタイルの数を定義します．
Base class for Contrast Limited Adaptive Histogram Equalization. ,Contrast Limited Adaptive Histogram Equalization（コントラスト限定適応ヒストグラム等化）の基底クラス．
Equalizes the histogram of a grayscale image using Contrast Limited Adaptive Histogram Equalization.,Contrast Limited Adaptive Histogram Equalizationを用いて，グレースケール画像のヒストグラムを均等化します．
src : Source image of type CV_8UC1 or CV_16UC1.,src : CV_8UC1 または CV_16UC1 形式の入力画像．
dst : Destination image.,dst : 出力画像。
Sets threshold for contrast limiting.,Contrast Limiting の閾値を設定します。
clipLimit : threshold value.,clipLimit : 閾値．
Returns threshold value for contrast limiting.,コントラスト制限の閾値を返します．
Sets size of grid for histogram equalization. Input image will be divided into equally sized rectangular tiles.,ヒストグラム・イコライゼーションのためのグリッドのサイズを設定します．入力画像は，同じ大きさの長方形のタイルに分割されます．
tileGridSize : defines the number of tiles in row and column.,tileGridSize : 行と列のタイルの数を定義します。
Returns Size defines the number of tiles in row and column.,タイルグリッドサイズ ： 行と列のタイルの数を定義します。
set template to search,検索するテンプレートの設定
find template on image,画像からテンプレートを探す
Canny low threshold.,Canny low threshold.
Canny high threshold.,Canny high threshold.
Minimum distance between the centers of the detected objects.,検出されたオブジェクトの中心間の最小距離を指定します。
Inverse ratio of the accumulator resolution to the image resolution.,画像の解像度に対するアキュムレータの解像度の逆数
Maximal size of inner buffers.,内部バッファの最大サイズ．
Creates a smart pointer to a cv::GeneralizedHoughBallard class and initializes it.,cv::GeneralizedHoughBallard クラスへのスマートポインタを作成し，それを初期化します．
finds arbitrary template in the grayscale image using Generalized Hough Transform ,Generalized Hough Transform を用いて，グレースケール画像中の任意のテンプレートを見つけます．
Detects position only without translation and rotation [14] . ,並進と回転を行わずに，位置のみを検出します [14] ．
R-Table levels.,R-テーブルのレベル。
"The accumulator threshold for the template centers at the detection stage. The smaller it is, the more false positions may be detected.",検出段階でのテンプレートセンターのアキュムレータの閾値です．これが小さければ小さいほど，誤った位置が検出される可能性があります．
Creates a smart pointer to a cv::GeneralizedHoughGuil class and initializes it.,cv::GeneralizedHoughGuil クラスへのスマートポインタを作成し，それを初期化します．
"Detects position, translation and rotation [100] . ",位置，並進，回転を検出します [100] ．
Angle difference in degrees between two points in feature.,特徴量内の2点間の角度差（度）．
Feature table levels.,特徴表のレベル．
Maximal difference between angles that treated as equal.,等しく扱われる角度の最大の差．
Minimal rotation angle to detect in degrees.,回転を検出する最小角度（度）。
Maximal rotation angle to detect in degrees.,検出する回転角度の最大値（単位：度
Angle step in degrees.,角度ステップ(度)
Angle votes threshold.,角度票の閾値
Minimal scale to detect.,検出する最小のスケール
Maximal scale to detect.,検出する最大のスケール
Scale step.,スケールステップ
Scale votes threshold.,スケール票のしきい値
Position votes threshold.,位置のしきい値を指定します。
Line iterator. ,ラインイテレータ。
The class is used to iterate over all the pixels on the raster line segment connecting two specified points.,このクラスは、指定した 2 点を結ぶラスター線分上のすべてのピクセルを反復処理します。
"The class LineIterator is used to get each pixel of a raster line. It can be treated as versatile implementation of the Bresenham algorithm where you can stop at each pixel and do some extra processing, for example, grab pixel values along the line or draw a line with an effect (for example, with XOR operation).",LineIterator クラスは、 ラスターラインの各ピクセルを取得するために使用します。これは、Bresenham アルゴリズムの汎用的な実装として扱うことができます。各ピクセルで停止して追加の処理を行うことができます。たとえば、線に沿ってピクセル値を取得したり、 XOR 演算などで効果的に線を描画したりすることができます。
The number of pixels along the line is stored in LineIterator::count. The method LineIterator::pos returns the current position in the image:,線に沿ったピクセルの数が LineIterator::count に格納されます。メソッド LineIterator::pos は、画像内の現在の位置を返します。
"// grabs pixels along the line (pt1, pt2)// from 8-bit 3-channel image to the bufferLineIterator it(img, pt1, pt2, 8);LineIterator it2 = it;vector<Vec3b> buf(it.count);for(int i = 0; i < it.count; i++, ++it)    buf[i] = *(const Vec3b*)*it;// alternative way of iterating through the linefor(int i = 0; i < it2.count; i++, ++it2){    Vec3b val = img.at<Vec3b>(it2.pos());    CV_Assert(buf[i] == val);} ","// ラインに沿ったピクセルをつかむ (pt1, pt2)// 8 ビット 3 チャンネル画像からバッファへLineIterator it(img, pt1, pt2, 8);LineIterator it2 = it;vector<Vec3b> buf(it.count);for(int i = 0; i < it.count; i++, ++it) buf[i] = *(const Vec3b*)*it;// 別の方法で，ラインを繰り返し処理するfor(int i = 0; i < it2.count; i++, ++it2){ Vec3b val = img.at<Vec3b>(it2.pos()); CV_Assert(buf[i] == val);}．"
Intelligent Scissors image segmentation. ,Intelligent Scissorsによる画像分割．
This class is used to find the path (contour) between two points which can be used for image segmentation.,このクラスは，画像のセグメンテーションに利用できる2点間のパス（輪郭）を求めるために利用されます．
"Usage example:     segmentation::IntelligentScissorsMB tool;    tool.setEdgeFeatureCannyParameters(16, 100)  // using Canny() as edge feature extractor        .setGradientMagnitudeMaxLimit(200);    // calculate image features    tool.applyImage(image);    // calculate map for specified source point    Point source_point(200, 100);    tool.buildMap(source_point);    // fast fetching of contours    // for specified target point and the pre-calculated map (stored internally)    Point target_point(400, 300);    std::vector<Point> pts;    tool.getContour(target_point, pts); Reference: ""Intelligent Scissors for Image Composition"" algorithm designed by Eric N. Mortensen and William A. Barrett, Brigham Young University [178] ","使用例： segmentation::IntelligentScissorsMB tool; tool.setEdgeFeatureCannyParameters(16, 100) // エッジ特徴抽出器としてCanny()を使用 .setGradientMagnitudeMaxLimit(200); // 画像特徴を計算 tool.applyImage(image); // 指定されたソースポイントのマップを計算 Point source_point(200, 100); tool.buildMap(source_point); // 指定されたターゲットポイントと事前に計算されたマップ（内部に保存されている）の輪郭を // 高速に取得する Point target_point(400, 300); std::vector<Point> pts; tool.getContour(target_point, pts); 参考にしてください。Eric N. MortensenとWilliam A. Barrettが設計したアルゴリズム「Intelligent Scissors for Image Composition」（Brigham Young University）[178]。"
Specify weights of feature functions.,特徴関数の重みを指定する。
weight_non_edge : Specify cost of non-edge pixels (default: 0.43f),weight_non_edge : エッジではないピクセルのコストを指定 (デフォルト: 0.43f)
weight_gradient_direction : Specify cost of gradient direction function (default: 0.43f),weight_gradient_direction : グラデーション方向の関数のコストを指定する (デフォルト: 0.43f)
weight_gradient_magnitude : Specify cost of gradient magnitude function (default: 0.14f),weight_gradient_magnitude :グラデーションの大きさを表す関数のコストを指定する (デフォルト: 0.14f)
Consider keeping weights normalized (sum of weights equals to 1.0) Discrete dynamic programming (DP) goal is minimization of costs between pixels.,重みを正規化しておくことを考慮する（重みの合計が1.0になるように） 離散的な動的計画法（DP）の目標は、ピクセル間のコストを最小化することです。
Specify gradient magnitude max value threshold.,グラディエントマグニチュードの最大値の閾値を指定します。
"gradient_magnitude_threshold_max : Specify gradient magnitude max value threshold (default: 0, disabled)","gradient_magnitude_threshold_max : グラディエントマグニチュードの最大値のしきい値を指定 (デフォルト: 0, 無効)"
"Zero limit value is used to disable gradient magnitude thresholding (default behavior, as described in original article). Otherwize pixels with gradient magnitude >= threshold have zero cost.NoteThresholding should be used for images with irregular regions (to avoid stuck on parameters from high-contract areas, like embedded logos).",0の制限値は、グラデーションの大きさのしきい値を無効にするために使用されます（元の記事に記載されているように、デフォルトの動作）。NoteThresholdingは、不規則な領域を持つ画像に使用する必要があります（埋め込まれたロゴのような、高契約領域からのパラメータに固執するのを避けるため）。
"Switch to ""Laplacian Zero-Crossing"" edge feature extractor and specify its parameters.","Laplacian Zero-Crossing ""エッジ特徴抽出器に切り替え、そのパラメータを指定します。"
"gradient_magnitude_min_value : Minimal gradient magnitude value for edge pixels (default: 0, check is disabled)",gradient_magnitude_min_value : エッジピクセルのグラデーションの大きさの最小値（デフォルト：0、チェックは無効）。
"This feature extractor is used by default according to article.Implementation has additional filtering for regions with low-amplitude noise. This filtering is enabled through parameter of minimal gradient amplitude (use some small value 4, 8, 16).NoteCurrent implementation of this feature extractor is based on processing of grayscale images (color image is converted to grayscale image first).",この特徴抽出器は、記事によるとデフォルトで使用されています。実装では、低振幅のノイズがある領域に対する追加のフィルタリングがあります。このフィルタリングは、最小グラデーション振幅のパラメータで有効になります（いくつかの小さな値4、8、16を使用します）。注意この特徴抽出器の現在の実装は、グレースケール画像の処理に基づいています（カラー画像はまずグレースケール画像に変換されます）。
"Canny edge detector is a bit slower, but provides better results (especially on color images): use setEdgeFeatureCannyParameters().",Cannyエッジ検出器は，少し遅いですが，より良い結果が得られます（特にカラー画像の場合）： setEdgeFeatureCannyParameters()を利用してください．
Switch edge feature extractor to use Canny edge detector.,エッジ特徴抽出器を，Cannyエッジ検出器を使うように切り替えます．
"Note""Laplacian Zero-Crossing"" feature extractor is used by default (following to original article)See alsoCanny","注意：デフォルトでは、""Laplacian Zero-Crossing ""特徴抽出器が使用されます（元の記事を参照してください）。"
Specify input image and extract image features.,入力画像を指定して、画像特徴を抽出します。
image : input image. Type is CV_8UC1 / CV_8UC3,image : 入力画像．種類は，CV_8UC1 / CV_8UC3です．
Specify custom features of imput image.,入力画像のカスタム特徴を指定します．
"non_edge : Specify cost of non-edge pixels. Type is CV_8UC1. Expected values are {0, 1}.","non_edge : エッジではないピクセルのコストを指定します．タイプは CV_8UC1 です．期待される値は {0, 1} です．"
gradient_direction : Specify gradient direction feature. Type is CV_32FC2. Values are expected to be normalized: x^2 + y^2 == 1,gradient_direction : グラデーション方向の特徴を指定します．タイプは CV_32FC2 です．期待される値は，正規化されたものです： x^2 + y^2 == 1．
"gradient_magnitude : Specify cost of gradient magnitude function: Type is CV_32FC1. Values should be in range [0, 1].","gradient_magnitude :グラディエントマグニチュード関数のコストを指定します．型は CV_32FC1．値は，[0, 1]の範囲でなければいけません．"
image : Optional parameter. Must be specified if subset of features is specified (non-specified features are calculated internally),image : オプションのパラメータ．特徴のサブセットが指定されている場合には，必ず指定しなければいけません（指定されていない特徴は，内部で計算されます）．
Customized advanced variant of applyImage() call.,applyImage() の呼び出しを，より高度なものにカスタマイズしたもの．
Prepares a map of optimal paths for the given source point on the image.,image上の与えられたソースポイントに対する最適経路のマップを作成します．
sourcePt : The source point used to find the paths,sourcePt : パスの探索に使用されるソースポイント
NoteapplyImage() / applyImageFeatures() must be called before this call,注意：applyImage() / applyImageFeatures()は、このコールの前にコールする必要があります。
Extracts optimal contour for the given target point on the image.,画像上の指定されたターゲット点に対する最適な輪郭を抽出します．
targetPt : The target point,targetPt : ターゲット点
contour : [out],輪郭 :[out].
"backward : Flag to indicate reverse order of retrived pixels (use ""true"" value to fetch points from the target to the source point)","backward : 抽出されたピクセルの順序が逆であることを示すフラグ（""true ""を指定すると，対象点から元点に向かって抽出される）"
NotebuildMap() must be called before this call,この呼び出しの前にNotebuildMap()を呼び出す必要があります。
Creates a new empty Delaunay subdivision.,新しい空のドロネー細分割を作成する．
rect : Rectangle that includes all of the 2D points that are to be added to the subdivision.,rect :細分化された部分に追加されるすべての2Dポイントを含む矩形。
Insert a single point into a Delaunay triangulation.,Delaunay Triangulationに1つの点を挿入します．
pt : Point to insert.,pt :挿入する点．
"The function inserts a single point into a subdivision and modifies the subdivision topology appropriately. If a point with the same coordinates exists already, no new point is added.NoteIf the point is outside of the triangulation specified rect a runtime error is raised.",この関数は，細分割に1つの点を挿入し，細分割のトポロジーを適切に変更します．同じ座標の点が既に存在する場合は、新しい点は追加されません。注意点点点が指定された三角測量の範囲外にある場合は、ランタイムエラーが発生します。
Returns the location of a point within a Delaunay triangulation.,ドロネー三角錐内の点の位置を返します。
pt : Point to locate.,pt :位置を決める点．
edge : Output edge that the point belongs to or is located to the right of it.,edge : その点が属する，あるいはその右に位置する出力辺．
vertex : Optional output vertex the input point coincides with.,vertex : 入力点が一致する出力頂点（オプション）．
The function locates the input point within the subdivision and gives one of the triangle edges or vertices.,この関数は，入力点を細分化された領域内に配置し，三角形の辺または頂点のいずれかを与えます．
Finds the subdivision vertex closest to the given point.,与えられた点に最も近い細分化された頂点を見つけます．
pt : Input point.,pt : 入力点．
nearestPt : Output subdivision vertex point.,nearestPt : 出力される細分割頂点の点．
"The function is another function that locates the input point within the subdivision. It finds the subdivision vertex that is the closest to the input point. It is not necessarily one of vertices of the facet containing the input point, though the facet (located using locate() ) is used as a starting point.",この関数は，入力点を細分割内に配置する別の関数です．入力点に最も近い細分化された頂点を見つけます．入力点を含むファセットの頂点とは限りませんが，そのファセット（locate()を用いて配置されたもの）が出発点として利用されます．
Returns a list of all edges.,すべてのエッジのリストを返します。
edgeList : Output vector.,edgeList :出力ベクトル．
"The function gives each edge as a 4 numbers vector, where each two are one of the edge vertices. i.e. org_x = v[0], org_y = v[1], dst_x = v[2], dst_y = v[3].","org_x = v[0], org_y = v[1], dst_x = v[2], dst_y = v[3]のように、各エッジを4つの数字のベクトルで出力する。"
Returns a list of the leading edge ID connected to each triangle.,各三角形に接続されたリーディングエッジIDのリストを返す。
leadingEdgeList : Output vector.,leadingEdgeList :出力ベクトル．
The function gives one edge ID for each triangle.,この関数は，各三角形に対して1つのエッジIDを与えます．
Returns a list of all triangles.,全てのトライアングルのリストを返します。
triangleList : Output vector.,triangleList :出力ベクトル．
"The function gives each triangle as a 6 numbers vector, where each two are one of the triangle vertices. i.e. p1_x = v[0], p1_y = v[1], p2_x = v[2], p2_y = v[3], p3_x = v[4], p3_y = v[5].",この関数は、各三角形を6つの数字のベクトルとして出力します。ここで、各2つは三角形の頂点の1つを表します。
Returns a list of all Voronoi facets.,すべてのボロノイファセットのリストを返します。
idx : Vector of vertices IDs to consider. For all vertices you can pass empty vector.,idx : 考慮する頂点のIDのベクトル。すべての頂点に対しては、空のベクトルを渡すことができます。
facetList : Output vector of the Voronoi facets.,facetList :ボロノイ面の出力ベクトル。
facetCenters : Output vector of the Voronoi facets center points.,facetCenters : ボロノイ面の中心点の出力ベクトル。
Returns vertex location from vertex ID.,vertex IDから頂点の位置を返します。
vertex : vertex ID.,vertex : 頂点ID.
firstEdge : Optional. The first edge ID which is connected to the vertex.,firstEdge : オプション。頂点に接続されている最初のエッジID。
Returns next edge around the edge origin.,edge originを中心とした次の辺を返します。
edge : Subdivision edge ID.,edge : 細分化されたエッジID．
Returns another edge of the same quad-edge.,同じquad-edgeの別の辺を返します。
rotate : Parameter specifying which of the edges of the same quad-edge as the input one to return. The following values are possible:,rotate :入力辺と同じquad-edgeのどの辺を返すかを指定するパラメータ．以下のような値が考えられます．
0 - the input edge ( e on the picture below if e is the input edge),0 - 入力辺 ( e が入力辺の場合，下の図の e )
1 - the rotated edge ( eRot ),1 - 回転した辺 ( eRot )
2 - the reversed edge (reversed e (in green)),2 - 逆転された辺（逆転されたe（緑））。
3 - the reversed rotated edge (reversed eRot (in green)),3 - 回転したエッジを反転させたもの (eRot を反転させたもの(緑))
Returns the edge origin.,辺の原点を返します。
orgpt : Output vertex location.,orgpt :出力される頂点の位置．
Returns the edge destination.,辺のデスティネーションを返します．
dstpt : Output vertex location.,dstpt :頂点の位置を出力する．
Sets training method and common parameters.,学習方法と共通のパラメータを設定します．
method : Default value is ANN_MLP::RPROP. See ANN_MLP::TrainingMethods.,method : デフォルト値はANN_MLP::RPROP。ANN_MLP::TrainingMethodsを参照してください．
param1 : passed to setRpropDW0 for ANN_MLP::RPROP and to setBackpropWeightScale for ANN_MLP::BACKPROP and to initialT for ANN_MLP::ANNEAL.,param1 : ANN_MLP::RPROPのsetRpropDW0、ANN_MLP::BACKPROPのsetBackpropWeightScale、ANN_MLP::ANNEALのinitialTに渡されます。
param2 : passed to setRpropDWMin for ANN_MLP::RPROP and to setBackpropMomentumScale for ANN_MLP::BACKPROP and to finalT for ANN_MLP::ANNEAL.,param2 : ANN_MLP::RPROPではsetRpropDWMinに、ANN_MLP::BACKPROPではsetBackpropMomentumScaleに、ANN_MLP::ANNEALではfinalTに渡される。
Returns current training method,現在の学習方法を返す
Initialize the activation function for each neuron. Currently the default and the only fully supported activation function is ANN_MLP::SIGMOID_SYM.,各ニューロンの活性化関数を初期化します。現在、デフォルトであり、唯一完全にサポートされている活性化関数はANN_MLP::SIGMOID_SYMです。
type : The type of activation function. See ANN_MLP::ActivationFunctions.,type :活性化関数の種類を指定します。ANN_MLP::ActivationFunctions を参照してください。
"param1 : The first parameter of the activation function, \(\alpha\). Default value is 0.",param1 : 活性化関数の最初のパラメータ。デフォルト値は0です．
"param2 : The second parameter of the activation function, \(\beta\). Default value is 0.",param2 ： 活性化関数の2番目のパラメータです。初期値は0です。
Integer vector specifying the number of neurons in each layer including the input and output layers. The very first element specifies the number of elements in the input layer. The last element - number of elements in the output layer. Default value is empty Mat.,入力層、出力層を含む各層のニューロンの数を表す整数ベクトルです。一番最初の要素は、入力層の要素数を表します。最後の要素は，出力層の要素数です．デフォルト値は，空の Mat です．
See alsogetLayerSizes,関連項目： getLayerSizes
"Termination criteria of the training algorithm. You can specify the maximum number of iterations (maxCount) and/or how much the error could change between the iterations to make the algorithm continue (epsilon). Default value is TermCriteria(TermCriteria::MAX_ITER + TermCriteria::EPS, 1000, 0.01).","学習アルゴリズムの終了基準．反復回数の最大値（maxCount）や，アルゴリズムを継続させるための反復間の誤差の変化量（epsilon）を指定できます．デフォルト値はTermCriteria(TermCriteria::MAX_ITER + TermCriteria::EPS, 1000, 0.01)です。"
See alsosetTermCriteria,alsosetTermCriteriaを参照
See alsogetTermCriteria,関連項目： 用語集の設定
BPROP: Strength of the weight gradient term. The recommended value is about 0.1. Default value is 0.1.,BPROP: 重み付けの項の強さ。推奨値は0.1程度。デフォルト値は0.1です。
See alsosetBackpropWeightScale,alsosetBackpropWeightScaleを参照。
See alsogetBackpropWeightScale,参照：「バックプロップウェイトスケールの設定
BPROP: Strength of the momentum term (the difference between weights on the 2 previous iterations). This parameter provides some inertia to smooth the random fluctuations of the weights. It can vary from 0 (the feature is disabled) to 1 and beyond. The value 0.1 or so is good enough. Default value is 0.1.,BPROP：モメンタム項の強さ（前の2回の反復での重みの差）。このパラメータは、重みのランダムな変動をスムーズにするための慣性を与えます。0（この機能は無効）から1、そしてそれ以上まで変化します。0.1程度の値であれば十分です。デフォルト値は0.1です。
See alsosetBackpropMomentumScale,関連項目： alsosetBackpropMomentumScale
See alsogetBackpropMomentumScale,alsogetBackpropMomentumScaleを参照。
RPROP: Initial value \(\Delta_0\) of update-values \(\Delta_{ij}\). Default value is 0.1.,RPROP：更新値の初期値\(Delta_{ij}\)です。初期値は0.1である。
See alsosetRpropDW0,alsosetRpropDW0参照。
See alsogetRpropDW0,alsogetRpropDW0を参照。
RPROP: Increase factor \(\eta^+\). It must be >1. Default value is 1.2.,RPROP: Increase factor ˶ˆ꒳ˆ˵ )1以上でなければなりません。初期値は1.2です。
See alsosetRpropDWPlus,alsosetRpropDWPlusを参照。
See alsogetRpropDWPlus,See alsogetRpropDWPlus
RPROP: Decrease factor \(\eta^-\). It must be <1. Default value is 0.5.,RPROPDecrease factor ˶ˆ꒳ˆ˵ )<1でなければならない。初期値は0.5です。
See alsosetRpropDWMinus,alsosetRpropDWMinus参照。
See alsogetRpropDWMinus,See alsogetRpropDWMinus
RPROP: Update-values upper limit \(\Delta_{max}\). It must be >1. Default value is 50.,RPROP：Update-values upper limit ˶‾᷅˵‾᷅˵。1以上でなければなりません。初期値は50です。
See alsosetRpropDWMax,alsosetRpropDWMax参照。
See alsogetRpropDWMax,See alsogetRpropDWMax
Creates empty model.,空のモデルを作成します。
"Use StatModel::train to train the model, Algorithm::load<ANN_MLP>(filename) to load the pre-trained model. Note that the train method has optional flags: ANN_MLP::TrainFlags.",モデルをトレーニングするには StatModel::train を、事前にトレーニングされたモデルをロードするには Algorithm::load<ANN_MLP>(filename) を使用してください。trainメソッドにはオプションのフラグがあることに注意してください。ANN_MLP::TrainFlagsです。
Artificial Neural Networks - Multi-Layer Perceptrons. ,人工ニューラルネットワーク - 多層パーセプトロン（Multi-Layer Perceptrons）。
"Unlike many other models in ML that are constructed and trained at once, in the MLP model these steps are separated. First, a network with the specified topology is created using the non-default constructor or the method ANN_MLP::create. All the weights are set to zeros. Then, the network is trained using a set of input and output vectors. The training procedure can be repeated more than once, that is, the weights can be adjusted based on the new training data.",MLの他の多くのモデルが構築と学習を一度に行うのに対し，MLPモデルではこれらのステップが分離されています．まず，デフォルトではないコンストラクタやANN_MLP::createメソッドを使って，指定されたトポロジーのネットワークを作成します．重みはすべてゼロに設定されています。次に，入力ベクトルと出力ベクトルを用いてネットワークを学習します．この学習手順は複数回繰り返すことができ、つまり、新しい学習データに基づいて重みを調整することができます。
Additional flags for StatModel::train are available: ANN_MLP::TrainFlags.,StatModel::train には追加のフラグが用意されています。ANN_MLP::TrainFlagsを参照してください。
See alsoNeural Networks  ,ニューラルネットワーク もご覧ください。
Loads and creates a serialized ANN from a file.,ファイルからシリアル化された ANN を読み込み、作成します。
filepath : path to serialized ANN,filepath : シリアル化されたANNのパス
"Use ANN::save to serialize and store an ANN to disk. Load the ANN from this file again, by calling this function with the path to the file.",ANNをシリアル化してディスクに保存するにはANN::saveを使います。ファイルへのパスを指定してこの関数を呼び出すことで、このファイルから再びANNをロードする。
Type of the boosting algorithm. See Boost::Types. Default value is Boost::REAL.,boostingアルゴリズムのタイプ。Boost::Typesを参照してください。デフォルト値は Boost::REAL です。
See alsosetBoostType,alsosetBoostType を参照してください。
See alsogetBoostType,関連項目： Boost::BoostType。
The number of weak classifiers. Default value is 100.,弱い分類器の数。デフォルト値は100です。
See alsosetWeakCount,alsosetWeakCountを参照してください。
See alsogetWeakCount,alsogetWeakCountを参照してください。
A threshold between 0 and 1 used to save computational time. Samples with summary weight \(\leq 1 - weight_trim_rate\) do not participate in the next iteration of training. Set this parameter to 0 to turn off this functionality. Default value is 0.95.,計算時間を短縮するために使われる0と1の間のしきい値です。summary weight\(1 - weight_trim_rate\)を持つサンプルは、次の反復トレーニングに参加しない。この機能をオフにするには、このパラメータを0に設定します。デフォルト値は0.95です。
See alsosetWeightTrimRate,関連項目： alsosetWeightTrimRate
See alsogetWeightTrimRate,alsogetWeightTrimRateを参照。
"Creates the empty model. Use StatModel::train to train the model, Algorithm::load<Boost>(filename) to load the pre-trained model.",空のモデルを作成します。モデルの学習には StatModel::train を、学習済みのモデルの読み込みには Algorithm::load<Boost>(filename) を使用してください。
Boosted tree classifier derived from DTrees. ,DTrees から派生したブーステッドツリー分類器です。
See alsoBoosting  ,ブースティングの項も参照してください。
"The number of mixture components in the Gaussian mixture model. Default value of the parameter is EM::DEFAULT_NCLUSTERS=5. Some of EM implementation could determine the optimal number of mixtures within a specified value range, but that is not the case in ML yet.",ガウス混合モデルにおける混合成分の数を指定します。このパラメータのデフォルト値はEM::DEFAULT_NCLUSTERS=5です。EMの実装の中には，指定された値の範囲内で最適な混合成分の数を決定できるものもありますが，MLではまだそのようにはなっていません．
See alsosetClustersNumber,関連項目： alsosetClustersNumber
See alsogetClustersNumber,alsogetClustersNumberを参照してください。
Constraint on covariance matrices which defines type of matrices. See EM::Types.,共分散行列に対する制約で、行列の種類を定義します。EM::Typesを参照してください。
See alsosetCovarianceMatrixType,alsosetCovarianceMatrixTypeを参照してください。
See alsogetCovarianceMatrixType,共分散マトリクスタイプを参照してください。
The termination criteria of the EM algorithm. The EM algorithm can be terminated by the number of iterations termCrit.maxCount (number of M-steps) or when relative change of likelihood logarithm is less than termCrit.epsilon. Default maximum number of iterations is EM::DEFAULT_MAX_ITERS=100.,EMアルゴリズムの終了基準。EMアルゴリズムは、反復回数 termCrit.maxCount（Mステップの数）、または尤度対数の相対的な変化が termCrit.epsilonより小さいときに終了させることができます。デフォルトの最大反復数はEM::DEFAULT_MAX_ITERS=100です。
Returns weights of the mixtures.,混合物の重みを返します。
Returns vector with the number of elements equal to the number of mixtures.,混合物の数と同じ数の要素を持つベクトルを返します。
Returns the cluster centers (means of the Gaussian mixture),クラスタ中心（ガウス混合の平均値）を返します。
Returns matrix with the number of rows equal to the number of mixtures and number of columns equal to the space dimensionality.,行数が混合の数に，列数が空間の次元に等しい行列を返します。
Returns covariation matrices.,共分散行列を返します。
"Returns vector of covariation matrices. Number of matrices is the number of gaussian mixtures, each matrix is a square floating-point matrix NxN, where N is the space dimensionality.",共分散行列のベクトルを返します。行列の数はガウス混合の数で、各行列は浮動小数点の正方行列NxN（Nは空間の次元）です。
Returns a likelihood logarithm value and an index of the most probable mixture component for the given sample.,与えられたサンプルに対して，尤度対数の値と，最も確率の高い混合成分のインデックスを返します。
sample : A sample for classification. It should be a one-channel matrix of \(1 \times dims\) or \(dims \times 1\) size.,sample : 分類のためのサンプル．1チャンネルの行列で，サイズは ˶ˆ꒳ˆ˵ / ˶ˆ꒳ˆ˵ です．
probs : Optional output matrix that contains posterior probabilities of each component given the sample. It has \(1 \times nclusters\) size and CV_64FC1 type.,probs : オプションの出力行列で，サンプルが与えられたときの各成分の事後確率を含みます．サイズは ˶ˆ꒳ˆ˵ ) 型は CV_64FC1 です．
The method returns a two-element double vector. Zero element is a likelihood logarithm value for the sample. First element is an index of the most probable mixture component for the given sample.,このメソッドは，2要素のダブルベクトルを返します．0番目の要素は，サンプルに対する尤度対数の値です．1番目の要素は，与えられたサンプルに対して最も確率の高い混合成分のインデックスです．
Estimate the Gaussian mixture parameters from a samples set.,サンプルセットからガウス混合パラメータを推定します。
"samples : Samples from which the Gaussian mixture model will be estimated. It should be a one-channel matrix, each row of which is a sample. If the matrix does not have CV_64F type it will be converted to the inner matrix of such type for the further computing.",samples :ガウス混合モデルが推定されるサンプル．これは1チャンネルの行列で，各行がサンプルである必要があります．この行列が CV_64F 型ではない場合，以降の計算のために，CV_64F 型の内部行列に変換されます．
logLikelihoods : The optional output matrix that contains a likelihood logarithm value for each sample. It has \(nsamples \times 1\) size and CV_64FC1 type.,logLikelihoods : オプションの出力行列で，各サンプルに対する尤度対数値を含みます．CV_64FC1 型で，サイズは ¶ nsamples ¶ times 1 ¶ です．
"labels : The optional output ""class label"" for each sample: \(\texttt{labels}_i=\texttt{arg max}_k(p_{i,k}), i=1..N\) (indices of the most probable mixture component for each sample). It has \(nsamples \times 1\) size and CV_32SC1 type.",labels : 各サンプルの「クラスラベル」が出力されます。\Ms.(各サンプルで最も確率の高い混合成分のインデックス)サイズは ¶ nsamples ¶ times 1 ¶ で，型は CV_32SC1 です．
probs : The optional output matrix that contains posterior probabilities of each Gaussian mixture component given the each sample. It has \(nsamples \times nclusters\) size and CV_64FC1 type.,probs : 各サンプルに対する各ガウス混合成分の事後確率を表すオプションの出力行列です．サイズは nsamples times nclusters\，型は CV_64FC1 です．
"This variation starts with Expectation step. Initial values of the model parameters will be estimated by the k-means algorithm.Unlike many of the ML models, EM is an unsupervised learning algorithm and it does not take responses (class labels or function values) as input. Instead, it computes the Maximum Likelihood Estimate of the Gaussian mixture parameters from an input sample set, stores all the parameters inside the structure: \(p_{i,k}\) in probs, \(a_k\) in means , \(S_k\) in covs[k], \(\pi_k\) in weights , and optionally computes the output ""class label"" for each sample: \(\texttt{labels}_i=\texttt{arg max}_k(p_{i,k}), i=1..N\) (indices of the most probable mixture component for each sample).The trained model can be used further for prediction, just like any other classifier. The trained model is similar to the NormalBayesClassifier.",このバリエーションは，Expectationステップから始まります．多くのMLモデルとは異なり，EMは教師なし学習アルゴリズムであり，入力として応答（クラスラベルや関数値）を受け取りません．多くのMLモデルとは異なり，EMは教師なし学習アルゴリズムであり，入力としてレスポンス（クラスラベルや関数値）を受け取りません．代わりに，入力サンプルセットからガウス混合パラメータの最尤推定値を計算し，すべてのパラメータを構造体の内部に格納します．\そして、オプションとして、各サンプルの「クラスラベル」を計算して出力します。\クラスラベルを計算します。(訓練されたモデルは，他の分類器と同様に，予測のためにさらに使用することができます．学習されたモデルは，NormalBayesClassifierと同様です．
probs0 : the probabilities,probs0 : 確率
"This variation starts with Maximization step. You need to provide initial probabilities \(p_{i,k}\) to use this option.","このバリエーションは，最大化ステップから始まります．このオプションを利用するには，初期確率\(p_{i,k}\)を用意する必要があります．"
"Creates empty EM model. The model should be trained then using StatModel::train(traindata, flags) method. Alternatively, you can use one of the EM::train* methods or load it from file using Algorithm::load<EM>(filename).","空のEMモデルを作成します．StatModel::train(traindata, flags)メソッドを用いてモデルを学習する必要があります。または、EM::train*メソッドを使用するか、Algorithm::load<EM>(filename)を用いてファイルからロードすることもできます。"
The class implements the Expectation Maximization algorithm. ,このクラスは、期待値最大化アルゴリズムを実装しています。
See alsoExpectation Maximization  ,関連項目： 期待値最大化法（Expectation Maximization
Learning rate.,学習率。
See alsosetLearningRate,See alsosetLearningRate
See alsogetLearningRate,alsogetLearningRate を参照してください。
Number of iterations.,イテレーションの数。
See alsosetIterations,alsosetIterationsを参照。
See alsogetIterations,alsogetIterationsを参照。
Kind of regularization to be applied. See LogisticRegression::RegKinds.,適用される正則化の種類。LogisticRegression::RegKindsを参照。
See alsosetRegularization,alsosetRegularizationを参照。
See alsogetRegularization,alsogetRegularizationを参照。
Kind of training method used. See LogisticRegression::Methods.,使用される学習方法の種類。LogisticRegression::Methodsを参照。
See alsosetTrainMethod,alsosetTrainMethodを参照。
See alsogetTrainMethod,alsogetTrainMethodを参照。
Specifies the number of training samples taken in each step of Mini-Batch Gradient Descent. Will only be used if using LogisticRegression::MINI_BATCH training algorithm. It has to take values less than the total number of training samples.,Mini-Batch Gradient Descentの各ステップで取るトレーニングサンプルの数を指定します。LogisticRegression::MINI_BATCH学習アルゴリズムを使用する場合にのみ使用されます。トレーニングサンプルの総数よりも少ない値を取らなければならない。
See alsosetMiniBatchSize,See alsosetMiniBatchSize
See alsogetMiniBatchSize,alsogetMiniBatchSizeを参照。
Termination criteria of the algorithm.,アルゴリズムの終了基準。
Predicts responses for input samples and returns a float type.,入力されたサンプルに対する応答を予測し、float 型で返す。
"samples : The input data for the prediction algorithm. Matrix [m x n], where each row contains variables (features) of one object being classified. Should have data type CV_32F.",samples :予測アルゴリズムの入力データ．行列 [m x n] で，各行には分類される1つのオブジェクトの変数（特徴量）が含まれます．CV_32F というデータ型を持ちます．
results : Predicted labels as a column matrix of type CV_32S.,results : 予測されたラベルを，CV_32S 型の列方向の行列として返します．
flags : Not used.,flags :使用されません．
Implements cv::ml::StatModel.,cv::ml::StatModel を実装しています．
This function returns the trained parameters arranged across rows.,この関数は，学習されたパラメータを列方向に並べて返します．
"For a two class classification problem, it returns a row matrix. It returns learnt parameters of the Logistic Regression as a matrix of type CV_32F.",2クラス分類問題の場合は，行行列が返されます．これは，ロジスティック回帰の学習済みパラメータを CV_32F 型の行列として返します．
Creates Logistic Regression model with parameters given.,与えられたパラメータでロジスティック回帰モデルを作成します．
Implements Logistic Regression classifier. ,ロジスティック回帰分類器を実装します．
See alsoLogistic Regression  ,ロジスティック回帰 も参照してください．
Predicts the response for sample(s).,サンプルの応答を予測します．
"The method estimates the most probable classes for input vectors. Input vectors (one or more) are stored as rows of the matrix inputs. In case of multiple input vectors, there should be one output vector outputs. The predicted class for a single input vector is returned by the method. The vector outputProbs contains the output probabilities corresponding to each element of result.",このメソッドは，入力ベクトルに対して最も確率の高いクラスを推定します．入力ベクトル（1つまたは複数）は，行列inputsの行として格納されます．複数の入力ベクトルがある場合，出力ベクトルは1つでなければなりません．1つの入力ベクトルに対して予測されたクラスが，このメソッドによって返されます．ベクトル outputProbs には、result の各要素に対応する出力確率が含まれています。
Creates empty model Use StatModel::train to train the model after creation.,空のモデルを作成 StatModel::train を使用して、作成後のモデルをトレーニングします。
Bayes classifier for normally distributed data. ,正規分布データに対するベイズ分類器
See alsoNormal Bayes Classifier  ,関連項目： 正規ベイズ分類法
Returns the number of variables in training samples.,トレーニングサンプルの変数の数を返します。
Returns true if the model is trained.,モデルがトレーニングされている場合は true を返します。
Returns true if the model is classifier.,モデルが分類器の場合は true を返します。
Trains the statistical model.,統計モデルを学習します。
samples : training samples,samples : 学習用サンプル
layout : See ml::SampleTypes.,layout :ml::SampleTypesを参照してください。
responses : vector of responses associated with the training samples.,responses : 訓練用サンプルに関連する応答のベクトル．
Predicts response(s) for the provided sample(s),与えられたサンプルに対する応答を予測する
"samples : The input samples, floating-point matrix",samples :入力サンプルを表す浮動小数点型行列．
results : The optional output matrix of results.,results : 結果を表すオプションの出力行列．
"flags : The optional flags, model-dependent. See cv::ml::StatModel::Flags.",flags :モデルに依存したオプションのフラグ．cv::ml::StatModel::Flags を参照してください．
"Implemented in cv::ml::LogisticRegression, and cv::ml::EM.","cv::ml::LogisticRegression, および cv::ml::EM で実装されています．"
Type of a SVM formulation. See SVM::Types. Default value is SVM::C_SVC.,SVM の定式化の種類．SVM::Types を参照してください．デフォルト値は SVM::C_SVC です．
See alsosetType,alsosetType を参照。
See alsogetType,alsogetType を参照してください。
"Parameter \(\gamma\) of a kernel function. For SVM::POLY, SVM::RBF, SVM::SIGMOID or SVM::CHI2. Default value is 1.","カーネル関数のパラメータ ˶ˆ꒳ˆ˵SVM::POLY, SVM::RBF, SVM::SIGMOID または SVM::CHI2 の場合。デフォルト値は 1。"
See alsosetGamma,alsosetGamma を参照。
See alsogetGamma,alsogetGamma を参照。
Parameter coef0 of a kernel function. For SVM::POLY or SVM::SIGMOID. Default value is 0.,カーネル関数のパラメータ coef0。SVM::POLY または SVM::SIGMOID の場合。既定値は0。
See alsosetCoef0,alsosetCoef0 を参照。
See alsogetCoef0,alsogetCoef0 を参照。
Parameter degree of a kernel function. For SVM::POLY. Default value is 0.,カーネル関数のパラメータ degree。SVM::POLY の場合。デフォルトの値は 0。
See alsosetDegree,alsosetDegree を参照。
See alsogetDegree,alsogetDegree 参照。
Parameter \(\epsilon\) of a SVM optimization problem. For SVM::EPS_SVR. Default value is 0.,SVM 最適化問題のパラメータ ˶ˆ꒳ˆ˵ ˶ˆ꒳ˆ˵SVM::EPS_SVR の場合。デフォルト値は 0 です。
"Parameter \(\nu\) of a SVM optimization problem. For SVM::NU_SVC, SVM::ONE_CLASS or SVM::NU_SVR. Default value is 0.","SVM 最適化問題のパラメータ ˶nu˶SVM::NU_SVC, SVM::ONE_CLASS, SVM::NU_SVR の場合。デフォルト値は 0。"
See alsosetNu,alsosetNu を参照。
See alsogetNu,alsogetNu を参照。
See alsogetClassWeights,alsogetClassWeights を参照。
Type of a SVM kernel. See SVM::KernelTypes. Default value is SVM::RBF.,SVM カーネルの種類。SVM::KernelTypes を参照。デフォルト値は SVM::RBF。
Initialize with one of predefined kernels. See SVM::KernelTypes.,定義済みカーネルの1つで初期化。SVM::KernelTypes を参照。
Retrieves all the support vectors.,すべてのサポートベクターを取得。
"The method returns all the support vectors as a floating-point matrix, where support vectors are stored as matrix rows.",このメソッドは，すべてのサポートベクターを浮動小数点型の行列として返します．サポートベクターは行列の行として保存されます．
Retrieves the decision function.,決定関数を検索します．
"i : the index of the decision function. If the problem solved is regression, 1-class or 2-class classification, then there will be just one decision function and the index should always be 0. Otherwise, in the case of N-class classification, there will be \(N(N-1)/2\) decision functions.",i : 決定関数のインデックス．解決する問題が回帰，1クラスまたは2クラス分類の場合，決定関数は1つだけで，インデックスは常に0でなければなりません．そうでない場合，Nクラス分類の場合は，\(N(N-1)/2\)個の決定関数が存在します．
"alpha : the optional output vector for weights, corresponding to different support vectors. In the case of linear SVM all the alpha's will be 1's.",alpha : 異なるサポートベクトルに対応する，重みのオプション出力ベクトルです．線形SVMの場合，αはすべて1になります．
"svidx : the optional output vector of indices of support vectors within the matrix of support vectors (which can be retrieved by SVM::getSupportVectors). In the case of linear SVM each decision function consists of a single ""compressed"" support vector.",svidx : サポートベクターの行列（SVM::getSupportVectors で取得できます）内のサポートベクターのインデックスを表すオプションの出力ベクトルです．線形 SVM の場合，各決定関数は1つの「圧縮された」サポートベクトルで構成されます．
"The method returns rho parameter of the decision function, a scalar subtracted from the weighted sum of kernel responses.",このメソッドは、決定関数の rho パラメータ（カーネル応答の加重和から減算されたスカラー）を返します。
Generates a grid for SVM parameters.,SVM パラメータのグリッドを生成します。
param_id : SVM parameters IDs that must be one of the SVM::ParamTypes. The grid is generated for the parameter with this ID.,param_id : SVM パラメータ ID で、SVM::ParamTypes のいずれかでなければならない。グリッドはこの ID を持つパラメータに対して生成されます。
The function generates a grid for the specified parameter of the SVM algorithm. The grid may be passed to the function SVM::trainAuto.,この関数は SVM アルゴリズムの指定されたパラメータ用のグリッドを生成します。このグリッドは関数 SVM::trainAuto に渡すことができる。
"Creates empty model. Use StatModel::train to train the model. Since SVM has several parameters, you may want to find the best parameters for your problem, it can be done with SVM::trainAuto.",空のモデルを作成します。モデルの学習には StatModel::train を使用します。SVM にはいくつかのパラメータがあるため、問題に最適なパラメータを見つけたい場合は、SVM::trainAuto で行うことができます。
Support Vector Machines. ,サポートベクターマシン。
See alsoSupport Vector Machines  ,関連項目サポートベクターマシン
Loads and creates a serialized svm from a file.,シリアル化された SVM をファイルから読み込み、作成する。
filepath : path to serialized svm,filepath : シリアライズされたSVMのパス
"Use SVM::save to serialize and store an SVM to disk. Load the SVM from this file again, by calling this function with the path to the file.",SVM::save を使用して SVM をシリアル化してディスクに保存する。ファイルへのパスを指定してこの関数を呼び出すことにより、このファイルから再度 SVM を読み込む。
Groups the object candidate rectangles.,オブジェクト候補の矩形をグループ化する。
rectList : Input/output vector of rectangles. Output vector includes retained and grouped rectangles. (The Python list is not modified in place.),rectList : 矩形の入出力ベクトル。出力ベクトルには，保持された矩形とグループ化された矩形が含まれます．(Pythonのリストは，その場では変更されません．)
groupThreshold : Minimum possible number of rectangles minus 1. The threshold is used in a group of rectangles to retain it.,groupThreshold : 最小限可能な矩形数から1を引いた値．この閾値は，グループ化された矩形を保持する際に用いられる．
eps : Relative difference between sides of the rectangles to merge them into a group.,eps :矩形をグループにまとめるための，各矩形の辺の相対的な差．
"The function is a wrapper for the generic function partition . It clusters all the input rectangles using the rectangle equivalence criteria that combines rectangles with similar sizes and similar locations. The similarity is defined by eps. When eps=0 , no clustering is done at all. If \(\texttt{eps}\rightarrow +\inf\) , all the rectangles are put in one cluster. Then, the small clusters containing less than or equal to groupThreshold rectangles are rejected. In each other cluster, the average rectangle is computed and put into the output rectangle list.",この関数は，汎用関数 partition のラッパーです．この関数は，類似したサイズと類似した位置にある矩形を結合する矩形等価基準を用いて，すべての入力矩形をクラスタリングします．この類似性は eps で定義されます．eps=0 の場合，クラスタリングは全く行われません．eps=0 の場合は，クラスタリングは行われません．また，(\{eps}\rightarrow +\inf\) の場合は，すべての長方形が1つのクラスタに入ります．そして，groupThreshold以下の矩形を含む小さなクラスタは拒否されます．他の各クラスタでは，平均的な矩形が計算され，出力矩形リストに入れられます．
Cascade classifier class for object detection. ,オブジェクト検出用のカスケード分類器クラス．
Examples: samples/cpp/facedetect.cpp.,例：samples/cpp/facedetect.cpp.
Checks whether the classifier has been loaded.,分類器が読み込まれたかどうかをチェックします。
Loads a classifier from a file.,ファイルから分類器を読み込みます。
filename : Name of the file from which the classifier is loaded. The file may contain an old HAAR classifier trained by the haartraining application or a new cascade classifier trained by the traincascade application.,filename : 分類器を読み込むファイルの名前．このファイルには，haartrainingアプリケーションで学習された古いHAAR分類器や，traincascadeアプリケーションで学習された新しいカスケード分類器が含まれています．
Detects objects of different sizes in the input image. The detected objects are returned as a list of rectangles.,入力画像中の異なるサイズの物体を検出します．検出されたオブジェクトは，矩形のリストとして返されます．
image : Matrix of the type CV_8U containing an image where objects are detected.,image : オブジェクトが検出された画像を含む CV_8U 型の行列．
"objects : Vector of rectangles where each rectangle contains the detected object, the rectangles may be partially outside the original image.",objects :各矩形が検出されたオブジェクトを含む矩形のベクトル．
scaleFactor : Parameter specifying how much the image size is reduced at each image scale.,scaleFactor : 各画像スケールにおいて，画像サイズをどの程度縮小するかを指定するパラメータ．
minNeighbors : Parameter specifying how many neighbors each candidate rectangle should have to retain it.,minNeighbors :候補となる矩形を保持するために必要な隣接数を指定するパラメータ．
flags : Parameter with the same meaning for an old cascade as in the function cvHaarDetectObjects. It is not used for a new cascade.,flags :古いカスケードに対しては，関数 cvHaarDetectObjects と同じ意味を持つパラメータ．これは，新しいカスケードでは利用されません．
minSize : Minimum possible object size. Objects smaller than that are ignored.,minSize : 可能なオブジェクトの最小サイズ．これよりも小さいオブジェクトは無視されます．
maxSize : Maximum possible object size. Objects larger than that are ignored. If maxSize == minSize model is evaluated on single scale.,maxSize : 可能な最大オブジェクトサイズ．それより大きいオブジェクトは無視される．maxSize == minSize の場合，モデルはシングルスケールで評価されます．
The function is parallelized with the TBB library.Note,この関数は，TBB ライブラリを用いて並列化されています．
(Python) A face detection example using cascade classifiers can be found at opencv_source_code/samples/python/facedetect.pyExamples: samples/cpp/facedetect.cpp.,(Python) カスケード分類器を用いた顔検出の例は，opencv_source_code/samples/python/facedetect.pyExamples: samples/cpp/facedetect.cppに掲載されています．
Implementation of HOG (Histogram of Oriented Gradients) descriptor and object detector. ,HOG (Histogram of Oriented Gradients) ディスクリプタとオブジェクト検出器の実装．
the HOG descriptor algorithm introduced by Navneet Dalal and Bill Triggs [50] .,Navneet DalalとBill Triggsによって紹介されたHOGディスクリプタアルゴリズム[50]を使用しています．
useful links:,便利なリンク集です．
https://hal.inria.fr/inria-00548512/document/,https://hal.inria.fr/inria-00548512/document/
https://en.wikipedia.org/wiki/Histogram_of_oriented_gradients,https://en.wikipedia.org/wiki/Histogram_of_oriented_gradients
https://software.intel.com/en-us/ipp-dev-reference-histogram-of-oriented-gradients-hog-descriptor,https://software.intel.com/en-us/ipp-dev-reference-histogram-of-oriented-gradients-hog-descriptor
http://www.learnopencv.com/histogram-of-oriented-gradients,http://www.learnopencv.com/histogram-of-oriented-gradients
http://www.learnopencv.com/handwritten-digits-classification-an-opencv-c-python-tutorial ,http://www.learnopencv.com/handwritten-digits-classification-an-opencv-c-python-tutorial
"Examples: samples/cpp/peopledetect.cpp, samples/cpp/train_HOG.cpp, and samples/tapi/hog.cpp.",例：samples/cpp/peopledetect.cpp，samples/cpp/train_HOG.cpp，samples/tapi/hog.cpp．
Returns the number of coefficients required for the classification.,分類に必要な係数の数を返します．
Checks if detector size equal to descriptor size.,検出器のサイズが記述子のサイズと等しいかどうかをチェックします．
Returns winSigma value.,winSigma の値を返します．
Sets coefficients for the linear SVM classifier.,線形SVM分類器の係数を設定します。
svmdetector : coefficients for the linear SVM classifier.,svmdetector : 線形 SVM 分類器の係数を設定。
loads HOGDescriptor parameters and coefficients for the linear SVM classifier from a file.,HOGDescriptor のパラメータと線形 SVM 分類器の係数をファイルから読み込む。
filename : Path of the file to read.,filename : 読み込むファイルのパス。
"objname : The optional name of the node to read (if empty, the first top-level node will be used).",objname : 読み込むノードのオプション名（空の場合は、最初のトップレベルノードが使用されます）。
saves HOGDescriptor parameters and coefficients for the linear SVM classifier to a file,HOGDescriptorのパラメータと線形SVM分類器の係数をファイルに保存します。
filename : File name,filename : ファイル名
objname : Object name,objname : オブジェクト名
Computes HOG descriptors of given image.,与えられた画像の HOG ディスクリプタを計算します．
img : Matrix of the type CV_8U containing an image where HOG features will be calculated.,img : HOG特徴量が計算される画像を含むCV_8U型の行列．
descriptors : Matrix of the type CV_32F,descriptors :CV_32F 型の行列．
winStride : Window stride. It must be a multiple of block stride.,winStride : ウィンドウのストライド．ブロックのストライドの倍数である必要があります．
padding : Padding,padding : パディング．
locations : Vector of Point,location :Pointのベクトル．
Performs object detection without a multi-scale window.,マルチスケールウィンドウを利用せずに，物体検出を行います．
img : Matrix of the type CV_8U or CV_8UC3 containing an image where objects are detected.,img : CV_8U または CV_8UC3 型の行列で，物体が検出される画像を含みます．
foundLocations : Vector of point where each point contains left-top corner point of detected object boundaries.,foundLocations :検出されたオブジェクトの境界の左上隅を含む点のベクトル．
"hitThreshold : Threshold for the distance between features and SVM classifying plane. Usually it is 0 and should be specified in the detector coefficients (as the last free coefficient). But if the free coefficient is omitted (which is allowed), you can specify it manually here.",hitThreshold : 特徴量とSVM分類平面との距離を表す閾値．通常は0であり，検出器の係数に（最後の自由係数として）指定されるべきである．しかし，自由係数が省略されている場合（これは許される），ここで手動で指定することができます．
searchLocations : Vector of Point includes locations to search.,searchLocations :Pointのベクトルは，検索する場所を含みます．
weights : Vector that will contain confidence values for each detected object.,weights : 検出された各オブジェクトの信頼値を含むベクター。
searchLocations : Vector of Point includes set of requested locations to be evaluated.,searchLocations :Pointのベクターには、評価されるべき要求された場所のセットが含まれます。
foundLocations : Vector of rectangles where each rectangle contains the detected object.,foundLocations :findLocations : 各矩形が検出されたオブジェクトを含む矩形のベクトル。
foundWeights : Vector that will contain confidence values for each detected object.,foundWeights : 検出された各オブジェクトの信頼値を含むベクトル。
scale : Coefficient of the detection window increase.,scale : 検出窓の増加係数．
finalThreshold : Final threshold,finalThreshold : 最終的な閾値
useMeanshiftGrouping : indicates grouping algorithm,useMeanshiftGrouping : グルーピングアルゴリズムを示す
Computes gradients and quantized gradient orientations.,グラデーションと量子化されたグラデーションの向きを計算します．
img : Matrix contains the image to be computed,img : 計算対象となる画像を含む行列．
grad : Matrix of type CV_32FC2 contains computed gradients,grad : CV_32FC2 型の行列で，計算されたグラデーションが格納されています．
angleOfs : Matrix of type CV_8UC2 contains quantized gradient orientations,angleOfs : CV_8UC2 型の行列で，量子化されたグラデーションの向きを表します．
paddingTL : Padding from top-left,paddingTL : 左上からのパディング
paddingBR : Padding from bottom-right,paddingBR : 右下からのパディング
evaluate specified ROI and return confidence value for each location,指定されたROIを評価し，各位置に対する信頼値を返す
foundLocations : Vector of Point where each Point is detected object's top-left point.,foundLocations :検出された物体の左上の点を示すPointのベクトル
confidences : confidences,confidences : 信頼度
"hitThreshold : Threshold for the distance between features and SVM classifying plane. Usually it is 0 and should be specified in the detector coefficients (as the last free coefficient). But if the free coefficient is omitted (which is allowed), you can specify it manually here",hitThreshold : 特徴量とSVM分類面との距離の閾値。通常は0であり，検出器の係数に（最後の自由係数として）指定されるべきである．しかし、自由係数が省略されている場合（これは許可されている）、ここで手動で指定することができます。
winStride : winStride,winStride : winStride
padding : padding,padding : パディング
evaluate specified ROI and return confidence value for each location in multiple scales,指定されたROIを評価し、各位置の信頼値を複数のスケールで返す
locations : Vector of DetectionROI,location :検出ROIのベクトル
weights : Input/output vector of weights of rectangles. Output vector includes weights of retained and grouped rectangles. (The Python list is not modified in place.),weights : 矩形の重みの入力/出力ベクトル．出力ベクトルには，保持された矩形とグループ化された矩形の重みが含まれます．(Pythonのリストはその場では変更されません。)
"Detection window size. Align to block size and block stride. Default value is Size(64,128).","検出ウィンドウサイズ．ブロックサイズとブロックストライドに合わせます．デフォルト値は Size(64,128)."
"Block size in pixels. Align to cell size. Default value is Size(16,16).","ブロックサイズ（ピクセル）。セルサイズに合わせます。初期値はSize(16,16)です。"
"Block stride. It must be a multiple of cell size. Default value is Size(8,8).","ブロックのストライド。セルサイズの倍数である必要があります。初期値は Size(8,8) です。"
"Cell size. Default value is Size(8,8).","セルの大きさ。初期値はSize(8,8)です。"
Number of bins used in the calculation of histogram of gradients. Default value is 9.,グラデーションのヒストグラムの計算に使われるビンの数．既定値は9です．
not documented,ドキュメントなし
Gaussian smoothing window parameter.,ガウス平滑化窓のパラメータ．
histogramNormType,histogramNormType
L2-Hys normalization method shrinkage.,L2-Hys正規化法の縮退．
Flag to specify whether the gamma correction preprocessing is required or not.,ガンマ補正の前処理が必要かどうかを指定するフラグ．
Maximum number of detection window increases. Default value is 64.,検出窓の最大増加数．デフォルト値は64です。
Examples: samples/tapi/hog.cpp.,例： samples/tapi/hog.cpp.
Indicates signed gradient will be used or not.,符号付きグラデーションを使用するかどうかを示します。
sets the epsilon used during the horizontal scan of QR code stop marker detection.,QRコードのストップマーカー検出の水平走査時に使用するイプシロンを設定します。
"epsX : Epsilon neighborhood, which allows you to determine the horizontal pattern of the scheme 1:1:3:1:1 according to QR code standard.",epsX : イプシロンの近傍で、QRコードの規格に従った1:1:3:1:1の方式の水平パターンを決定することができます。
sets the epsilon used during the vertical scan of QR code stop marker detection.,は、QRコードのストップマーカ検出の垂直走査時に使用するイプシロンを設定します。
"epsY : Epsilon neighborhood, which allows you to determine the vertical pattern of the scheme 1:1:3:1:1 according to QR code standard.",epsY : イプシロンの近傍で、QRコードの規格に従ったスキーム1:1:3:1:1の垂直パターンを決定することができます。
Detects QR code in image and returns the quadrangle containing the code.,画像中のQRコードを検出し、そのコードを含む四角形を返します。
img : grayscale or color (BGR) image containing (or not) QR code.,img : QRコードを含む（または含まない）グレースケールまたはカラー（BGR）画像．
points : Output vector of vertices of the minimum-area quadrangle containing the code.,points :コードを含む最小面積の四角形の頂点の出力ベクトル．
Decodes QR code in image once it's found by the detect() method.,detect()メソッドで見つかった画像中のQRコードをデコードします。
img : grayscale or color (BGR) image containing QR code.,img : QRコードを含むグレースケールまたはカラー（BGR）画像．
points : Quadrangle vertices found by detect() method (or some other algorithm).,points :detect()メソッド（または他のアルゴリズム）で見つけた四角形の頂点。
straight_qrcode : The optional output image containing rectified and binarized QR code,"straight_qrcode :rectified, binarized QR code を含む出力画像（オプション）．"
Returns UTF8-encoded output string or empty string if the code cannot be decoded.,UTF8 エンコードされた出力文字列、またはコードがデコードできない場合は空の文字列を返します。
Both detects and decodes QR code.,QRコードの検出とデコードの両方を行います。
points : optional output array of vertices of the found QR code quadrangle. Will be empty if not found.,points : 見つかったQRコードの四角形の頂点を示す出力配列（オプション）．見つからない場合は空になります．
Detects QR codes in image and returns the vector of the quadrangles containing the codes.,画像中のQRコードを検出し，そのコードを含む四角形のベクトルを返す．
img : grayscale or color (BGR) image containing (or not) QR codes.,img : QRコードを含む（または含まない）グレースケールまたはカラー（BGR）画像．
points : Output vector of vector of vertices of the minimum-area quadrangle containing the codes.,points :コードを含む最小面積の四角形の頂点のベクトルの出力ベクトル．
Decodes QR codes in image once it's found by the detect() method.,detect()メソッドで見つかった画像のQRコードをデコードします。
img : grayscale or color (BGR) image containing QR codes.,img : QRコードを含むグレースケールまたはカラー（BGR）画像．
decoded_info : UTF8-encoded output vector of string or empty vector of string if the codes cannot be decoded.,decoded_info : UTF8エンコードされた文字列の出力ベクトル，または，コードがデコードできない場合は空の文字列のベクトル．
points : vector of Quadrangle vertices found by detect() method (or some other algorithm).,points : detect()メソッド（あるいは他のアルゴリズム）で見つかった四角形の頂点のベクトル．
straight_qrcode : The optional output vector of images containing rectified and binarized QR codes,straight_qrcode :整形・2値化されたQRコードを含む画像の出力ベクトル（オプション）．
Restores the selected region in an image using the region neighborhood.,region neighborhood を利用して，画像中の選択領域を復元します．
"src : Input 8-bit, 16-bit unsigned or 32-bit float 1-channel or 8-bit 3-channel image.",src : 入力8ビット，16ビット符号なし，32ビット浮動小数点型の1チャンネル画像，または8ビット3チャンネル画像．
"inpaintMask : Inpainting mask, 8-bit 1-channel image. Non-zero pixels indicate the area that needs to be inpainted.",inpaintMask : 8ビット1チャンネル画像に設定された，インペイントマスク．非0のピクセルは，インペインティングが必要な領域を示します．
dst : Output image with the same size and type as src .,dst : src と同じサイズ・タイプの出力画像．
inpaintRadius : Radius of a circular neighborhood of each point inpainted that is considered by the algorithm.,inpaintRadius : アルゴリズムで考慮される，塗り潰した各点の円形の近傍領域の半径．
flags : Inpainting method that could be cv::INPAINT_NS or cv::INPAINT_TELEA,flags :cv::INPAINT_NS または cv::INPAINT_TELEA である可能性がある，インペインティング手法．
"The function reconstructs the selected image area from the pixel near the area boundary. The function may be used to remove dust and scratches from a scanned photo, or to remove undesirable objects from still images or video. See http://en.wikipedia.org/wiki/Inpainting for more details.Note",この関数は，選択された画像領域を，その領域の境界付近のピクセルから再構成します．この関数は，スキャンした写真から埃や傷を除去したり，静止画像やビデオから望ましくないオブジェクトを除去したりするために利用できます．詳しくは http://en.wikipedia.org/wiki/Inpainting をご覧ください。
An example using the inpainting technique can be found at opencv_source_code/samples/cpp/inpaint.cpp,inpainting techniqueを使った例は，opencv_source_code/samples/cpp/inpaint.cppにあります．
(Python) An example using the inpainting technique can be found at opencv_source_code/samples/python/inpaint.py,(Python) inpainting technique を使用した例は、opencv_source_code/samples/python/inpaint.py にあります。
Perform image denoising using Non-local Means Denoising algorithm http://www.ipol.im/pub/algo/bcm_non_local_means_denoising/ with several computational optimizations. Noise expected to be a gaussian white noise.,Non-Local Means Denoising アルゴリズム http://www.ipol.im/pub/algo/bcm_non_local_means_denoising/ を使用して、いくつかの計算最適化を行い、画像のノイズ除去を実行します。ノイズは，ガウスホワイトノイズを想定しています．
"src : Input 8-bit 1-channel, 2-channel, 3-channel or 4-channel image.",src : 8ビット，1チャンネル，2チャンネル，3チャンネル，4チャンネルの画像を入力．
templateWindowSize : Size in pixels of the template patch that is used to compute weights. Should be odd. Recommended value 7 pixels,templateWindowSize : 重みの計算に利用されるテンプレートパッチのサイズ（ピクセル単位）．奇数でなければいけません．推奨値：7ピクセル
searchWindowSize : Size in pixels of the window that is used to compute weighted average for given pixel. Should be odd. Affect performance linearly: greater searchWindowsSize - greater denoising time. Recommended value 21 pixels,searchWindowSize : 与えられたピクセルの加重平均を計算するために利用されるウィンドウのサイズ（ピクセル単位）．奇数にしてください。searchWindowSizeが大きいほど、ノイズ除去にかかる時間が長くなります。推奨値：21ピクセル
"h : Parameter regulating filter strength. Big h value perfectly removes noise but also removes image details, smaller h value preserves details but also preserves some noise",h : フィルタの強度を調整するパラメータ．hが大きいと、ノイズは完全に除去されますが、画像のディテールも除去されます。hが小さいと、ディテールは維持されますが、ノイズも若干維持されます。
This function expected to be applied to grayscale images. For colored images look at fastNlMeansDenoisingColored. Advanced usage of this functions can be manual denoising of colored image in different colorspaces. Such approach is used in fastNlMeansDenoisingColored by converting image to CIELAB colorspace and then separately denoise L and AB components with different h parameter.,この機能は、グレースケール画像に適用することを想定しています。色付きの画像の場合は、 fastNlMeansDenoisingColored をご覧ください。この関数の高度な利用法として，異なる色空間を持つカラー画像を手動でノイズ除去することができます．FastNlMeansDenoisingColoredでは，画像をCIELAB色空間に変換し，L成分とAB成分をそれぞれ異なるhパラメータで別々にノイズ除去するという手法が用いられています．
Modification of fastNlMeansDenoising function for colored images.,fastNlMeansDenoising関数を，色付き画像用に改良します．
src : Input 8-bit 3-channel image.,src : 入力8ビット3チャンネル画像．
"h : Parameter regulating filter strength for luminance component. Bigger h value perfectly removes noise but also removes image details, smaller h value preserves details but also preserves some noise",h : 輝度成分に対するフィルタの強さを調整するパラメータ．h が大きいと，ノイズは完全に除去されますが，画像の細部も除去されます．h が小さいと，細部は保持されますが，ノイズも若干保持されます．
hColor : The same as h but for color components. For most images value equals 10 will be enough to remove colored noise and do not distort colors,hColor : h と同様に、色成分に対するフィルタリングを行います。ほとんどの画像では、10に等しい値を設定すれば、色ノイズを除去し、色を歪ませることはありません。
The function converts image to CIELAB colorspace and then separately denoise L and AB components with given h parameters using fastNlMeansDenoising function.,この関数は，画像を CIELAB 色空間に変換した後，fastNlMeansDenoising 関数を用いて，与えられた h パラメータで L 成分と AB 成分を個別にノイズ除去します．
Modification of fastNlMeansDenoising function for images sequence where consecutive images have been captured in small period of time. For example video. This version of the function is for grayscale images or for manual manipulation with colorspaces. For more details see http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.131.6394,fastNlMeansDenoising関数を，短時間に連続して撮影された画像群用に変更しました．例えば、ビデオなどです。このバージョンの関数は，グレースケール画像や，色空間を使った手動操作のためのものです．詳細は， http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.131.6394 を参照してください．
"srcImgs : Input 8-bit 1-channel, 2-channel, 3-channel or 4-channel images sequence. All images should have the same type and size.",srcImgs :8ビット，1チャンネル，2チャンネル，3チャンネル，4チャンネルの画像シーケンスを入力します．すべての画像は，同じ種類，同じサイズでなければいけません．
imgToDenoiseIndex : Target image to denoise index in srcImgs sequence,imgToDenoiseIndex : srcImgs シーケンス中の，ノイズ除去を行う対象画像．
temporalWindowSize : Number of surrounding images to use for target image denoising. Should be odd. Images from imgToDenoiseIndex - temporalWindowSize / 2 to imgToDenoiseIndex - temporalWindowSize / 2 from srcImgs will be used to denoise srcImgs[imgToDenoiseIndex] image.,temporalWindowSize : 対象画像のノイズ除去に利用する周辺画像の数．奇数とする．srcImgs の imgToDenoiseIndex - temporalWindowSize / 2 から imgToDenoiseIndex - temporalWindowSize / 2 までの画像が， srcImgs[imgToDenoiseIndex] の画像をノイズ除去するために利用されます．
dst : Output image with the same size and type as srcImgs images.,dst : srcImgs の画像と同じサイズ，同じタイプの出力画像．
"h : Parameter regulating filter strength. Bigger h value perfectly removes noise but also removes image details, smaller h value preserves details but also preserves some noise",h : フィルタの強度を調整するパラメータ．h の値が大きいと，ノイズは完全に除去されますが，画像の詳細も除去されます．h の値が小さいと，詳細は保持されますが，ノイズも若干保持されます．
Modification of fastNlMeansDenoisingMulti function for colored images sequences.,FastNlMeansDenoisingMulti 関数を，カラー画像シーケンス用に改良しました．
srcImgs : Input 8-bit 3-channel images sequence. All images should have the same type and size.,srcImgs :8ビット，3チャンネルの画像列を入力します．すべての画像は，同じ種類，同じサイズでなければいけません．
"h : Parameter regulating filter strength for luminance component. Bigger h value perfectly removes noise but also removes image details, smaller h value preserves details but also preserves some noise.",h : 輝度成分に対するフィルタの強さを調整するパラメータ．h の値が大きいと，ノイズは完全に除去されますが，画像の細部も除去されます．h の値が小さいと，細部は保持されますが，ノイズも若干保持されます．
hColor : The same as h but for color components.,hColor : h と同様に、色成分に対するフィルタリングを行います。
The function converts images to CIELAB colorspace and then separately denoise L and AB components with given h parameters using fastNlMeansDenoisingMulti function.,この関数は，画像をCIELAB色空間に変換した後，fastNlMeansDenoisingMulti関数を用いて，与えられたhパラメータでL成分とAB成分を別々にノイズ除去します．
"Primal-dual algorithm is an algorithm for solving special types of variational problems (that is, finding a function to minimize some functional). As the image denoising, in particular, may be seen as the variational problem, primal-dual algorithm then can be used to perform denoising and this is exactly what is implemented.",Primal-dualアルゴリズムは，特殊な変分問題（ある関数を最小化するための関数を求める問題）を解くためのアルゴリズムです．特に、画像のノイズ除去は、変分問題と見なすことができるので、プライマル・デュアル・アルゴリズムを使ってノイズ除去を行うことができ、まさにそれを実現しています。
observations : This array should contain one or more noised versions of the image that is to be restored.,observations :この配列には，復元されるべき画像の1つ以上のノイズバージョンが含まれます．
"result : Here the denoised image will be stored. There is no need to do pre-allocation of storage space, as it will be automatically allocated, if necessary.",result : ここには，ノイズ除去された画像が格納されます．必要に応じて自動的に割り当てられるので，記憶領域を事前に確保する必要はありません．
"lambda : Corresponds to \(\lambda\) in the formulas above. As it is enlarged, the smooth (blurred) images are treated more favorably than detailed (but maybe more noised) ones. Roughly speaking, as it becomes smaller, the result will be more blur but more sever outliers will be removed.",lambda : 上の式の中の\(λ)に相当します。拡大すると、詳細な画像（ただし、ノイズが多いかもしれない）よりも、滑らかな（ぼやけた）画像が有利に扱われます。大雑把に言うと、小さくなるにつれて、結果的にはよりぼやけた画像になりますが、より深刻な異常値は除去されます。
"niters : Number of iterations that the algorithm will run. Of course, as more iterations as better, but it is hard to quantitatively refine this statement, so just use the default and increase it if the results are poor.",niters : アルゴリズムを実行する反復回数。もちろん、反復回数が多いほど良いのですが、定量的に絞り込むのは難しいので、デフォルトで使用し、結果が悪ければ増やすようにしてください。
"It should be noted, that this implementation was taken from the July 2013 blog entry [177] , which also contained (slightly more general) ready-to-use source code on Python. Subsequently, that code was rewritten on C++ with the usage of openCV by Vadim Pisarevsky at the end of July 2013 and finally it was slightly adapted by later authors.Although the thorough discussion and justification of the algorithm involved may be found in [42], it might make sense to skim over it here, following [177] . To begin with, we consider the 1-byte gray-level images as the functions from the rectangular domain of pixels (it may be seen as set \(\left\{(x,y)\in\mathbb{N}\times\mathbb{N}\mid 1\leq x\leq n,\;1\leq y\leq m\right\}\) for some \(m,\;n\in\mathbb{N}\)) into \(\{0,1,\dots,255\}\). We shall denote the noised images as \(f_i\) and with this view, given some image \(x\) of the same size, we may measure how bad it is by the formula\[\left\|\left\|\nabla x\right\|\right\| + \lambda\sum_i\left\|\left\|x-f_i\right\|\right\|\]\(\|\|\cdot\|\|\) here denotes \(L_2\)-norm and as you see, the first addend states that we want our image to be smooth (ideally, having zero gradient, thus being constant) and the second states that we want our result to be close to the observations we've got. If we treat \(x\) as a function, this is exactly the functional what we seek to minimize and here the Primal-Dual algorithm comes into play.",この実装は、2013年7月のブログエントリ[177]から取ったもので、Pythonですぐに使えるソースコードも含まれていたことに注意してください。その後，2013年7月末にVadim PisarevskyによってopenCVを用いてC++に書き直され，最終的には後続の著者によって若干の改良が加えられました．このアルゴリズムの詳細な議論と正当性は[42]に記載されていますが，ここでは[177]に倣ってざっと目を通すのがよいでしょう．まず，1バイトのグレイレベル画像を，画素の矩形領域からの関数と考えます（これは，集合 ˶‾᷄ -̫ ‾᷅˵ ˶‾᷅˵）。1\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\このように考えて、同じ大きさの画像\(x\)が与えられたとき、ノイズ化された画像を\(f_i\)とします。どれくらいひどいかを式で表すと、[⋈◍＞◡＜◍＞◡＜◍＞]ということになります。最初の付記は，画像が滑らかであること（理想的には，勾配がゼロであること，つまり，一定であること）を示し，2番目の付記は，結果が我々が得た観測値に近いものであることを示しています。˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ )
"Transforms a color image to a grayscale image. It is a basic tool in digital printing, stylized black-and-white photograph rendering, and in many single channel image processing applications [155] .",カラー画像をグレースケール画像に変換します。これは，デジタル印刷や白黒写真のレンダリングなどの基本的なツールであり，多くのシングルチャンネル画像処理アプリケーションでも使用されています[155]．
grayscale : Output 8-bit 1-channel image.,grayscale : 8ビットの1チャンネル画像を出力します．
color_boost : Output 8-bit 3-channel image.,color_boost :8 ビットの 3 チャンネル画像を出力します。
This function is to be applied on color images.,この機能は，カラー画像に適用されます．
"Image editing tasks concern either global changes (color/intensity corrections, filters, deformations) or local changes concerned to a selection. Here we are interested in achieving local changes, ones that are restricted to a region manually selected (ROI), in a seamless and effortless manner. The extent of the changes ranges from slight distortions to complete replacement by novel content [192] .",画像編集作業には，大局的な変化（色や濃度の補正，フィルタ，変形）と，選択範囲に関わる局所的な変化があります．ここでは，手動で選択した領域（ROI）に限定した局所的な変更を，シームレスかつ容易に実現することに関心がある．その変化の程度は，わずかな歪みから新しいコンテンツへの完全な置き換えまで様々である[192]．
dst : Input 8-bit 3-channel image.,dst : 入力された8ビットの3チャンネル画像．
mask : Input 8-bit 1 or 3-channel image.,mask : 入力された8ビットの1チャンネルまたは3チャンネルの画像
p : Point in dst image where object is placed.,p :dst 画像の中で，物体が置かれている点．
blend : Output image with the same size and type as dst.,blend : dst と同じサイズとタイプの出力画像．
"flags : Cloning method that could be cv::NORMAL_CLONE, cv::MIXED_CLONE or cv::MONOCHROME_TRANSFER","flags :cv::NORMAL_CLONE, cv::MIXED_CLONE, cv::MONOCHROME_TRANSFER のいずれかのクローン作成方法．"
Examples: samples/cpp/tutorial_code/photo/seamless_cloning/cloning_demo.cpp.,例： samples/cpp/tutorial_code/photo/seamless_cloning/cloning_demo.cpp．
"Given an original color image, two differently colored versions of this image can be mixed seamlessly.",元のカラー画像が与えられると，色の異なる2つのバージョンをシームレスに混合することができます．
red_mul : R-channel multiply factor.,red_mul : Rチャンネルの乗算係数．
green_mul : G-channel multiply factor.,green_mul : Gチャンネルの乗算器。
blue_mul : B-channel multiply factor.,blue_mul : Bチャンネルの乗算器。
Multiplication factor is between .5 to 2.5.Examples: samples/cpp/tutorial_code/photo/seamless_cloning/cloning_demo.cpp.,例：samples/cpp/tutorial_code/photo/seamless_cloning/cloning_demo.cpp.
"Applying an appropriate non-linear transformation to the gradient field inside the selection and then integrating back with a Poisson solver, modifies locally the apparent illumination of an image.",選択範囲内のグラデーションフィールドに適切な非線形変換を適用し、ポアソンソルバーで積分し直すことで、画像の見かけ上の照明を局所的に変更します。
alpha : Value ranges between 0-2.,alpha : 値の範囲は0〜2です。
beta : Value ranges between 0-2.,beta :値の範囲は 0-2 です。
This is useful to highlight under-exposed foreground objects or to reduce specular reflections.Examples: samples/cpp/tutorial_code/photo/seamless_cloning/cloning_demo.cpp.,例：samples/cpp/tutorial_code/photo/seamless_cloning/cloning_demo.cpp.
"By retaining only the gradients at edge locations, before integrating with the Poisson solver, one washes out the texture of the selected region, giving its contents a flat aspect. Here Canny Edge Detector is used.",ポアソンソルバーで積分する前に、エッジ位置のグラデーションのみを保持することで、選択された領域のテクスチャを洗い流し、その内容をフラットにすることができます。ここではCanny Edge Detectorを使用しています。
low_threshold : Range from 0 to 100.,low_threshold : 0〜100の範囲で設定します。
high_threshold : Value > 100.,high_threshold : 100以上の値。
kernel_size : The size of the Sobel kernel to be used.,kernel_size : 使用するSobelカーネルのサイズ。
"NoteThe algorithm assumes that the color of the source image is close to that of the destination. This assumption means that when the colors don't match, the source image color gets tinted toward the color of the destination image.Examples: samples/cpp/tutorial_code/photo/seamless_cloning/cloning_demo.cpp.",注）このアルゴリズムは、ソース画像の色がデスティネーションの色に近いことを前提としています。例：samples/cpp/tutorial_code/photo/seamless_cloning/cloning_demo.cpp.
Filtering is the fundamental operation in image and video processing. Edge-preserving smoothing filters are used in many different applications [86] .,フィルタリングは、画像・映像処理の基本的な操作です。エッジを保持する平滑化フィルタは，さまざまな用途で使用されています[86]．
dst : Output 8-bit 3-channel image.,dst : 8ビット3チャンネル画像の出力．
flags : Edge preserving filters: cv::RECURS_FILTER or cv::NORMCONV_FILTER,flags :エッジを保存するフィルタ： cv::RECURS_FILTER または cv::NORMCONV_FILTER．
sigma_s : Range between 0 to 200.,sigma_s :0から200の範囲で指定します．
sigma_r : Range between 0 to 1.,sigma_r :0から1の範囲で指定します．
Examples: samples/cpp/tutorial_code/photo/non_photorealistic_rendering/npr_demo.cpp.,例： samples/cpp/tutorial_code/photo/non_photorealistic_rendering/npr_demo.cpp.
This filter enhances the details of a particular image.,特定の画像のディテールを強調するフィルタです。
Pencil-like non-photorealistic line drawing.,鉛筆で描いたようなノンフォトリアリスティックな線画です。
dst1 : Output 8-bit 1-channel image.,dst1 : 8ビット1チャンネルの画像を出力します。
dst2 : Output image with the same size and type as src.,dst2 : src と同じサイズ・タイプの画像を出力します．
shade_factor : Range between 0 to 0.1.,shade_factor : 0〜0.1 の範囲で指定します。
"Stylization aims to produce digital imagery with a wide variety of effects not focused on photorealism. Edge-aware filters are ideal for stylization, as they can abstract regions of low contrast while preserving, or enhancing, high-contrast features.",スタイライゼーションは、フォトリアリズムにこだわらず、多様な効果を持つデジタル画像を作り出すことを目的としています。エッジを考慮したフィルターは、コントラストの低い領域を抽象化する一方で、コントラストの高い特徴を維持または強調することができるため、スタイライゼーションに最適です。
Creates CalibrateDebevec object.,CalibrateDebevecオブジェクトを作成します。
samples : number of pixel locations to use,samples : 使用するピクセル位置の数
"lambda : smoothness term weight. Greater values produce smoother results, but can alter the response.",lambda : スムーズネス用語の重み。値が大きいほど滑らかな結果が得られますが、レスポンスが変化する可能性があります。
"random : if true sample pixel locations are chosen at random, otherwise they form a rectangular grid.",random : trueの場合，サンプルピクセルの位置はランダムに選択され，そうでない場合は長方形のグリッドを形成する．
"Inverse camera response function is extracted for each brightness value by minimizing an objective function as linear system. Objective function is constructed using pixel values on the same position in all images, extra term is added to make the result smoother. ",逆カメラ応答関数は，線形システムとしての目的関数を最小化することにより，各輝度値に対して抽出されます．目的関数は，すべての画像の同じ位置にあるピクセル値を用いて構成され，結果をより滑らかにするために追加の項が加えられます．
For more information see [55] . ,詳細は[55]を参照してください。
Creates CalibrateRobertson object.,CalibrateRobertson オブジェクトを作成します．
max_iter : maximal number of Gauss-Seidel solver iterations.,max_iter : ガウス・ザイデルソルバーの最大反復回数．
threshold : target difference between results of two successive steps of the minimization.,threshold : 最小化の2つの連続したステップの結果間の目標差．
Inverse camera response function is extracted for each brightness value by minimizing an objective function as linear system. This algorithm uses all image pixels. ,逆カメラ応答関数は，線形システムとしての目的関数を最小化することにより，各輝度値に対して抽出されます．このアルゴリズムは，すべての画像ピクセルを使用します．
For more information see [204] . ,詳細は[204]を参照してください。
Recovers inverse camera response.,逆カメラ応答関数を復元します．
src : vector of input images,src : 入力画像のベクトル
dst : 256x1 matrix with inverse camera response function,dst : カメラの逆反応関数を表す 256x1 の行列
times : vector of exposure time values for each image,times : 各画像に対する露光時間値のベクトル
Creates MergeDebevec object.,MergeDebevecオブジェクトを作成します。
The resulting HDR image is calculated as weighted average of the exposures considering exposure values and camera response. ,結果として得られるHDR画像は、露出値とカメラレスポンスを考慮した露出の加重平均として計算されます。
Creates MergeMertens object.,MergeMertens オブジェクトを作成します。
contrast_weight : contrast measure weight. See MergeMertens.,contrast_weight : コントラスト測定値の重み。MergeMertens参照。
saturation_weight : saturation measure weight,saturation_weight : 飽和度の指標となる重み
exposure_weight : well-exposedness measure weight,exposure_weight : well-exposure measure weight
"Pixels are weighted using contrast, saturation and well-exposedness measures, than images are combined using laplacian pyramids. ",ピクセルは、コントラスト、彩度、および露出度測定値を使用して重み付けされ、画像はラプラシアン ピラミッドを使用して結合されます。
"The resulting image weight is constructed as weighted average of contrast, saturation and well-exposedness measures.",結果として得られる画像の重みは、コントラスト、彩度、および露出度の尺度の加重平均として構成されます。
"The resulting image doesn't require tonemapping and can be converted to 8-bit image by multiplying by 255, but it's recommended to apply gamma correction and/or linear tonemapping.",結果として得られた画像はトーンマッピングを必要とせず、255 を乗じることで 8 ビット画像に変換できますが、ガンマ補正やリニアトーンマッピングを行うことが推奨されます。
For more information see [167] . ,詳細は [167] を参照してください。
Merges images.,画像を統合します。
dst : result image,dst : 結果画像
"response : 256x1 matrix with inverse camera response function for each pixel value, it should have the same number of channels as images.",response : 各ピクセル値に対する逆カメラ応答関数を含む 256x1 の行列，画像と同じチャンネル数でなければいけません．
"Implemented in cv::MergeRobertson, cv::MergeMertens, and cv::MergeDebevec.","cv::MergeRobertson, cv::MergeMertens, および cv::MergeDebevec で実装されています．"
Implements cv::MergeExposures.,cv::MergeExposures を実装しています．
Tonemaps image.,画像をトーンマップします．
src : source image - CV_32FC3 Mat (float 32 bits 3 channels),src : ソース画像 - CV_32FC3 Mat (float 32 bits 3 channels)
"dst : destination image - CV_32FC3 Mat with values in [0, 1] range","dst : 出力画像 - CV_32FC3 Mat （float 32 bits 3 channels）， [0, 1] の範囲の値を持ちます．"
Creates simple linear mapper with gamma correction.,ガンマ補正を行うシンプルなリニアマッパーを作成します．
"gamma : positive value for gamma correction. Gamma value of 1.0 implies no correction, gamma equal to 2.2f is suitable for most displays. Generally gamma > 1 brightens the image and gamma < 1 darkens it.",gamma : ガンマ補正のための正の値．gamma の値が 1.0 の場合は，補正を行わないことを意味し，2.2f に等しい gamma は，ほとんどのディスプレイに適しています．一般的に、ガンマ値が1を超えると画像が明るくなり、ガンマ値が1を超えると画像が暗くなります。
Base class for tonemapping algorithms - tools that are used to map HDR image to 8-bit range. ,トーンマッピングアルゴリズムのベースクラスです。これは、HDR画像を8ビットの範囲にマッピングするためのツールです。
Creates TonemapDrago object.,TonemapDragoオブジェクトを作成します。
"saturation : positive saturation enhancement value. 1.0 preserves saturation, values greater than 1 increase saturation and values less than 1 decrease it.",saturation : 正の彩度強調値。1.0は彩度を維持し、1より大きい値は彩度を高め、1より小さい値は彩度を下げます。
"bias : value for bias function in [0, 1] range. Values from 0.7 to 0.9 usually give best results, default value is 0.85.","bias : [0, 1]の範囲のバイアス関数の値。通常、0.7から0.9の値が最良の結果をもたらし、デフォルト値は0.85です。"
Adaptive logarithmic mapping is a fast global tonemapping algorithm that scales the image in logarithmic domain. ,Adaptive Logarithmic Mapping は、画像を対数領域でスケーリングする高速なグローバルトーンマッピングアルゴリズムです。
"Since it's a global operator the same function is applied to all the pixels, it is controlled by the bias parameter.",これはグローバルな演算子なので、すべてのピクセルに同じ関数が適用されます。これはバイアスパラメータで制御されます。
Optional saturation enhancement is possible as described in [71] .,オプションとして，[71]で説明されているように，彩度の向上が可能です．
For more information see [58] . ,詳細については，[58]を参照してください．
Creates TonemapReinhard object.,TonemapReinhardオブジェクトを作成します。
"intensity : result intensity in [-8, 8] range. Greater intensity produces brighter results.","intensity : [-8, 8]の範囲の結果の強度。強度が高いほど、明るい結果が得られます。"
"light_adapt : light adaptation in [0, 1] range. If 1 adaptation is based only on pixel value, if 0 it's global, otherwise it's a weighted mean of this two cases.","light_adapt : 光の適応度を [0, 1] の範囲で指定します。1の場合はピクセル値のみに基づいた適応、0の場合はグローバルな適応、それ以外の場合はこれら2つのケースの加重平均となります。"
"color_adapt : chromatic adaptation in [0, 1] range. If 1 channels are treated independently, if 0 adaptation level is the same for each channel.","color_adapt : [0, 1]の範囲での色調補正。1の場合、チャンネルは独立して処理され、0の場合、適応レベルは各チャンネルで同じになります。"
This is a global tonemapping operator that models human visual system. ,これは、人間の視覚システムをモデルとした、グローバルなトーンマッピング演算子です。
"Mapping function is controlled by adaptation parameter, that is computed using light adaptation and color adaptation.",マッピング機能は，光適応と色適応を用いて計算された適応パラメータによって制御されます．
For more information see [201] . ,詳細については、[201]を参照してください。
Creates TonemapMantiuk object.,TonemapMantiukオブジェクトを作成します。
"scale : contrast scale factor. HVS response is multiplied by this parameter, thus compressing dynamic range. Values from 0.6 to 0.9 produce best results.",scale : コントラストのスケールファクターです。HVSレスポンスにこのパラメータを乗じて、ダイナミックレンジを圧縮します。0.6～0.9程度の値が最適です。
"This algorithm transforms image to contrast using gradients on all levels of gaussian pyramid, transforms contrast values to HVS response and scales the response. After this the image is reconstructed from new contrast values. ",このアルゴリズムでは、ガウシアンピラミッドのすべてのレベルのグラデーションを使用して画像をコントラストに変換し、コントラスト値をHVSレスポンスに変換し、そのレスポンスをスケーリングします。その後，新しいコントラスト値から画像が再構成されます．
For more information see [161] . ,詳細は[161]を参照してください．
Compute the shape distance between two shapes defined by its contours.,輪郭で定義される2つの形状間の形状距離を計算します．
contour1 : Contour defining first shape.,contour1 : 最初の形状を定義する輪郭．
contour2 : Contour defining second shape.,contour2 : 第2の形状を定義する輪郭．
Implementation of the Shape Context descriptor and matching algorithm. ,Shape Context 記述子とマッチングアルゴリズムの実装。
"proposed by Belongie et al. in ""Shape Matching and Object Recognition Using Shape Contexts"" (PAMI 2002). This implementation is packaged in a generic scheme, in order to allow you the implementation of the common variations of the original pipeline. ","Belongieらが ""Shape Matching and Object Recognition Using Shape Contexts"" (PAMI 2002)で提案したものです。この実装は、オリジナルのパイプラインの一般的なバリエーションの実装を可能にするために、汎用的なスキームでパッケージ化されています。"
Establish the number of angular bins for the Shape Context Descriptor used in the shape matching pipeline.,形状マッチングパイプラインで使用されるShape Context Descriptorの角度ビンの数を設定する。
nAngularBins : The number of angular bins in the shape context descriptor.,nAngularBins : 形状コンテキスト記述子の角度ビンの数。
Establish the number of radial bins for the Shape Context Descriptor used in the shape matching pipeline.,形状マッチングパイプラインで使用されるShape Context DescriptorのRadial Binsの数を設定する。
nRadialBins : The number of radial bins in the shape context descriptor.,nRadialBins : 形状コンテキスト記述子の半径ビンの数。
Set the inner radius of the shape context descriptor.,形状コンテキスト記述子の内側半径を設定する。
innerRadius : The value of the inner radius.,innerRadius : 内側の半径の値です。
Set the outer radius of the shape context descriptor.,シェイプコンテキスト記述子の外半径を設定します。
outerRadius : The value of the outer radius.,outerRadius : 外側の半径の値．
"Set the weight of the shape context distance in the final value of the shape distance. The shape context distance between two shapes is defined as the symmetric sum of shape context matching costs over best matching points. The final value of the shape distance is a user-defined linear combination of the shape context distance, an image appearance distance, and a bending energy.",形状距離の最終値における形状コンテキスト距離の重みを設定する。2つの形状間の形状コンテクスト距離は、ベストマッチングポイントに対する形状コンテクストマッチングコストの対称的な合計として定義される。形状距離の最終値は、形状コンテキスト距離、画像出現距離、曲げエネルギーのユーザー定義の線形結合である。
shapeContextWeight : The weight of the shape context distance in the final distance value.,shapeContextWeight : 最終的な距離値における形状コンテキスト距離の重みを設定する。
"Set the weight of the Image Appearance cost in the final value of the shape distance. The image appearance cost is defined as the sum of squared brightness differences in Gaussian windows around corresponding image points. The final value of the shape distance is a user-defined linear combination of the shape context distance, an image appearance distance, and a bending energy. If this value is set to a number different from 0, is mandatory to set the images that correspond to each shape.",形状距離の最終値における画像出現コストの重みを設定する。画像外観コストは、対応する画像ポイントの周りのガウス窓における輝度差の二乗の合計として定義される。形状距離の最終値は、形状コンテキスト距離、画像外観距離、および曲げエネルギーのユーザー定義の線形結合です。この値が0以外の数値に設定された場合、各形状に対応する画像の設定が必須となる。
imageAppearanceWeight : The weight of the appearance cost in the final distance value.,imageAppearanceWeight : 最終的な距離値におけるアピアランスコストの重みを設定します。
"Set the weight of the Bending Energy in the final value of the shape distance. The bending energy definition depends on what transformation is being used to align the shapes. The final value of the shape distance is a user-defined linear combination of the shape context distance, an image appearance distance, and a bending energy.",形状距離の最終値における曲げエネルギーの重みを設定する。曲げエネルギーの定義は、形状の整列にどのような変換が使用されているかによって異なります。形状距離の最終値は、形状コンテキスト距離、画像外観距離、曲げエネルギーのユーザー定義の線形結合です。
bendingEnergyWeight : The weight of the Bending Energy in the final distance value.,bendingEnergyWeight : 最終的な距離値におけるベンディングエナジーの重み。
Set the images that correspond to each shape. This images are used in the calculation of the Image Appearance cost.,各形状に対応する画像を設定します。この画像は、Image Appearance costの計算に使用されます。
image1 : Image corresponding to the shape defined by contours1.,image1 : 輪郭1で定義された形状に対応する画像です。
image2 : Image corresponding to the shape defined by contours2.,image2 : 輪郭2で定義された形状に対応する画像。
Set the value of the standard deviation for the Gaussian window for the image appearance cost.,画像見栄えコストのためのガウス窓の標準偏差の値を設定します。
sigma : Standard Deviation.,sigma : 標準偏差です。
Examples: modules/shape/samples/shape_example.cpp.,例： modules/shape/samples/shape_example.cpp.
A simple Hausdorff distance measure between shapes defined by contours. ,輪郭で定義された形状間の単純なハウズドルフ距離測定。
"according to the paper ""Comparing Images using the Hausdorff distance."" by D.P. Huttenlocher, G.A. Klanderman, and W.J. Rucklidge. (PAMI 1993). : ","D.P. Huttenlocher, G.A. Klanderman, W.J. Rucklidge の論文 ""Comparing Images using the Hausdorff distance."" による。(PAMI 1993) を参照してください。"
Set the norm used to compute the Hausdorff value between two shapes. It can be L1 or L2 norm.,2つの形状間のハウズドルフ値の計算に使用するノルムを設定します。L1ノルムまたはL2ノルムを指定します。
"distanceFlag : Flag indicating which norm is used to compute the Hausdorff distance (NORM_L1, NORM_L2).","distanceFlag :どちらのノルムを用いてHausdorff距離を計算するかを示すフラグ (NORM_L1, NORM_L2)."
This method sets the rank proportion (or fractional value) that establish the Kth ranked value of the partial Hausdorff distance. Experimentally had been shown that 0.6 is a good value to compare shapes.,この方法では，部分ハウズドルフ距離のK番目のランク値を確定するランク比率（または分数値）を設定します．実験的には、0.6 が形状を比較するのに適した値であることがわかっています。
rankProportion : fractional value (between 0 and 1).,rankProportion : フラクショナル値（0以上1以下）。
Creates a Stitcher configured in one of the stitching modes.,ステッチモードの一つに設定されたStitcherを作成します。
mode : Scenario for stitcher operation. This is usually determined by source of images to stitch and their transformation. Default parameters will be chosen for operation in given scenario.,mode :ステッチャーの動作のシナリオ。ステッチする画像のソースとその変換によって決定されます。指定されたシナリオで動作するように、デフォルトのパラメータが選択されます。
High level image stitcher. ,ハイレベルイメージステッチャ。
"It's possible to use this class without being aware of the entire stitching pipeline. However, to be able to achieve higher stitching stability and quality of the final images at least being familiar with the theory is recommended.",このクラスは、ステッチングパイプライン全体を意識することなく使用することができます。しかし、より高いスティッチングの安定性と最終画像の品質を得るためには、少なくともその理論に精通していることが推奨されます。
A basic example on image stitching can be found at opencv_source_code/samples/cpp/stitching.cpp,画像のスティッチングの基本的な例は、opencv_source_code/samples/cpp/stitching.cpp にあります。
A basic example on image stitching in Python can be found at opencv_source_code/samples/python/stitching.py,Python でのイメージスティッチングの基本的な例は、opencv_source_code/samples/python/stitching.py にあります。
A detailed example on image stitching can be found at opencv_source_code/samples/cpp/stitching_detailed.cpp ,画像のスティッチングの詳細な例は、opencv_source_code/samples/cpp/stitching_detailed.cpp にあります。
These functions try to match the given images and to estimate rotations of each camera.,これらの関数は、与えられた画像のマッチングと、各カメラの回転の推定を試みます。
images : Input images.,画像．入力画像．
masks : Masks for each input image specifying where to look for keypoints (optional).,masks :各入力画像に対して，キーポイントを探す場所を指定するマスク（オプション）．
"NoteUse the functions only if you're aware of the stitching pipeline, otherwise use Stitcher::stitch.",注意これらの関数は，ステッチングパイプラインを意識している場合にのみ利用してください．
These functions try to compose the given images (or images stored internally from the other function calls) into the final pano under the assumption that the image transformations were estimated before.,これらの関数は，与えられた画像（あるいは，他の関数呼び出しによって内部に保存された画像）を，あらかじめ画像変換が推定されていると仮定して，最終的なパノに合成しようとします．
pano : Final pano.,pano : 最終的なパノ。
These functions try to stitch the given images.,これらの関数は，与えられた画像をスティッチしようとします．
Examples: samples/cpp/stitching_detailed.cpp.,例: samples/cpp/stitching_detailed.cpp.
Feature matchers base class. ,特徴照合器の基底クラス。
Frees unused memory allocated before if there is any.,以前に割り当てられた未使用のメモリがあれば，それを解放します．
Reimplemented in cv::detail::BestOf2NearestMatcher.,cv::detail::BestOf2NearestMatcher で再実装されています．
Features matcher which finds two best matches for each feature and leaves the best one only if the ratio between descriptor distances is greater than the threshold match_conf. ,各特徴量に対して2つのベストマッチを見つけ，ディスクリプタ間の距離の比が閾値 match_conf よりも大きい場合にのみ，ベストマッチを残す特徴量マッチャ．
See alsodetail::FeaturesMatcher ,関連項目：odetail::FeaturesMatcher
Reimplemented from cv::detail::FeaturesMatcher.,cv::detail::FeaturesMatcher を再実装したものです．
Features matcher similar to cv::detail::BestOf2NearestMatcher which finds two best matches for each feature and leaves the best one only if the ratio between descriptor distances is greater than the threshold match_conf. ,cv::detail::BestOf2NearestMatcher に似た特徴量 matcher で，各特徴量に対して2つのベストマッチを見つけ，記述子の距離の比が閾値 match_conf よりも大きい場合にのみ，ベストマッチを残します．
Unlike cv::detail::BestOf2NearestMatcher this matcher uses affine transformation (affine transformation estimate will be placed in matches_info).,cv::detail::BestOf2NearestMatcher とは異なり，この Matcher はアフィン変換を利用します（アフィン変換の推定値は matches_info に格納されます）．
See alsocv::detail::FeaturesMatcher cv::detail::BestOf2NearestMatcher ,関連項目： cv::detail::FeaturesMatcher， cv::detail::BestOf2NearestMatcher．
See alsosetPyrScale,alsosetPyrScale を参照してください．
See alsogetPyrScale,alsogetPyrScale を参照してください．
See alsosetLevelsNumber,alsosetLevelsNumber を参照してください．
See alsogetLevelsNumber,アルソージェットレベルズナンバー参照
See alsosetWindowSize,アルソセットウィンドウサイズを見る
See alsogetWindowSize,alsogetWindowSize参照
See alsosetPolyN,アルソセットポリンを見る
See alsogetPolyN,alsogetPolyNを参照
See alsosetPolySigma,アルソセットポリシグマを見る
See alsogetPolySigma,アルソゲートポリシグマ参照
See alsosetFlags,アルソセットフラッグスを見る
See alsogetFlags,alsogetFlagsを見る
See alsosetTau,アルソセットタウを見る
See alsogetTau,アルソージェットタウを見る
See alsosetLambda,アルソセットラムダを見る
See alsogetLambda,アルソゲトラムダを見る
See alsosetTheta,アルソセットシータを見る
See alsogetTheta,アルソゲトシータを参照
See alsosetScalesNumber,アルソセットスケールナンバーを見る
See alsogetScalesNumber,アルソジェットスケールナンバー参照
See alsosetWarpingsNumber,アルソセットワッピングナンバーを見る
See alsogetWarpingsNumber,alsogetWarpingsNumber参照
See alsosetEpsilon,アルソセットイプシロン参照
See alsogetEpsilon,alsogetEpsilonを参照
See alsosetUseInitialFlow,アルソセットユーズイニシャルフロー参照
See alsogetUseInitialFlow,alsogetUseInitialFlow参照
Flow smoothness.,フローの滑らかさ。
See alsosetAlpha,アルソセットアルファー参照
See alsogetAlpha,alsogetAlpha参照
Gradient constancy importance.,勾配の恒常性の重要性。
Pyramid scale factor.,ピラミッドのスケールファクター。
See alsosetScaleFactor,alsosetScaleFactorを参照してください。
See alsogetScaleFactor,alsogetScaleFactorを参照
Number of lagged non-linearity iterations (inner loop),遅れた非線形性の反復回数（内側ループ）
See alsosetInnerIterations,alsosetInnerIterationsを参照。
See alsogetInnerIterations,alsogetInnerIterationsを参照。
Number of warping iterations (number of pyramid levels),ワープの反復回数（ピラミッドのレベル数）
See alsosetOuterIterations,alsosetOuterIterationsを参照。
See alsogetOuterIterations,alsogetOuterIterationsを参照。
Number of linear system solver iterations.,線形システムソルバーのイテレーション数。
See alsosetSolverIterations,alsosetSolverIterationsを参照。
See alsogetSolverIterations,alsogetSolverIterationsを参照。
See alsosetMaxLevel,参照：アルソセットマックスレベル
See alsogetMaxLevel,alsogetMaxLevel を参照してください．
Implemented in cv::superres::SuperResolution.,cv::superres::SuperResolution で実装されています．
Set input frame source for Super Resolution algorithm.,超解像アルゴリズムの入力フレームソースを設定します．
frameSource : Input frame source,frameSource : 入力フレームソース．
Process next frame from input and return output result.,入力フレームの次のフレームを処理して，出力結果を返します．
frame : Output result,frame : 出力結果．
Implements cv::superres::FrameSource.,cv::superres::FrameSource を実装します．
Clear all inner buffers.,すべての内部バッファをクリアします．
Create Bilateral TV-L1 Super Resolution.,両側TV-L1超解像を作成します．
"This class implements Super Resolution algorithm described in the papers [70] and [175] .Here are important members of the class that control the algorithm, which you can set after constructing the class instance:int scale Scale factor.",このクラスは，論文 [70] と [175] で述べられている超解像アルゴリズムを実装しています．アルゴリズムを制御するクラスの重要なメンバは以下の通りで，クラスのインスタンスを生成した後に設定することができます．
int iterations Iteration count.,int iterations 反復回数．
double tau Asymptotic value of steepest descent method.,double tau 最急降下法の漸近値．
double lambda Weight parameter to balance data term and smoothness term.,double lambda data項とsmoothness項のバランスをとるための重みパラメータ．
double alpha Parameter of spacial distribution in Bilateral-TV.,double alpha Bilateral-TVでの空間分布のパラメータ。
int btvKernelSize Kernel size of Bilateral-TV filter.,int btvKernelSize Bilateral-TVフィルタのカーネルサイズ。
int blurKernelSize Gaussian blur kernel size.,int blurKernelSize ガウス・ブラーのカーネルサイズ。
double blurSigma Gaussian blur sigma.,double blurSigma ガウスぼかしのシグマ．
int temporalAreaRadius Radius of the temporal search area.,int temporalAreaRadius 時間軸方向の探索領域の半径を指定します。
Ptr<DenseOpticalFlowExt> opticalFlow Dense optical flow algorithm.,Ptr<DenseOpticalFlowExt> opticalFlow 密なオプティカルフローアルゴリズム．
Base class for Super Resolution algorithms. ,超解像アルゴリズムのベースクラスです．
The class is only used to define the common interface for the whole family of Super Resolution algorithms. ,このクラスは，超解像アルゴリズムのファミリー全体に共通するインターフェースを定義するためにのみ使用されます．
Scale factor.,スケールファクター。
See alsosetScale,alsosetScale参照
See alsogetScale,alsogetScaleを参照してください。
Iterations count.,反復回数.
Asymptotic value of steepest descent method.,最急降下法の漸近値。
Weight parameter to balance data term and smoothness term.,data項とsmoothness項のバランスをとるためのウェイトパラメータ。
Parameter of spacial distribution in Bilateral-TV.,Bilateral-TVの空間分布のパラメータです。
Kernel size of Bilateral-TV filter.,Bilateral-TV フィルタのカーネルサイズ．
See alsosetKernelSize,alsosetKernelSize を参照してください。
See alsogetKernelSize,alsogetKernelSize を参照してください。
Gaussian blur kernel size.,ガウスブラーのカーネルサイズ
See alsosetBlurKernelSize,alsosetBlurKernelSize参照。
See alsogetBlurKernelSize,alsogetBlurKernelSize参照
Gaussian blur sigma.,ガウスぼかしのシグマ。
See alsosetBlurSigma,参照：「alsosetBlurSigma
See alsogetBlurSigma,参照：「alsogetBlurSigma
Radius of the temporal search area.,時間軸方向の検索領域の半径。
See alsosetTemporalAreaRadius,参照：alsosetTemporalAreaRadius
See alsogetTemporalAreaRadius,alsogetTemporalAreaRadius を参照してください。
Dense optical flow algorithm.,密なオプティカルフローアルゴリズム。
See alsogetOpticalFlow,関連項目： alsogetOpticalFlow
Recognize text using the tesseract-ocr API.,tesseract-ocr APIを用いてテキストを認識します．
image : Input image CV_8UC1 or CV_8UC3,image : 入力画像 CV_8UC1 または CV_8UC3．
output_text : Output text of the tesseract-ocr.,output_text : tesseract-ocr の出力テキスト．
component_rects : If provided the method will output a list of Rects for the individual text elements found (e.g. words or text lines).,component_rects :これが指定された場合，このメソッドは検出された個々のテキスト要素（例えば，単語やテキストライン）に対するRectsのリストを出力します．
component_texts : If provided the method will output a list of text strings for the recognition of individual text elements found (e.g. words or text lines).,component_texts :提供された場合、このメソッドは、見つかった個々のテキスト要素（例えば、単語やテキストライン）の認識のためのテキスト文字列のリストを出力します。
component_confidences : If provided the method will output a list of confidence values for the recognition of individual text elements found (e.g. words or text lines).,component_confidences :提供された場合、このメソッドは、検出された個々のテキスト要素（例：単語やテキスト行）の認識に関する信頼値のリストを出力します。
"component_level : OCR_LEVEL_WORD (by default), or OCR_LEVEL_TEXTLINE.",component_level : OCR_LEVEL_WORD（デフォルト）または OCR_LEVEL_TEXTLINE。
"Takes image on input and returns recognized text in the output_text parameter. Optionally provides also the Rects for individual text elements found (e.g. words), and the list of those text elements with their confidence values.Implements cv::text::BaseOCR.",入力として画像を受け取り、output_textパラメータで認識したテキストを返します。オプションとして，検出された個々のテキスト要素（例えば，単語）のRectsと，それらのテキスト要素のリストとその信頼度の値も提供します．
Creates an instance of the OCRTesseract class. Initializes Tesseract.,OCRTesseract クラスのインスタンスを作成します．Tesseractを初期化します．
"datapath : the name of the parent directory of tessdata ended with ""/"", or NULL to use the system's default directory.","datapath : tessdataの親ディレクトリの名前で，""/""で終わるもの，またはシステムのデフォルトディレクトリを利用する場合はNULL．"
"language : an ISO 639-3 code or NULL will default to ""eng"".","language : ISO 639-3のコード、もしくはNULLであればデフォルトで ""eng ""になります。"
"char_whitelist : specifies the list of characters used for recognition. NULL defaults to ""0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ"".","char_whitelist : 認識に使用する文字のリストを指定する．NULLのデフォルトは ""0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ ""になります。"
"oem : tesseract-ocr offers different OCR Engine Modes (OEM), by default tesseract::OEM_DEFAULT is used. See the tesseract-ocr API documentation for other possible values.",oem : tesseract-ocrは異なるOCRエンジンモード(OEM)を提供しますが、デフォルトではtesseract::OEM_DEFAULTが使用されます。他の可能な値についてはtesseract-ocrのAPIドキュメントを参照してください。
psmode : tesseract-ocr offers different Page Segmentation Modes (PSM) tesseract::PSM_AUTO (fully automatic layout analysis) is used. See the tesseract-ocr API documentation for other possible values.,psmode : tesseract-ocrは異なるページ分割モード(PSM)を提供します。デフォルトではtesseract::PSM_AUTO(完全自動レイアウト分析)が使用されます。他の可能な値についてはtesseract-ocrのAPIドキュメントを参照してください。
OCRTesseract class provides an interface with the tesseract-ocr API (v3.02.02) in C++. ,OCRTesseractクラスはC++でtesseract-ocr API (v3.02.02)とのインターフェイスを提供します。
Notice that it is compiled only when tesseract-ocr is correctly installed.,これはtesseract-ocrが正しくインストールされている場合にのみコンパイルされることに注意してください。
(C++) An example of OCRTesseract recognition combined with scene text detection can be found at the end_to_end_recognition demo: https://github.com/opencv/opencv_contrib/blob/master/modules/text/samples/end_to_end_recognition.cpp,(C++) シーンテキスト検出と組み合わせたOCRTesseract認識の例は、end_to_end_recognitionデモで見ることができます： https://github.com/opencv/opencv_contrib/blob/master/modules/text/samples/end_to_end_recognition.cpp
(C++) Another example of OCRTesseract recognition combined with scene text detection can be found at the webcam_demo: https://github.com/opencv/opencv_contrib/blob/master/modules/text/samples/webcam_demo.cpp ,(C++) シーンテキスト検出と組み合わせたOCRTesseract認識の別の例は、Webcam_demo: https://github.com/opencv/opencv_contrib/blob/master/modules/text/samples/webcam_demo.cpp で見ることができます。
"Applies the Stroke Width Transform operator followed by filtering of connected components of similar Stroke Widths to return letter candidates. It also chain them by proximity and size, saving the result in chainBBs.",ストローク幅変換演算子を適用した後、類似したストローク幅の接続されたコンポーネントのフィルタリングを行い、文字候補を返します。また，それらを近接度とサイズで連鎖させ，結果を chainBB に保存する．
input : the input image with 3 channels.,input : 3チャンネルの入力画像．
result : a vector of resulting bounding boxes where probability of finding text is high,result : 文字が見つかる確率の高いバウンディングボックスの結果のベクトル
"dark_on_light : a boolean value signifying whether the text is darker or lighter than the background, it is observed to reverse the gradient obtained from Scharr operator, and significantly affect the result.",dark_on_light : テキストが背景よりも暗いか明るいかを示す真偽値．これは，シャール演算子で得られたグラデーションを反転させ，結果に大きな影響を与えることが分かります．
draw : an optional Mat of type CV_8UC3 which visualises the detected letters using bounding boxes.,draw : CV_8UC3型のオプションのMatで，検出された文字をバウンディングボックスを用いて可視化します．
chainBBs : an optional parameter which chains the letter candidates according to heuristics in the paper and returns all possible regions where text is likely to occur.,chainBBs : オプションのパラメータ．このパラメータは，論文中のヒューリスティックな手法に従って文字候補をチェーン化し，テキストが存在する可能性のあるすべての領域を返します．
Method that provides a quick and simple interface to detect text inside an image.,画像内のテキストを検出するための迅速かつシンプルなインターフェースを提供するメソッド。
inputImage : an image to process,inputImage : 処理する画像
Bbox : a vector of Rect that will store the detected word bounding box,Bbox : 検出された単語のバウンディングボックスを格納するRectのベクトル
confidence : a vector of float that will be updated with the confidence the classifier has for the selected bounding box,confidence : 選択されたバウンディングボックスに対する分類器の信頼度が更新される float 型のベクトル．
Implemented in cv::text::TextDetectorCNN.,cv::text::TextDetectorCNN で実装されています．
inputImage : an image expected to be a CV_U8C3 of any size,inputImage : 任意のサイズの CV_U8C3 であることが期待される画像．
Implements cv::text::TextDetector.,cv::text::TextDetector を実装しています．
Creates an instance of the TextDetectorCNN class using the provided parameters.,与えられたパラメータを用いて，TextDetectorCNN クラスのインスタンスを作成します．
modelArchFilename : the relative or absolute path to the prototxt file describing the classifiers architecture.,modelArchFilename : 分類器のアーキテクチャを記述した prototxt ファイルへの相対パス，または絶対パス．
modelWeightsFilename : the relative or absolute path to the file containing the pretrained weights of the model in caffe-binary form.,modelWeightsFilename : プリトレーニングされたモデルの重みをcaffe-binary形式で格納したファイルへの相対パスまたは絶対パスです．
"detectionSizes : a list of sizes for multiscale detection. The values[(300,300),(700,500),(700,300),(700,700),(1600,1600)] are recommended in [146] to achieve the best quality.","detectionSizes : マルチスケール検出のためのサイズのリスト．最高の品質を得るために，値[(300,300),(700,500),(700,300),(700,700),(1600,1600)]が[146]で推奨されています．"
TextDetectorCNN class provides the functionallity of text bounding box detection. This class is representing to find bounding boxes of text words given an input image. This class uses OpenCV dnn module to load pre-trained model described in [146]. The original repository with the modified SSD Caffe version: https://github.com/MhLiao/TextBoxes. Model can be downloaded from DropBox. Modified .prototxt file with the model description can be found in opencv_contrib/modules/text/samples/textbox.prototxt. ,TextDetectorCNNクラスは、テキストのバウンディングボックスを検出する機能を提供します。このクラスは，入力画像に含まれるテキスト単語のバウンディングボックスを検出するためのものです．このクラスは，OpenCV dnnモジュールを使用して，[146]に記載されている事前学習済みモデルをロードします．SSDのCaffeバージョンを修正したオリジナルのリポジトリ： https://github.com/MhLiao/TextBoxes.モデルはDropBoxからダウンロードできます。モデルの説明を含む修正された.prototxtファイルは、opencv_contrib/modules/text/samples/textbox.prototxtにあります。
Create KCF tracker instance.,KCFトラッカーのインスタンスを作成します。
parameters : KCF parameters TrackerKCF::Params,parameters : KCFパラメータ TrackerKCF::Params
the KCF (Kernelized Correlation Filter) tracker ,KCF（Kernelized Correlation Filter）トラッカー
"KCF is a novel tracking framework that utilizes properties of circulant matrix to enhance the processing speed. This tracking method is an implementation of [108] which is extended to KCF with color-names features ([51]). The original paper of KCF is available at http://www.robots.ox.ac.uk/~joao/publications/henriques_tpami2015.pdf as well as the matlab implementation. For more information about KCF with color-names features, please refer to http://www.cvl.isy.liu.se/research/objrec/visualtracking/colvistrack/index.html. ",KCFは，サーキュラント行列の特性を利用して処理速度を向上させる，新しいトラッキングフレームワークです．このトラッキング手法は，[108]の実装であり，それを色名の特徴を持つKCFに拡張したものです（[51]）．KCFの原論文は、http://www.robots.ox.ac.uk/~joao/publications/henriques_tpami2015.pdfで公開されており、matlabの実装も公開されています。色名の特徴を持つKCFについては、http://www.cvl.isy.liu.se/research/objrec/visualtracking/colvistrack/index.html を参照してください。
Create CSRT tracker instance.,CSRTのトラッカーインスタンスを作成します。
parameters : CSRT parameters TrackerCSRT::Params,parameters : CSRTパラメータ TrackerCSRT::Params
the CSRT tracker ,CSRTトラッカー
The implementation is based on [156] Discriminative Correlation Filter with Channel and Spatial Reliability ,実装は、[156] Discriminative Correlation Filter with Channel and Spatial Reliability に基づいています。
Computes a background image.,背景画像を計算します。
backgroundImage : The output background image.,backgroundImage : 出力される背景画像．
"NoteSometimes the background image can be very blurry, as it contain the average background statistics.Implemented in cv::bgsegm::BackgroundSubtractorLSBP, cv::bgsegm::BackgroundSubtractorGSOC, and cv::bgsegm::BackgroundSubtractorCNT.","CV::bgsegm::BackgroundSubtractorLSBP, cv::bgsegm::BackgroundSubtractorGSOC, および cv::bgsegm::BackgroundSubtractorCNT で実装されています．"
Computes a foreground mask.,前景マスクを計算します．
image : Next video frame.,image : 次のビデオフレーム．
fgmask : The output foreground mask as an 8-bit binary image.,fgmask : 8ビットのバイナリ画像として出力される前景マスク．
"learningRate : The value between 0 and 1 that indicates how fast the background model is learnt. Negative parameter value makes the algorithm to use some automatically chosen learning rate. 0 means that the background model is not updated at all, 1 means that the background model is completely reinitialized from the last frame.",learningRate :背景モデルの学習速度を示す0から1の値．負のパラメータ値を設定すると、アルゴリズムは自動的に選択された学習速度を使用するようになります。0 は，背景モデルが全く更新されないことを意味し，1 は，背景モデルが最後のフレームから完全に再初期化されることを意味します．
"Implemented in cv::bgsegm::BackgroundSubtractorLSBP, cv::bgsegm::BackgroundSubtractorGSOC, cv::BackgroundSubtractorMOG2, and cv::bgsegm::BackgroundSubtractorCNT.","cv::bgsegm::BackgroundSubtractorLSBP, cv::bgsegm::BackgroundSubtractorGSOC, cv::BackgroundSubtractorMOG2, および cv::bgsegm::BackgroundSubtractorCNT で実装されています．"
Base class for background/foreground segmentation. : ,背景/前景セグメンテーションの基本クラス．
The class is only used to define the common interface for the whole family of background/foreground segmentation algorithms. ,このクラスは，背景/前景セグメンテーションアルゴリズム群全体に共通するインタフェースを定義するためにのみ利用されます．
Creates MOG2 Background Subtractor.,MOG2 Background Subtractorを作成します。
varThreshold : Threshold on the squared Mahalanobis distance between the pixel and the model to decide whether a pixel is well described by the background model. This parameter does not affect the background update.,varThreshold : 画素が背景モデルによって十分に記述されているかどうかを判断するための、画素とモデルの間のマハラノビス距離の二乗に対するしきい値。このパラメータは、背景の更新には影響しません。
"detectShadows : If true, the algorithm will detect shadows and mark them. It decreases the speed a bit, so if you do not need this feature, set the parameter to false.",detectShadows :trueの場合、アルゴリズムは影を検出してマークします。速度が少し低下するので、この機能が不要な場合は、パラメータをfalseに設定してください。
Examples: samples/cpp/segment_objects.cpp.,例： samples/cpp/segment_objects.cpp.
The class implements the Gaussian mixture model background subtraction described in [299] and [298] . ,このクラスは，[299]および[298]で述べられているガウス混合モデルによる背景減算を実装しています．
Returns the number of last frames that affect the background model.,背景モデルに影響を与える最後のフレームの数を返します。
Returns the number of gaussian components in the background model.,背景モデルに含まれるガウス成分の数を返します。
Sets the number of gaussian components in the background model.,背景モデルのガウシアン成分の数を設定します。
The model needs to be reinitalized to reserve memory.,メモリを確保するためにモデルを再起動する必要がある。
"Returns the ""background ratio"" parameter of the algorithm.","アルゴリズムの ""背景比率 ""パラメーターを返します。"
"If a foreground pixel keeps semi-constant value for about backgroundRatio*history frames, it's considered background and added to the model as a center of a new component. It corresponds to TB parameter in the paper.",前景のピクセルが backgroundRatio*history フレーム程度の間、半一定の値を維持する場合、そのピクセルは背景とみなされ、新しい成分の中心としてモデルに追加されます。これは論文中のTBパラメータに相当します。
"Sets the ""background ratio"" parameter of the algorithm.","アルゴリズムの ""background ratio ""パラメータを設定します。"
Returns the variance threshold for the pixel-model match.,ピクセルとモデルのマッチングのための分散しきい値を返します。
The main threshold on the squared Mahalanobis distance to decide if the sample is well described by the background model or not. Related to Cthr from the paper.,サンプルが背景モデルによってよく記述されているかどうかを判断するための、二乗マハラノビス距離に関する主な閾値です。論文中の Cthr に関連しています。
Sets the variance threshold for the pixel-model match.,pixel-model match用の分散しきい値を設定します。
Returns the variance threshold for the pixel-model match used for new mixture component generation.,Return the variance threshold for the pixel-model match used for new mixture component generation.
"Threshold for the squared Mahalanobis distance that helps decide when a sample is close to the existing components (corresponds to Tg in the paper). If a pixel is not close to any component, it is considered foreground or added as a new component. 3 sigma => Tg=3*3=9 is default. A smaller Tg value generates more components. A higher Tg value may result in a small number of components but they can grow too large.",Threshold for squared Mahalanobis distance これは、サンプルが既存の成分に近いかどうかを判断するのに役立ちます（論文中の Tg に対応）。あるピクセルがどのコンポーネントにも近接していない場合、そのピクセルは前景とみなされ、新しいコンポーネントとして追加されます。3シグマ => Tg=3*3=9がデフォルトです。Tgの値を小さくすると、より多くのコンポーネントが生成されます。Tg値を大きくすると、成分の数は少なくても、大きくなりすぎてしまうことがあります。
Sets the variance threshold for the pixel-model match used for new mixture component generation.,新しい混合成分の生成に使われるピクセルとモデルのマッチングのための分散のしきい値を設定します。
Returns the initial variance of each gaussian component.,Return the initial variance of each gaussian components. （各ガウス成分の初期分散を返します。
Sets the initial variance of each gaussian component.,各ガウシアンコンポーネントの初期分散を設定します。
Returns the complexity reduction threshold.,複雑さを軽減するための閾値を返します。
This parameter defines the number of samples needed to accept to prove the component exists. CT=0.05 is a default value for all the samples. By setting CT=0 you get an algorithm very similar to the standard Stauffer&Grimson algorithm.,このパラメータは、成分の存在を証明するために受け入れるのに必要なサンプル数を定義します。CT=0.05は、すべてのサンプルに対するデフォルト値です。CT=0を設定することで、標準的なStauffer&Grimsonアルゴリズムに非常に似たアルゴリズムが得られます。
Sets the complexity reduction threshold.,複雑さ軽減のしきい値を設定します。
Returns the shadow detection flag.,影の検出フラグを返します。
"If true, the algorithm detects shadows and marks them. See createBackgroundSubtractorMOG2 for details.",trueの場合，アルゴリズムはシャドウを検出し，それをマークします。詳細は createBackgroundSubtractorMOG2 を参照してください。
Enables or disables shadow detection.,影の検出を有効または無効にします。
Returns the shadow value.,影の値を返します。
"Shadow value is the value used to mark shadows in the foreground mask. Default value is 127. Value 0 in the mask always means background, 255 means foreground.",シャドウ値とは、フォアグラウンドマスクでシャドウをマークするために使われる値です。デフォルト値は 127 です。マスク内の値0は常に背景を意味し、255は前景を意味します。
Sets the shadow value.,シャドー値を設定します。
Returns the shadow threshold.,影のしきい値を返します。
"A shadow is detected if pixel is a darker version of the background. The shadow threshold (Tau in the paper) is a threshold defining how much darker the shadow can be. Tau= 0.5 means that if a pixel is more than twice darker then it is not shadow. See Prati, Mikic, Trivedi and Cucchiara, Detecting Moving Shadows...*, IEEE PAMI,2003.","シャドウは、ピクセルが背景よりも暗くなっている場合に検出されます。シャドウしきい値（論文では Tau）は、シャドウがどれだけ暗くなるかを定義するしきい値です。Tau=0.5 は，ピクセルが 2 倍以上暗くなった場合，シャドウではないことを意味します。Prati, Mikic, Trivedi, Cucchiara, Detecting Moving Shadows...*, IEEE PAMI,2003を参照。"
Sets the shadow threshold.,影のしきい値を設定します。
Creates KNN Background Subtractor.,KNN背景減算器を作成します。
dist2Threshold : Threshold on the squared distance between the pixel and the sample to decide whether a pixel is close to that sample. This parameter does not affect the background update.,dist2Threshold : ピクセルがそのサンプルに近いかどうかを判断するための、ピクセルとサンプルの間の二乗距離に対するしきい値。このパラメータは、背景の更新には影響しません。
K-nearest neighbours - based Background/Foreground Segmentation Algorithm. ,K-nearest neighboursに基づく背景/森林のセグメンテーションアルゴリズム。
The class implements the K-nearest neighbours background subtraction described in [298] . Very efficient if number of foreground pixels is low. ,このクラスは，[298]で説明されているK-nearest neighboursによる背景減算を実装しています．前景ピクセルの数が少ない場合は，非常に効率的です．
Sets the number of last frames that affect the background model.,背景モデルに影響を与える最後のフレームの数を設定します。
Returns the number of data samples in the background model.,背景モデルのデータサンプルの数を返します。
Sets the number of data samples in the background model.,背景モデルのデータサンプルの数を設定します。
Returns the threshold on the squared distance between the pixel and the sample.,画素とサンプルの間の二乗距離の閾値を返します。
The threshold on the squared distance between the pixel and the sample to decide whether a pixel is close to a data sample.,ピクセルがデータサンプルに近いかどうかを判断するための、ピクセルとサンプルの間の二乗距離のしきい値です。
Sets the threshold on the squared distance.,二乗距離の閾値を設定します。
"Returns the number of neighbours, the k in the kNN.",kNNのkである隣人の数を返します。
K is the number of samples that need to be within dist2Threshold in order to decide that that pixel is matching the kNN background model.,Kは、そのピクセルがkNN背景モデルにマッチしていると判断するために、dist2Threshold以内に収まる必要があるサンプルの数です。
Sets the k in the kNN. How many nearest neighbours need to match.,kNNのkを設定します。何個の最近傍が一致する必要があるかです。
"If true, the algorithm detects shadows and marks them. See createBackgroundSubtractorKNN for details.",trueの場合、アルゴリズムは影を検出してマークします。詳細は createBackgroundSubtractorKNN を参照してください。
"Finds an object center, size, and orientation.",オブジェクトの中心、サイズ、方向を見つける。
probImage : Back projection of the object histogram. See calcBackProject.,probImage : オブジェクトヒストグラムのバックプロジェクション．calcBackProjectを参照してください．
window : Initial search window.,window : 初期検索ウィンドウ．
"criteria : Stop criteria for the underlying meanShift. returns (in old interfaces) Number of iterations CAMSHIFT took to converge The function implements the CAMSHIFT object tracking algorithm [33] . First, it finds an object center using meanShift and then adjusts the window size and finds the optimal rotation. The function returns the rotated rectangle structure that includes the object position, size, and orientation. The next position of the search window can be obtained with RotatedRect::boundingRect()",criteria :returns (in old interfaces) CAMSHIFT が収束するまでに要した反復回数． この関数は，CAMSHIFT オブジェクトトラッキングアルゴリズム [33]を実装しています．まず，meanShift を利用してオブジェクトの中心を求め，ウィンドウサイズを調整して最適な回転を求めます．この関数は，オブジェクトの位置，サイズ，向きを含む，回転した矩形構造を返します．探索窓の次の位置は， RotatedRect::boundingRect() で得ることができます．
See the OpenCV sample camshiftdemo.c that tracks colored objects.Note,色付きオブジェクトを追跡する OpenCV サンプル camshiftdemo.c を参照してください．
(Python) A sample explaining the camshift tracking algorithm can be found at opencv_source_code/samples/python/camshift.pyExamples: samples/cpp/camshiftdemo.cpp.,(Python）カムシフト・トラッキング・アルゴリズムを説明するサンプルは， opencv_source_code/samples/python/camshift.pyExamples: samples/cpp/camshiftdemo.cpp にあります．
Finds an object on a back projection image.,バックプロジェクション画像上のオブジェクトを検索します。
probImage : Back projection of the object histogram. See calcBackProject for details.,probImage : オブジェクトのヒストグラムの逆投影画像．詳細は，calcBackProjectを参照してください．
"criteria : Stop criteria for the iterative search algorithm. returns : Number of iterations CAMSHIFT took to converge. The function implements the iterative object search algorithm. It takes the input back projection of an object and the initial position. The mass center in window of the back projection image is computed and the search window center shifts to the mass center. The procedure is repeated until the specified number of iterations criteria.maxCount is done or until the window center shifts by less than criteria.epsilon. The algorithm is used inside CamShift and, unlike CamShift , the search window size or orientation do not change during the search. You can simply pass the output of calcBackProject to this function. But better results can be obtained if you pre-filter the back projection and remove the noise. For example, you can do this by retrieving connected components with findContours , throwing away contours with small area ( contourArea ), and rendering the remaining contours with drawContours.",criteria :returns : CAMSHIFTが収束するまでに要した反復回数．この関数は，反復型オブジェクト探索アルゴリズムを実装しています．この関数は，物体の背面投影図と初期位置を入力とします．バックプロジェクション画像のウィンドウ内の質量中心が計算され，探索ウィンドウの中心が質量中心に移動します．この手順は、指定された反復回数 criteria.maxCount が終了するまで、または窓の中心が criteria.epsilon よりも小さくシフトするまで繰り返されます。このアルゴリズムはCamShiftの内部で使用されており、CamShiftとは異なり、探索中に探索窓のサイズや方向が変化することはありません。単純に calcBackProject の出力をこの関数に渡すことができます。しかし、バックプロジェクションを事前にフィルタリングしてノイズを除去すると、より良い結果が得られます。例えば，findContours で連結成分を取得し，面積の小さい輪郭 ( contourArea ) を捨て，残りの輪郭を drawContours で描画することで，これを行うことができます．
Constructs the image pyramid which can be passed to calcOpticalFlowPyrLK.,calcOpticalFlowPyrLK に渡すことのできる画像ピラミッドを構築します．
img : 8-bit input image.,img : 8ビットの入力画像．
pyramid : output pyramid.,pyramid : 出力ピラミッド．
winSize : window size of optical flow algorithm. Must be not less than winSize argument of calcOpticalFlowPyrLK. It is needed to calculate required padding for pyramid levels.,winSize : オプティカルフローアルゴリズムのウィンドウサイズ．calcOpticalFlowPyrLKの引数winSize以上でなければならない．ピラミッドのレベルに必要なパディングを計算するために必要です．
maxLevel : 0-based maximal pyramid level number.,maxLevel : 0ベースのピラミッドレベルの最大値．
withDerivatives : set to precompute gradients for the every pyramid level. If pyramid is constructed without the gradients then calcOpticalFlowPyrLK will calculate them internally.,withDerivatives : ピラミッドの各レベルのグラデーションを事前に計算するように設定します．もしピラミッドがグラデーションなしで構築された場合は、calcOpticalFlowPyrLKが内部でグラデーションを計算します。
pyrBorder : the border mode for pyramid layers.,pyrBorder : ピラミッド層の境界線のモードを指定します。
derivBorder : the border mode for gradients.,derivBorder : グラデーションの境界線モードです。
tryReuseInputImage : put ROI of input image into the pyramid if possible. You can pass false to force data copying.,tryReuseInputImage : 可能であれば、入力画像のROIをピラミッドに入れる。falseを渡せば強制的にデータをコピーします。
Calculates an optical flow for a sparse feature set using the iterative Lucas-Kanade method with pyramids.,ピラミッドを用いた反復ルーカス-カナード法を用いて，疎な特徴セットに対するオプティカルフローを計算します．
prevImg : first 8-bit input image or pyramid constructed by buildOpticalFlowPyramid.,prevImg : 最初の8ビット入力画像，あるいはbuildOpticalFlowPyramidで作られたピラミッド．
nextImg : second input image or pyramid of the same size and the same type as prevImg.,nextImg : prevImg と同じサイズ、同じタイプの2番目の入力画像またはピラミッド。
prevPts : vector of 2D points for which the flow needs to be found; point coordinates must be single-precision floating-point numbers.,prevPts : 流れを見つける必要のある2次元点のベクトル．
"nextPts : output vector of 2D points (with single-precision floating-point coordinates) containing the calculated new positions of input features in the second image; when OPTFLOW_USE_INITIAL_FLOW flag is passed, the vector must have the same size as in the input.",nextPts : 2枚目の画像における入力特徴の新しい位置を計算した2次元点（単精度浮動小数点座標）の出力ベクトル；OPTFLOW_USE_INITIAL_FLOWフラグが渡された場合、このベクトルは入力と同じサイズでなければならない。
"status : output status vector (of unsigned chars); each element of the vector is set to 1 if the flow for the corresponding features has been found, otherwise, it is set to 0.",status : 出力状態を表すベクトル（符号なし文字）；このベクトルの各要素は，対応する特徴量に対するフローが見つかった場合には1にセットされ，そうでない場合には0にセットされる．
"err : output vector of errors; each element of the vector is set to an error for the corresponding feature, type of the error measure can be set in flags parameter; if the flow wasn't found then the error is not defined (use the status parameter to find such cases).",err : エラーの出力ベクトル; ベクトルの各要素は、対応する特徴のエラーに設定され、エラーの種類はflagsパラメータで設定できる。
winSize : size of the search window at each pyramid level.,winSize : 各ピラミッドレベルでの検索ウィンドウのサイズ．
"maxLevel : 0-based maximal pyramid level number; if set to 0, pyramids are not used (single level), if set to 1, two levels are used, and so on; if pyramids are passed to input then algorithm will use as many levels as pyramids have but no more than maxLevel.",maxLevel : 0を基準としたピラミッドレベルの最大値．0を設定するとピラミッドを使用せず（1レベル），1を設定すると2レベルを使用し，以下同様とする．ピラミッドが入力に渡された場合，アルゴリズムはピラミッドが持つレベルの数だけ使用するが，maxLevelを超えることはない．
"criteria : parameter, specifying the termination criteria of the iterative search algorithm (after the specified maximum number of iterations criteria.maxCount or when the search window moves by less than criteria.epsilon.",criteria : パラメータ．反復検索アルゴリズムの終了基準を指定する（指定された最大反復回数 criteria.maxCount の後，あるいは検索窓の移動量が criteria.epsilon よりも小さくなったとき）．
flags : operation flags:,flags : 操作フラグ。
"OPTFLOW_USE_INITIAL_FLOW uses initial estimations, stored in nextPts; if the flag is not set, then prevPts is copied to nextPts and is considered the initial estimate.",OPTFLOW_USE_INITIAL_FLOW は nextPts に格納されている初期推定値を使用します。このフラグが設定されていない場合、prevPts は nextPts にコピーされ、初期推定値とみなされます。
"OPTFLOW_LK_GET_MIN_EIGENVALS use minimum eigen values as an error measure (see minEigThreshold description); if the flag is not set, then L1 distance between patches around the original and a moved point, divided by number of pixels in a window, is used as a error measure.",OPTFLOW_LK_GET_MIN_EIGENVALS エラー指標として最小固有値を使用します (minEigThresholdの説明を参照してください); フラグが設定されていない場合、オリジナルと移動した点の周りのパッチ間のL1距離をウィンドウ内のピクセル数で割ったものがエラー指標として使用されます。
"minEigThreshold : the algorithm calculates the minimum eigen value of a 2x2 normal matrix of optical flow equations (this matrix is called a spatial gradient matrix in [30]), divided by number of pixels in a window; if this value is less than minEigThreshold, then a corresponding feature is filtered out and its flow is not processed, so it allows to remove bad points and get a performance boost.",minEigThreshold : このアルゴリズムは，オプティカルフロー方程式の2x2正規行列（この行列は，[30]では空間勾配行列と呼ばれています）の最小固有値を，ウィンドウ内のピクセル数で割った値を計算します．この値が minEigThreshold よりも小さい場合，対応する特徴がフィルタリングされてそのフローが処理されないので，悪い点を削除して性能を向上させることができます．
The function implements a sparse iterative version of the Lucas-Kanade optical flow in pyramids. See [30] . The function is parallelized with the TBB library.NoteAn example using the Lucas-Kanade optical flow algorithm can be found at opencv_source_code/samples/cpp/lkdemo.cpp,この関数は，Lucas-Canade オプティカルフローの疎な反復版をピラミッド型に実装したものです．30]を参照してください．注）Lucas-Kanadeオプティカルフローアルゴリズムを用いた例は，opencv_source_code/samples/cpp/lkdemo.cppにあります．
(Python) An example using the Lucas-Kanade optical flow algorithm can be found at opencv_source_code/samples/python/lk_track.py,(Python) Lucas-Canade オプティカルフローアルゴリズムを用いた例は， opencv_source_code/samples/python/lk_track.py にあります．
(Python) An example using the Lucas-Kanade tracker for homography matching can be found at opencv_source_code/samples/python/lk_homography.pyExamples: samples/cpp/lkdemo.cpp.,(Python) Lucas-Kanade トラッカーを用いたホモグラフィマッチングの例は、opencv_source_code/samples/python/lk_homography.pyにあります。 サンプル: samples/cpp/lkdemo.cpp.
Computes a dense optical flow using the Gunnar Farneback's algorithm.,Gunnar Farneback のアルゴリズムを用いて密なオプティカルフローを計算します。
prev : first 8-bit single-channel input image.,prev : 8ビットシングルチャンネルの最初の入力画像.
next : second input image of the same size and the same type as prev.,next : prev と同じサイズ，同じ型の 2 番目の入力画像．
flow : computed flow image that has the same size as prev and type CV_32FC2.,flow : prev と同じサイズ，タイプが CV_32FC2 である，計算されたフロー画像．
"pyr_scale : parameter, specifying the image scale (<1) to build pyramids for each image; pyr_scale=0.5 means a classical pyramid, where each next layer is twice smaller than the previous one.",pyr_scale : パラメータ．各画像に対してピラミッドを構築する際の画像スケール（<1）を指定します．pyr_scale=0.5 は，次の層が前の層よりも2倍小さくなる古典的なピラミッドを意味します．
levels : number of pyramid layers including the initial image; levels=1 means that no extra layers are created and only the original images are used.,levels : 初期画像を含むピラミッドの層数; levels=1は，余分な層を作らず，元の画像のみを利用することを意味します．
"winsize : averaging window size; larger values increase the algorithm robustness to image noise and give more chances for fast motion detection, but yield more blurred motion field.",winsize : 平均化窓のサイズ。値を大きくすると、画像ノイズに対するアルゴリズムのロバスト性が高まり、高速な動き検出が可能になりますが、動きのあるフィールドがぼやけてしまいます。
iterations : number of iterations the algorithm does at each pyramid level.,iterations : アルゴリズムが各ピラミッドレベルで行う反復の回数。
"poly_n : size of the pixel neighborhood used to find polynomial expansion in each pixel; larger values mean that the image will be approximated with smoother surfaces, yielding more robust algorithm and more blurred motion field, typically poly_n =5 or 7.",poly_n : 各ピクセルの多項式展開を見つけるために使われるピクセル近傍のサイズ。大きな値は、画像がより滑らかな面で近似されることを意味し、よりロバストなアルゴリズムとよりぼやけたモーションフィールドが得られます（通常は poly_n = 5 または 7）。
"poly_sigma : standard deviation of the Gaussian that is used to smooth derivatives used as a basis for the polynomial expansion; for poly_n=5, you can set poly_sigma=1.1, for poly_n=7, a good value would be poly_sigma=1.5.",poly_sigma : 多項式展開の基礎となる導関数を平滑化するために使用されるガウスの標準偏差。poly_n=5の場合はpoly_sigma=1.1、poly_n=7の場合はpoly_sigma=1.5とするのが良い。
flags : operation flags that can be a combination of the following:,flags : 操作フラグで、以下のような組み合わせが可能です。
OPTFLOW_USE_INITIAL_FLOW uses the input flow as an initial flow approximation.,OPTFLOW_USE_INITIAL_FLOW 入力フローを初期フロー近似値として使用します。
"OPTFLOW_FARNEBACK_GAUSSIAN uses the Gaussian \(\texttt{winsize}\times\texttt{winsize}\) filter instead of a box filter of the same size for optical flow estimation; usually, this option gives z more accurate flow than with a box filter, at the cost of lower speed; normally, winsize for a Gaussian window should be set to a larger value to achieve the same level of robustness.",OPTFLOW_FARNEBACK_GAUSSIAN は，オプティカルフロー推定に，同じサイズのボックスフィルタの代わりに，ガウスフィルタを用います．通常，このオプションは，ボックスフィルタを用いた場合よりも正確なフローを与えますが，速度は低下します．通常，同じレベルのロバスト性を得るためには，ガウス窓の winsize をより大きな値に設定する必要があります．
"The function finds an optical flow for each prev pixel using the [69] algorithm so that\[\texttt{prev} (y,x) \sim \texttt{next} ( y + \texttt{flow} (y,x)[1], x + \texttt{flow} (y,x)[0])\]NoteAn example using the optical flow algorithm described by Gunnar Farneback can be found at opencv_source_code/samples/cpp/fback.cpp","この関数は，[69] のアルゴリズムを用いて，各 prev ピクセルに対するオプティカルフローを求めます．(y,x)[1], x + \\(y,x)[0])＼(^o^)／注Gunnar Farneback氏のオプティカルフローアルゴリズムを用いた例はopencv_source_code/samples/cpp/fback.cppにあります。"
(Python) An example using the optical flow algorithm described by Gunnar Farneback can be found at opencv_source_code/samples/python/opt_flow.py,(Python) Gunnar Farneback 氏のオプティカルフローアルゴリズムを使用した例は、opencv_source_code/samples/python/opt_flow.py にあります。
Computes the Enhanced Correlation Coefficient value between two images [67] .,2つの画像間の強化相関係数を計算します [67] ．
templateImage : single-channel template image; CV_8U or CV_32F array.,templateImage : シングルチャンネルのテンプレート画像，CV_8U または CV_32F の配列．
"inputImage : single-channel input image to be warped to provide an image similar to templateImage, same type as templateImage.",inputImage : templateImage と同様の画像を得るためにワープされる，シングルチャンネルの入力画像．
inputMask : An optional mask to indicate valid values of inputImage.,inputMask : inputImage の有効な値を示す，オプションのマスク．
See alsofindTransformECC,参照：findTransformECC
Finds the geometric transform (warp) between two images in terms of the ECC criterion [67] .,2 つの画像間の幾何学的変換（ワープ）を，ECC 基準[67]に基づいて求めます．
"inputImage : single-channel input image which should be warped with the final warpMatrix in order to provide an image similar to templateImage, same type as templateImage.",inputImage : templateImage と同様の画像を得るために，最終的に warpMatrix を用いてワープされるべきシングルチャンネルの入力画像で，templateImage と同じ型．
warpMatrix : floating-point \(2\times 3\) or \(3\times 3\) mapping matrix (warp).,warpMatrix : 浮動小数点型のマッピング行列（warp）．
"motionType : parameter, specifying the type of motion:",motionType : モーションの種類を指定するパラメータ．
MOTION_TRANSLATION sets a translational motion model; warpMatrix is \(2\times 3\) with the first \(2\times 2\) part being the unity matrix and the rest two parameters being estimated.,MOTION_TRANSLATIONは並進運動のモデルを設定し、warpMatrixは\(2\)の部分をユニティ行列とし、残りの2つのパラメータを推定します。
MOTION_EUCLIDEAN sets a Euclidean (rigid) transformation as motion model; three parameters are estimated; warpMatrix is \(2\times 3\).,MOTION_EUCLIDEANはユークリッド（剛体）変換をモーションモデルとして設定します。
MOTION_AFFINE sets an affine motion model (DEFAULT); six parameters are estimated; warpMatrix is \(2\times 3\).,MOTION_AFFINEはアフィン変換のモーションモデルを設定します（DEFAULT）；6つのパラメータが推定されます；WarpMatrixは
MOTION_HOMOGRAPHY sets a homography as a motion model; eight parameters are estimated;`warpMatrix` is \(3\times 3\).,MOTION_HOMOGRAPHY はホモグラフィーをモーションモデルとして設定します。8つのパラメータが推定され、warpMatrix は ˶ˆ꒳ˆ˵ )
"criteria : parameter, specifying the termination criteria of the ECC algorithm; criteria.epsilon defines the threshold of the increment in the correlation coefficient between two iterations (a negative criteria.epsilon makes criteria.maxcount the only termination criterion). Default values are shown in the declaration above.",criteria : パラメータ．ECC アルゴリズムの終了基準を指定する．cliteria.epsilon は，2 回の反復の間の相関係数の増分の閾値を定義する（cliteria.epsilon が負の場合，cliteria.maxcount が唯一の終了基準となる）．デフォルト値は，上記の宣言に示されています．
gaussFiltSize : An optional value indicating size of gaussian blur filter; (DEFAULT: 5),gaussFiltSize : ガウスぼかしフィルタのサイズを示すオプション値; (初期値: 5)
"The function estimates the optimum transformation (warpMatrix) with respect to ECC criterion ([67]), that is\[\texttt{warpMatrix} = \arg\max_{W} \texttt{ECC}(\texttt{templateImage}(x,y),\texttt{inputImage}(x',y'))\]where\[\begin{bmatrix} x' \\ y' \end{bmatrix} = W \cdot \begin{bmatrix} x \\ y \\ 1 \end{bmatrix}\](the equation holds with homogeneous coordinates for homography). It returns the final enhanced correlation coefficient, that is the correlation coefficient between the template image and the final warped input image. When a \(3\times 3\) matrix is given with motionType =0, 1 or 2, the third row is ignored.Unlike findHomography and estimateRigidTransform, the function findTransformECC implements an area-based alignment that builds on intensity similarities. In essence, the function updates the initial transformation that roughly aligns the images. If this information is missing, the identity warp (unity matrix) is used as an initialization. Note that if images undergo strong displacements/rotations, an initial transformation that roughly aligns the images is necessary (e.g., a simple euclidean/similarity transform that allows for the images showing the same image content approximately). Use inverse warping in the second image to take an image close to the first one, i.e. use the flag WARP_INVERSE_MAP with warpAffine or warpPerspective. See also the OpenCV sample image_alignment.cpp that demonstrates the use of the function. Note that the function throws an exception if algorithm does not converges.See alsocomputeECC, estimateAffine2D, estimateAffinePartial2D, findHomographyExamples: samples/cpp/image_alignment.cpp.","この関数は，ECC基準（[67]）に照らし合わせて，最適な変換（warpMatrix）を推定します．テンプレート画像と、最終的にワープした入力画像の相関係数である、最終的な拡張相関係数を返します。関数 findTransformECC は，findHomography や estimateRigidTransform とは異なり，強度の類似性を利用したエリアベースの位置合わせを行います．要するに，この関数は，画像を大まかに整列させる最初の変換を更新します．この情報が不足している場合は，初期化として identity warp（ユニティ行列）が利用されます．なお，画像が強い変位や回転を受ける場合は，画像を大まかに揃える初期変換が必要になります（例えば，画像の内容がほぼ同じになるような単純なユークリッド変換や類似性変換など）．1枚目の画像に近い画像を得るために，2枚目の画像に逆ワープを利用します．つまり，フラグ WARP_INVERSE_MAP と warpAffine または warpPerspective を利用します．この関数の使用方法を示す OpenCV のサンプル image_alignment.cpp も参照してください．他にも，ocomputeECC, estimateAffine2D, estimateAffinePartial2D, findHomographyExamples: samples/cpp/image_alignment.cpp を参照してください．"
Kalman filter class. ,カルマンフィルタクラスです．
"The class implements a standard Kalman filter http://en.wikipedia.org/wiki/Kalman_filter, [272] . However, you can modify transitionMatrix, controlMatrix, and measurementMatrix to get an extended Kalman filter functionality. NoteIn C API when CvKalman* kalmanFilter structure is not needed anymore, it should be released with cvReleaseKalman(&kalmanFilter) ","このクラスは，標準的なカルマンフィルタ http://en.wikipedia.org/wiki/Kalman_filter, [272] を実装しています．しかし， transitionMatrix, controlMatrix, measurementMatrix を変更することで，拡張カルマンフィルタの機能を得ることができます．注意 C API では， CvKalman* kalmanFilter 構造体が不要になった場合， cvReleaseKalman(&kalmanFilter) で解放されます．"
Examples: samples/cpp/kalman.cpp.,例： samples/cpp/kalman.cpp.
Re-initializes Kalman filter. The previous content is destroyed.,カルマンフィルタを再初期化します．以前の内容は破棄されます．
dynamParams : Dimensionality of the state.,dynamParams :状態の次元性。
measureParams : Dimensionality of the measurement.,measureParams :測定値の次元
controlParams : Dimensionality of the control vector.,controlParams :コントロール・ベクトルの次元
type : Type of the created matrices that should be CV_32F or CV_64F.,type :CV_32F または CV_64F であるべき，作成される行列の種類．
Computes a predicted state.,予測される状態を計算します．
control : The optional input control,control : オプションである入力コントロール．
Updates the predicted state from the measurement.,測定値から予測状態を更新します．
measurement : The measured system parameters,measurement : 計測されたシステムパラメータ
predicted state (x'(k)): x(k)=A*x(k-1)+B*u(k),予測状態 (x'(k)): x(k)=A*x(k-1)+B*u(k)
corrected state (x(k)): x(k)=x'(k)+K(k)*(z(k)-H*x'(k)),補正後の状態（x(k)）：x(k)=x'(k)+K(k)*(z(k)-H*x'(k))
state transition matrix (A),状態遷移行列(A)
control matrix (B) (not used if there is no control),制御行列(B) (制御がない場合は使用しない)
measurement matrix (H),測定行列(H)
process noise covariance matrix (Q),プロセスノイズ共分散行列 (Q)
measurement noise covariance matrix (R),測定ノイズ共分散行列 (R)
priori error estimate covariance matrix (P'(k)): P'(k)=A*P(k-1)*At + Q)*/,プリオリ誤差推定共分散行列（P'(k)）。P'(k)=A*P(k-1)*At + Q)/*。
Kalman gain matrix (K(k)): K(k)=P'(k)*Ht*inv(H*P'(k)*Ht+R),カルマンゲイン行列(K(k)):K(k)=P'(k)*Ht*inv(H*P'(k)*Ht+R)
posteriori error estimate covariance matrix (P(k)): P(k)=(I-K(k)*H)*P'(k),posteriori誤差推定共分散行列(P(k)):P(k)=(I-K(k)*H)*P'(k)
Initialize the tracker with a known bounding box that surrounded the target.,ターゲットを囲んだ既知のバウンディングボックスでトラッカーを初期化する。
image : The initial frame,image : 初期フレーム
boundingBox : The initial bounding box,boundingBox : 初期のバウンディングボックス
"Update the tracker, find the new most likely bounding box for the target.",トラッカーを更新し、ターゲットに最も適した新しいバウンディングボックスを見つける。
image : The current frame,image : 現在のフレーム
"boundingBox : The bounding box that represent the new target location, if true was returned, not modified otherwise",boundingBox : 新しいターゲットの位置を表すバウンディングボックス（trueが返ってきた場合）、それ以外は変更なし
Create MIL tracker instance.,MILトラッカーのインスタンスを作成します。
parameters : MIL parameters TrackerMIL::Params,parameters : MILパラメータ TrackerMIL::Params
The MIL algorithm trains a classifier in an online manner to separate the object from the background. ,MILアルゴリズムは、オンラインで分類器を学習し、背景からオブジェクトを分離します。
Multiple Instance Learning avoids the drift problem for a robust tracking. The implementation is based on [13] .,複数インスタンス学習により、ドリフト問題を回避し、ロバストなトラッキングを実現します。実装は[13]を参考にしています．
Original code can be found here http://vision.ucsd.edu/~bbabenko/project_miltrack.shtml ,オリジナルのコードは http://vision.ucsd.edu/~bbabenko/project_miltrack.shtml にあります。
Constructor.,コンストラクタです。
parameters : GOTURN parameters TrackerGOTURN::Params,parameters : GOTURNパラメータ TrackerGOTURN::Params
the GOTURN (Generic Object Tracking Using Regression Networks) tracker ,GOTURN (Generic Object Tracking Using Regression Networks) トラッカー
"GOTURN ([107]) is kind of trackers based on Convolutional Neural Networks (CNN). While taking all advantages of CNN trackers, GOTURN is much faster due to offline training without online fine-tuning nature. GOTURN tracker addresses the problem of single target tracking: given a bounding box label of an object in the first frame of the video, we track that object through the rest of the video. NOTE: Current method of GOTURN does not handle occlusions; however, it is fairly robust to viewpoint changes, lighting changes, and deformations. Inputs of GOTURN are two RGB patches representing Target and Search patches resized to 227x227. Outputs of GOTURN are predicted bounding box coordinates, relative to Search patch coordinate system, in format X1,Y1,X2,Y2. Original paper is here: http://davheld.github.io/GOTURN/GOTURN.pdf As long as original authors implementation: https://github.com/davheld/GOTURN#train-the-tracker Implementation of training algorithm is placed in separately here due to 3d-party dependencies: https://github.com/Auron-X/GOTURN_Training_Toolkit GOTURN architecture goturn.prototxt and trained model goturn.caffemodel are accessible on opencv_extra GitHub repository. ","GOTURN（[107]）は、Convolutional Neural Networks（CNN）をベースにしたトラッカーの一種です。GOTURNはCNNをベースにしたトラッカーの一種です。CNNトラッカーのすべての利点を利用していますが、GOTURNはオンラインでの微調整を必要としないオフラインでのトレーニングにより、はるかに高速です。GOTURNトラッカーは、ビデオの最初のフレームにあるオブジェクトのバウンディングボックスのラベルが与えられた場合、そのオブジェクトをビデオの残りの部分で追跡するという、シングルターゲットのトラッキングの問題に対応しています。注意：現在のGOTURNはオクルージョンを扱いませんが、視点の変更、照明の変更、変形にはかなり強くなっています。GOTURNの入力は、227x227にリサイズされたTargetパッチとSearchパッチを表す2つのRGBパッチです。GOTURNの出力は、X1,Y1,X2,Y2のフォーマットで表された、サーチパッチの座標系に対する予測バウンディングボックス座標です。Original paper is here: http://davheld.github.io/GOTURN/GOTURN.pdf As long as original authors implementation: https://github.com/davheld/GOTURN#train-the-tracker 3d-party dependencies due to the Implementation of training algorithm is placed in separately here: https://github.com/Auron-X/GOTURN_Training_Toolkit GOTURN architecture goturn.prototxt and trained model goturn.caffemodel is accessible on opencv_extra GitHub repository."
Performs thresholding on input images using Niblack's technique or some of the popular variations it inspired.,Niblackの手法や、Niblackにインスパイアされた一般的なバリエーションを用いて、入力画像の閾値処理を行う。
_src : Source 8-bit single-channel image.,_src : 8ビットシングルチャンネルの入力画像．
_dst : Destination image of the same size and the same type as src.,_dst : src と同じサイズ，同じタイプの出力画像．
"maxValue : Non-zero value assigned to the pixels for which the condition is satisfied, used with the THRESH_BINARY and THRESH_BINARY_INV thresholding types.","maxValue : 閾値処理タイプTHRESH_BINARY, THRESH_BINARY_INVで用いられる，条件を満たすピクセルに割り当てられる0ではない値．"
"type : Thresholding type, see cv::ThresholdTypes.",type :cv::ThresholdTypes を参照してください．
"k : The user-adjustable parameter used by Niblack and inspired techniques. For Niblack, this is normally a value between 0 and 1 that is multiplied with the standard deviation and subtracted from the mean.",k : Niblackや，それにインスパイアされた手法で利用される，ユーザが調整可能なパラメータ．Niblackの場合，通常は0から1の間の値で，これに標準偏差を掛け，平均値から差し引きます．
"binarizationMethod : Binarization method to use. By default, Niblack's technique is used. Other techniques can be specified, see cv::ximgproc::LocalBinarizationMethods.",binarizationMethod : 使用する2値化手法。デフォルトでは，Niblackの手法が利用されます．他の手法を指定することも可能で， cv::ximgproc::LocalBinarizationMethods を参照してください．
r : The user-adjustable parameter used by Sauvola's technique. This is the dynamic range of standard deviation.,r : Sauvolaの手法で利用される，ユーザが調整可能なパラメータです．これは，標準偏差のダイナミックレンジです．
" where \(T(x,y)\) is a threshold calculated individually for each pixel.The threshold value \(T(x, y)\) is determined based on the binarization method chosen. For classic Niblack, it is the mean minus \( k \) times standard deviation of \(\texttt{blockSize} \times\texttt{blockSize}\) neighborhood of \((x, y)\).The function can't process the image in-place.See alsothreshold, adaptiveThreshold"," ここで、˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ ) ˶ˆ꒳ˆ˵ )この関数は，画像をその場で処理することはできません．See alsothreshold, adaptiveThreshold"
"Applies a binary blob thinning operation, to achieve a skeletization of the input image.",入力画像をスケルチゼーションするために，2値のblobを間引く操作を行います．
"src : Source 8-bit single-channel image, containing binary blobs, with blobs having 255 pixel values.",src : 2値のblobを含む，8ビットシングルチャンネルの入力画像．
dst : Destination image of the same size and the same type as src. The function can work in-place.,dst : src と同じサイズ，同じタイプの出力画像．この関数は，インプレースで動作します．
thinningType : Value that defines which thinning algorithm should be used. See cv::ximgproc::ThinningTypes,thinningType．どのような間引きアルゴリズムを利用するかを定義する値．cv::ximgproc::ThinningTypes を参照してください．
The function transforms a binary blob image into a skeletized form using the technique of Zhang-Suen.,この関数は， Zhang-Suen の手法を用いて，2値の blob 画像をスケルティッド形式に変換します．
Performs anisotropic diffusion on an image.,また，画像に対して異方性拡散を行います．
src : Source image with 3 channels.,src : 3チャンネルのソース画像．
"alpha : The amount of time to step forward by on each iteration (normally, it's between 0 and 1).",alpha : 反復ごとにステップアップする時間を指定します（通常は，0から1の間です）．
K : sensitivity to the edges,K : エッジに対する感度
niters : The number of iterations,niters : 反復処理の回数
"The function applies Perona-Malik anisotropic diffusion to an image. This is the solution to the partial differential equation:\[{\frac {\partial I}{\partial t}}={\mathrm {div}}\left(c(x,y,t)\nabla I\right)=\nabla c\cdot \nabla I+c(x,y,t)\Delta I\]Suggested functions for c(x,y,t) are:\[c\left(\|\nabla I\|\right)=e^{{-\left(\|\nabla I\|/K\right)^{2}}}\]or\[ c\left(\|\nabla I\|\right)={\frac {1}{1+\left({\frac {\|\nabla I\|}{K}}\right)^{2}}} \]","この関数は，Perona-Malik異方性拡散を画像に適用します．これは，偏微分方程式の解である。\C(x,y,t)の関数としては、次のようなものがあります。\]"
creates a quaternion image.,は、クォータニオンイメージを作成します。
"img : Source 8-bit, 32-bit or 64-bit image, with 3-channel image.",img : ソースとなる 8 ビット，32 ビット，64 ビットの画像（3 チャンネル画像を含む）．
"qimg : result CV_64FC4 a quaternion image( 4 chanels zero channel and B,G,R).","qimg : 結果 CV_64FC4 quaternion image( 4 channel zero channel and B,G,R)."
calculates conjugate of a quaternion image.,4元画像の共役を計算します．
qimg : quaternion image.,qimg : クオータニオン画像．
qcimg : conjugate of qimg,qcimg : qimgの共役画像
divides each element by its modulus.,は、各要素をそのモジュラスで除算します。
qnimg : conjugate of qimg,qnimg : qimgの共役
Calculates the per-element quaternion product of two arrays.,2つの配列の要素ごとのクォータニオン積を計算します。
src1 : quaternion image.,src1 : クオータニオンイメージ
src2 : quaternion image.,src2 : クォータニオン画像
dst : product dst(I)=src1(I) . src2(I),dst : 積 dst(I)=src1(I) . src2(I)
Performs a forward or inverse Discrete quaternion Fourier transform of a 2D quaternion array.,2次元のクォータニオン配列に対して，離散的クォータニオンフーリエ変換（順変換）または逆変換を行います．
img : quaternion image.,img : クオータニオン画像．
qimg : quaternion image in dual space.,qimg : 二重空間におけるクォータニオン画像．
flags : quaternion image in dual space. only DFT_INVERSE flags is supported,flags : 二重空間のクォータニオンイメージ DFT_INVERSEフラグのみサポート
sideLeft : true the hypercomplex exponential is to be multiplied on the left (false on the right ).,sideLeft : 左側で超複素数指数が乗算される場合はtrue，右側ではfalse．
Compares a color template against overlapped color image regions.,カラーテンプレートを，オーバーラップしたカラー画像領域と比較します．
img : Image where the search is running. It must be 3 channels image,img : 探索が行われる画像．3チャンネルの画像である必要があります。
templ : Searched template. It must be not greater than the source image and have 3 channels,templ : 検索されたテンプレート．元画像以上の大きさで、3チャンネルである必要があります。
result : Map of comparison results. It must be single-channel 64-bit floating-point,result : 比較結果のマップ．1チャンネルの64ビット浮動小数点である必要があります．
Applies Y Deriche filter to an image.,Y Dericheフィルタを画像に適用します．
"op : Source 8-bit or 16bit image, 1-channel or 3-channel image.",op :op : 8ビットまたは16ビットの入力画像，1チャンネルまたは3チャンネルの入力画像．
dst : result CV_32FC image with same number of channel than _op.,dst : _op と同じチャンネル数の CV_32FC 画像の比較結果．
alpha : double see paper,alpha : ダブル 詳しくはこちら
omega : double see paper,omega : double 論文を参照してください．
"For more details about this implementation, please see http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.476.5736&rep=rep1&type=pdf",この実装の詳細については，http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.476.5736&rep=rep1&type=pdf を参照してください．
Applies X Deriche filter to an image.,X Deriche フィルタを画像に適用します．
Smoothes an image using the Edge-Preserving filter.,Edge-Preerving フィルタを使って，画像を滑らかにします．
src : Source 8-bit 3-channel image.,src : 8ビット3チャンネルの入力画像．
dst : Destination image of the same size and type as src.,dst : src と同じサイズ，同じタイプの出力画像．
d : Diameter of each pixel neighborhood that is used during filtering. Must be greater or equal 3.,d : フィルタリングの際に利用される，各ピクセルの近傍領域の直径．3以上でなければいけません．
"threshold : Threshold, which distinguishes between noise, outliers, and data.",threshold : ノイズ，異常値，データを区別するための閾値．
"The function smoothes Gaussian noise as well as salt & pepper noise. For more details about this implementation, please see [ReiWoe18] Reich, S. and Wörgötter, F. and Dellen, B. (2018). A Real-Time Edge-Preserving Denoising Filter. Proceedings of the 13th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications (VISIGRAPP): Visapp, 85-94, 4. DOI: 10.5220/0006509000850094.","この関数は，ガウシアンノイズだけでなく，ソルト＆ペッパーノイズも平滑化します．この実装の詳細については、[ReiWoe18] Reich, S. and Wörgötter, F. and Dellen, B. (2018).A Real-Time Edge-Preserving Denoising Filter.Proceedings of the 13th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications (VISIGRAPP):Visapp, 85-94, 4.doi: 10.5220/0006509000850094."
Computes the estimated covariance matrix of an image using the sliding window forumlation.,スライディングウィンドウ法を用いて，画像の共分散行列の推定値を計算します．
src : The source image. Input image must be of a complex type.,src : 入力画像．入力画像は，複素数型でなければいけません．
"dst : The destination estimated covariance matrix. Output matrix will be size (windowRows*windowCols, windowRows*windowCols).","dst : 出力される推定共分散行列．出力行列のサイズは，(windowRows*windowCols, windowRows*windowCols) になります．"
windowRows : The number of rows in the window.,windowRows :ウィンドウ内の行数．
"windowCols : The number of cols in the window. The window size parameters control the accuracy of the estimation. The sliding window moves over the entire image from the top-left corner to the bottom right corner. Each location of the window represents a sample. If the window is the size of the image, then this gives the exact covariance matrix. For all other cases, the sizes of the window will impact the number of samples and the number of elements in the estimated covariance matrix.",windowCols : ウィンドウ内のコルの数．ウィンドウサイズのパラメータは，推定の精度を制御します．スライディングウィンドウは，画像の左上から右下に向かって，画像全体を移動します．窓の各位置は，1つのサンプルを表します。窓の大きさが画像の大きさと同じであれば，正確な共分散行列が得られます．その他のケースでは、窓のサイズがサンプル数と推定共分散行列の要素数に影響を与えます。
Calculates 2D Fast Hough transform of an image.,画像の2次元高速ハフ変換を計算します．
"dst : The destination image, result of transformation.",dst : 変換結果である出力画像．
src : The source (input) image.,src : ソース（入力）画像
dstMatDepth : The depth of destination image,dstMatDepth : 出力画像の深度
"op : The operation to be applied, see cv::HoughOp",op :適用される操作， cv::HoughOp を参照してください．
"angleRange : The part of Hough space to calculate, see cv::AngleRangeOption",angleRange : 計算されるハフ空間の一部， cv::AngleRangeOption を参照してください．
"makeSkew : Specifies to do or not to do image skewing, see cv::HoughDeskewOption",makeSkew : 画像の歪曲を行うか否かを指定します．
"The function calculates the fast Hough transform for full, half or quarter range of angles.",この関数は，全角，半角，1/4角の範囲で高速ハフ変換を計算します．
Calculates coordinates of line segment corresponded by point in Hough space.,Hough空間における点に対応する線分の座標を求めます．
houghPoint : Point in Hough space.,houghPoint : ハフ空間上の点．
srcImgInfo : The source (input) image of Hough transform.,srcImgInfo : ハフ変換のソース（入力）画像．
"angleRange : The part of Hough space where point is situated, see cv::AngleRangeOption",angleRange : ハフ空間における点の位置を表す部分， cv::AngleRangeOption を参照してください．
"rules : Specifies strictness of line segment calculating, see cv::RulesOption",rules :線分の計算の厳密さを指定します， cv::RulesOption を参照してください．
Return values,戻り値
    [Vec4i]Coordinates of line segment corresponded by point in Hough space.RemarksIf rules parameter set to RO_STRICT then returned line cut along the border of source image. ,    [Vec4i]Hough空間における点に対応する線分の座標．備考パラメータ rules が RO_STRICT に設定されている場合，返される線分は元画像の境界線に沿って切断されます．
"If rules parameter set to RO_WEAK then in case of point, which belongs the incorrect part of Hough image, returned line will not intersect source image.The function calculates coordinates of line segment corresponded by point in Hough space.","rulesパラメータがRO_WEAKに設定されている場合, Hough画像の不正な部分に属する点の場合, 返される線は元画像と交差しません. この関数は, Hough空間における点に対応する線分の座標を計算します."
Applies Paillou filter to an image.,画像にPaillouフィルタをかけます。
"op : Source CV_8U(S) or CV_16U(S), 1-channel or 3-channels image.","op:CV_8U(S)またはCV_16U(S), 1チャンネルまたは3チャンネルの画像を入力とします．"
_dst : result CV_32F image with same number of channel than op.,_dst : op と同じチャンネル数の CV_32F 画像．
"For more details about this implementation, please see [189]See alsoGradientPaillouX, GradientPaillouY","この実装の詳細については，[189]を参照してください． GradientPaillouX, GradientPaillouY も参照してください．"
Calculates an affine transformation that normalize given image using Pei&Lin Normalization.,Pei&Lin Normalization を用いて，与えられた画像を正規化するアフィン変換を求めます．
I : Given transformed image.,I : 与えられた変換後の画像。
"Assume given image \(I=T(\bar{I})\) where \(\bar{I}\) is a normalized image and \(T\) is an affine transformation distorting this image by translation, rotation, scaling and skew. The function returns an affine transformation matrix corresponding to the transformation \(T^{-1}\) described in [PeiLin95]. For more details about this implementation, please see [PeiLin95] Soo-Chang Pei and Chao-Nan Lin. Image normalization for pattern recognition. Image and Vision Computing, Vol. 13, N.10, pp. 711-723, 1995.","I=T(\bar{I})\は正規化された画像、\(T\)はこの画像を並進、回転、スケーリング、スキューなどで歪ませるアフィン変換を表します。この関数は，[PeiLin95]で述べられている変換\(T^{-1}\)に対応するアフィン変換行列を返します．この実装の詳細については，[PeiLin95] Soo-Chang Pei and Chao-Nan Lin を参照してください．パターン認識のための画像正規化.Image and Vision Computing, Vol.13, N.10, pp.711-723, 1995."
src : input array (single-channel).,src : 入力配列（シングルチャンネル）．
rlDest : resulting run length encoded image.,rlDest : ランレングス符号化された結果の画像．
type : thresholding type (only cv::THRESH_BINARY and cv::THRESH_BINARY_INV are supported),type : 閾値処理タイプ（ cv::THRESH_BINARY と cv::THRESH_BINARY_INV のみがサポートされます）．
Dilates an run-length encoded binary image by using a specific structuring element.,特定の構造化要素を用いて，ランレングス符号化された2値画像を希釈します．
rlSrc : input image,rlSrc : 入力画像．
rlDest : result,rlDest : 結果
rlKernel : kernel,rlKernel : カーネル
"anchor : position of the anchor within the element; default value (0, 0) is usually the element center.","anchor : 要素内のアンカーの位置。デフォルト値(0, 0)は通常、要素の中心になります。"
Erodes an run-length encoded binary image by using a specific structuring element.,特定の構造化要素を用いてランレングス符号化された二値画像を電極化する。
"bBoundaryOn : indicates whether pixel outside the image boundary are assumed to be on (True: works in the same way as the default of cv::erode, False: is a little faster)",bBoundaryOn : 画像の境界の外側にあるピクセルをオンと仮定するかどうかを指定します（True: cv::erode のデフォルトと同じように動作し，False: は少し高速です）．
Paint run length encoded binary image into an image.,ランレングス符号化されたバイナリ画像を，画像にペイントします．
image : image to paint into (currently only single channel images).,image : 描画される画像（現在は，シングルチャンネル画像のみ）．
rlSrc : run length encoded image,rlSrc : ランレングス符号化された画像．
value : all foreground pixel of the binary image are set to this value,value : 2値画像のすべての前景画素をこの値に設定します。
Check whether a custom made structuring element can be used with run length morphological operations. (It must consist of a continuous array of single runs per row),カスタムメイドの構造化要素をランレングスモフォロジカルオペレーションで使用できるかどうかをチェックします。(カスタムメイドの構造化要素がランレングスモフォロジカルオペレーションで使用できるかどうかをチェックします。）
rlStructuringElement : mask to be tested,rlStructuringElement : テストされるマスク
"Creates a run-length encoded image from a vector of runs (column begin, column end, row)",runのベクトル(列開始、列終了、行)からランレングスエンコードされた画像を作成する。
runs : vector of runs,runs : ランのベクトル
res : result,res : 結果
"size : image size (to be used if an ""on"" boundary should be used in erosion, using the default means that the size is computed from the extension of the input)","size : 画像の大きさ（侵食に ""on ""の境界を使う場合に使用される。デフォルトを使うと、入力の拡張からサイズが計算されることになる）"
Applies a morphological operation to a run-length encoded binary image.,ランレングスエンコードされた2値画像に対して，モルフォロジカルな処理を行います．
op : all operations supported by cv::morphologyEx (except cv::MORPH_HITMISS),op : cv::morphologyEx がサポートするすべての操作（ただし，cv::MORPH_HITMISS を除く）．
"bBoundaryOnForErosion : indicates whether pixel outside the image boundary are assumed to be on for erosion operations (True: works in the same way as the default of cv::erode, False: is a little faster)",bBoundaryOnForErosion : 画像の境界の外側にあるピクセルを，侵食操作の対象とするかどうかを示します（True: cv::erode のデフォルトと同じように動作し，False: 少し高速になります）．
Applies weighted median filter to an image.,画像に重み付けされたメディアンフィルタを適用します．
"joint : Joint 8-bit, 1-channel or 3-channel image.",joint : 8ビット，1チャンネル，3チャンネルの結合画像．
"r : Radius of filtering kernel, should be a positive integer.",r :フィルタリングカーネルの半径，正の整数でなければいけません．
sigma : Filter range standard deviation for the joint image.,sigma : ジョイント画像に対するフィルタリング範囲の標準偏差．
"weightType : weightType The type of weight definition, see WMFWeightType",weightType : weightType 重みの定義の種類，WMFWeightTypeを参照．
"mask : A 0-1 mask that has the same size with I. This mask is used to ignore the effect of some pixels. If the pixel value on mask is 0, the pixel will be ignored when maintaining the joint-histogram. This is useful for applications like optical flow occlusion handling.",mask : Iと同じ大きさの0-1マスク．このマスクは，一部のピクセルの影響を無視するために使用される．mask上のピクセル値が0の場合，そのピクセルはジョイントヒストグラムを維持する際に無視されます．これは、オプティカルフローのオクルージョン処理などの用途に役立ちます。
"For more details about this implementation, please see [292]See alsomedianBlur, jointBilateralFilter","この実装の詳細については、[292]参照：somedianBlur, jointBilateralFilter"
Returns array containing proposal boxes.,提案ボックスを含む配列を返します。
edge_map : edge image.,edge_map : エッジ画像。
orientation_map : orientation map.,orientation_map : オリエンテーションマップ。
boxes : proposal boxes.,boxes : 提案の箱。
"scores : of the proposal boxes, provided a vector of float types.",scores : 提案ボックスの点数、float型のベクトルで指定。
Returns the step size of sliding window search.,スライディングウィンドウサーチのステップサイズを返します。
Sets the step size of sliding window search.,スライディングウィンドウサーチのステップサイズを設定します。
Returns the nms threshold for object proposals.,オブジェクト提案のnms閾値を返します。
Sets the nms threshold for object proposals.,提案されたオブジェクトの nms 閾値を設定します。
Returns adaptation rate for nms threshold.,Return adaptation rate for nms threshold.
Sets the adaptation rate for nms threshold.,nmsしきい値の適応率を設定します。
Returns the min score of boxes to detect.,検出するボックスの最小スコアを返します。
Sets the min score of boxes to detect.,検出するボックスの最小スコアを設定します。
Returns the max number of boxes to detect.,Returns the max number of boxes to detect detective
Sets max number of boxes to detect.,検出するボックスの最大数を設定する
Returns the edge min magnitude.,Returns the edge min magnitude.
Sets the edge min magnitude.,エッジの最小の大きさを設定します。
Returns the edge merge threshold.,Returns the edge merge threshold.
Sets the edge merge threshold.,エッジのマージしきい値を設定します。
Returns the cluster min magnitude.,Returns the cluster min magnitude.
Sets the cluster min magnitude.,クラスターの最小の大きさを設定します。
Returns the max aspect ratio of boxes.,Returns the max aspect ratio of boxes.
Sets the max aspect ratio of boxes.,ボックスのアスペクト比の最大値を設定します。
Returns the minimum area of boxes.,Returns the minimum area of boxes - ボックスの最小面積を返します。
Sets the minimum area of boxes.,ボックスの最小面積を設定します。
Returns the affinity sensitivity.,Return the affinity sensitivity.
Sets the affinity sensitivity.,親和性の感度を設定します。
Returns the scale sensitivity.,Return the scale sensitivity.
Sets the scale sensitivity.,スケールの感度を設定します。
Creates a Edgeboxes.,Edgeboxes を作成します。
alpha : step size of sliding window search.,alpha : スライディングウィンドウサーチのステップサイズ。
beta : nms threshold for object proposals.,beta : オブジェクト提案のための nms 閾値.
eta : adaptation rate for nms threshold.,eta : nms 閾値の適応率.
minScore : min score of boxes to detect.,minScore : 検出するボックスの最小スコア.
maxBoxes : max number of boxes to detect.,maxBoxes : 検出するボックスの最大数.
edgeMinMag : edge min magnitude. Increase to trade off accuracy for speed.,edgeMinMag : エッジの最小マグニチュード.精度と速度をトレードオフするために増やします。
edgeMergeThr : edge merge threshold. Increase to trade off accuracy for speed.,edgeMergeThr : エッジのマージ閾値。速度と精度をトレードオフするために増やします。
clusterMinMag : cluster min magnitude. Increase to trade off accuracy for speed.,clusterMinMag : クラスターの最小マグニチュード。速度と精度をトレードオフするために増やします。
maxAspectRatio : max aspect ratio of boxes.,maxAspectRatio : ボックスの最大アスペクト比を指定します。
minBoxArea : minimum area of boxes.,minBoxArea : ボックスの最小面積を指定します。
gamma : affinity sensitivity.,gamma : アフィニティ感度
kappa : scale sensitivity.,kappa : スケール感度.
Class implementing EdgeBoxes algorithm from [297] : ,297]の EdgeBoxes アルゴリズムを実装したクラス．
Interface for realizations of Domain Transform filter. ,Domain Transformフィルタを実現するためのインタフェースです．
For more details about this filter see [87] . ,このフィルタの詳細については，[87]を参照してください．
Produce domain transform filtering operation on source image.,入力画像に対して，ドメイン変換によるフィルタリング処理を行います．
src : filtering image with unsigned 8-bit or floating-point 32-bit depth and up to 4 channels.,src : 符号なし8ビット，または浮動小数点型32ビットの深度を持ち，最大4チャンネルまでのフィルタリング画像．
dst : destination image.,dst : 出力画像．
"dDepth : optional depth of the output image. dDepth can be set to -1, which will be equivalent to src.depth().",dDepth : オプションで，出力画像の深度を指定します． dDepth を -1 に設定すると， src.depth() と同等の効果が得られます．
"Factory method, create instance of DTFilter and produce initialization routines.",DTFilter のインスタンスを作成し，初期化ルーチンを生成するファクトリーメソッド．
"guide : guided image (used to build transformed distance, which describes edge structure of guided image).",guide : ガイド付き画像（ガイド付き画像のエッジ構造を表す，変換後の距離を構築するために利用されます）．
"sigmaSpatial : \({\sigma}_H\) parameter in the original article, it's similar to the sigma in the coordinate space into bilateralFilter.",sigmaSpatial :\本家では、bilateralFilterの座標空間におけるシグマに相当するパラメータです。
"sigmaColor : \({\sigma}_r\) parameter in the original article, it's similar to the sigma in the color space into bilateralFilter.",sigmaColor：オリジナルでは、色空間でのシグマをbilateralFilterで表現したものです。
"mode : one form three modes DTF_NC, DTF_RF and DTF_IC which corresponds to three modes for filtering 2D signals in the article.","mode : DTF_NC, DTF_RF, DTF_IC の3つのモードのうちの1つで，記事中の2次元信号をフィルタリングするための3つのモードに対応しています．"
"numIters : optional number of iterations used for filtering, 3 is quite enough.",numIters : オプションで、フィルタリングに使用するイテレーションの回数を指定します。
"For more details about Domain Transform filter parameters, see the original article [87] and Domain Transform filter homepage.",Domain Transformフィルタのパラメータの詳細については，原著論文[87]およびDomain Transformフィルタのホームページを参照してください．
Simple one-line Domain Transform filter call. If you have multiple images to filter with the same guided image then use DTFilter interface to avoid extra computations on initialization stage.,シンプルな1行のDomain Transformフィルタ呼び出し。もし，同じガイド画像を使ってフィルタリングする画像が複数ある場合は，初期化段階での余分な計算を避けるために DTFilter インタフェースを利用してください．
guide : guided image (also called as joint image) with unsigned 8-bit or floating-point 32-bit depth and up to 4 channels.,guide : ガイド付き画像（ジョイント画像とも呼ばれます）、深度は符号なし8ビットまたは浮動小数点32ビット、チャンネル数は最大4です。
"See alsobilateralFilter, guidedFilter, amFilter",関連項目： バイラテラルフィルタ，ガイド付きフィルタ，AMフィルタ
Interface for realizations of Guided Filter. ,ガイド付きフィルタを実現するためのインターフェースです。
For more details about this filter see [106] . ,このフィルタの詳細については [106] を参照してください。
Apply Guided Filter to the filtering image.,フィルタリング画像に Guided Filter を適用します。
src : filtering image with any numbers of channels.,src : 任意のチャンネル数のフィルタリング画像。
"Factory method, create instance of GuidedFilter and produce initialization routines.",GuidedFilter のインスタンスを作成したり、初期化ルーチンを生成するファクトリーメソッドです。
"guide : guided image (or array of images) with up to 3 channels, if it have more then 3 channels then only first 3 channels will be used.",guide : 3チャンネルまでのガイド画像（または、画像の配列）。3チャンネル以上の場合は、最初の3チャンネルのみが利用されます。
radius : radius of Guided Filter.,radius : ガイド付きフィルタの半径。
eps : regularization term of Guided Filter. \({eps}^2\) is similar to the sigma in the color space into bilateralFilter.,eps : Guided Filterの正則化項．\bilateralFilterの色空間でのシグマに相当します。
"For more details about Guided Filter parameters, see the original article [106] .",Guided Filter のパラメータの詳細については，原著論文 [106] を参照してください．
Simple one-line Guided Filter call.,シンプルな1行のGuided Filterの呼び出しです。
dDepth : optional depth of the output image.,dDepth : オプションで，出力画像の深度を指定します．
"If you have multiple images to filter with the same guided image then use GuidedFilter interface to avoid extra computations on initialization stage.See alsobilateralFilter, dtFilter, amFilter","同じガイド画像を使って複数の画像をフィルタリングする場合は、GuidedFilterインターフェイスを使うと、初期化時に余分な計算をしなくて済みます。他にも、bilateralFilter, dtFilter, amFilterを参照してください。"
Interface for Adaptive Manifold Filter realizations. ,アダプティブ・マニホールド・フィルターを実現するためのインターフェイスです。
For more details about this filter see [88] and References_.,このフィルターの詳細については、[88]およびReferences_を参照してください。
Below listed optional parameters which may be set up with Algorithm::set function.,Algorithm::set関数で設定できるオプション・パラメータを以下に示します。
member double sigma_s = 16.0 Spatial standard deviation.,member double sigma_s = 16.0 空間的な標準偏差。
member double sigma_r = 0.2 Color space standard deviation.,member double sigma_r = 0.2 色空間の標準偏差.
member int tree_height = -1 Height of the manifold tree (default = -1 : automatically computed).,member int tree_height = -1 マニフォールドツリーの高さ (default = -1 : 自動計算).
member int num_pca_iterations = 1 Number of iterations to computed the eigenvector.,member int num_pca_iterations = 1 固有ベクトルを計算するための反復回数。
member bool adjust_outliers = false Specify adjust outliers using Eq. 9 or not.,member bool adjust_outliers = false 式(9)を用いて外れ値を調整するかどうかを指定する。
member bool use_RNG = true Specify use random number generator to compute eigenvector or not. ,member bool use_RNG = true 固有ベクトルの計算に乱数発生器を使用するかどうかを指定する。
Apply high-dimensional filtering using adaptive manifolds.,適応型多様体を用いた高次元フィルタリングの適用。
joint : optional joint (also called as guided) image with any numbers of channels.,joint : 任意のチャンネル数の結合画像（ガイド画像とも呼ばれます）．
See alsosetSigmaS,alsosetSigmaSを参照してください。
See alsogetSigmaS,アルソジェットシグマSを参照
See alsogetSigmaR,アルソセットシグマRを参照
See alsosetTreeHeight,アルソセットツリーの高さを見る
See alsogetTreeHeight,アルソジェットツリーハイトを参照
See alsosetPCAIterations,アルソセットPCAIterationsを見る
See alsogetPCAIterations,アルソジェットPCAIterationsを参照
See alsosetAdjustOutliers,アルソセットアジャストアウトライアを見る
See alsogetAdjustOutliers,alsogetAdjustOutliersを参照。
See alsosetUseRNG,参照：「UseRNG」（英語
See alsogetUseRNG,alsogetUseRNGを参照
"Factory method, create instance of AdaptiveManifoldFilter and produce some initialization routines.",AdaptiveManifoldFilterのインスタンスを生成し、初期化ルーチンを生成するファクトリーメソッドです。
sigma_s : spatial standard deviation.,sigma_s : 空間標準偏差．
"sigma_r : color space standard deviation, it is similar to the sigma in the color space into bilateralFilter.",sigma_r : 色空間の標準偏差で、bilateralFilter の色空間のシグマと同じです。
"adjust_outliers : optional, specify perform outliers adjust operation or not, (Eq. 9) in the original paper.",adjust_outliers : オプションで，外れ値の調整処理を行うかどうかを指定します（原著論文ではEq.9）。
"For more details about Adaptive Manifold Filter parameters, see the original article [88] .NoteJoint images with CV_8U and CV_16U depth converted to images with CV_32F depth and [0; 1] color range before processing. Hence color space sigma sigma_r must be in [0; 1] range, unlike same sigmas in bilateralFilter and dtFilter functions.",Adaptive Manifold Filter のパラメータの詳細については，原著論文 [88] を参照してください． 注： CV_8U と CV_16U の深度を持つ結合画像は，処理前に CV_32F の深度と [0; 1] の色域を持つ画像に変換されます．したがって，色空間のシグマ sigma_r は， bilateralFilter や dtFilter のシグマとは異なり， [0; 1] の範囲になければいけません．
Simple one-line Adaptive Manifold Filter call.,シンプルな1行のAdaptive Manifold Filterコールです。
joint : joint (also called as guided) image or array of images with any numbers of channels.,joint : ジョイント画像（ガイド画像とも呼ばれます），あるいは任意のチャンネル数の画像の配列．
"NoteJoint images with CV_8U and CV_16U depth converted to images with CV_32F depth and [0; 1] color range before processing. Hence color space sigma sigma_r must be in [0; 1] range, unlike same sigmas in bilateralFilter and dtFilter functions.See alsobilateralFilter, dtFilter, guidedFilter",注意ジョイント画像のうち，深度が CV_8U と CV_16U のものは，処理の前に深度が CV_32F，色域が [0; 1] の画像に変換されます．そのため，色空間シグマ sigma_r は [0; 1] の範囲になければいけません（bilateralFilter や dtFilter のシグマとは異なります）。
Applies the joint bilateral filter to an image.,ジョイントバイラテラルフィルターを画像に適用します。
"joint : Joint 8-bit or floating-point, 1-channel or 3-channel image.",joint : ジョイント（8ビットまたは浮動小数点）、1チャンネルまたは3チャンネルの画像
"src : Source 8-bit or floating-point, 1-channel or 3-channel image with the same depth as joint image.",src : ジョイント画像と同じ深度の、8ビットまたは浮動小数点型の1チャンネルまたは3チャンネルのソース画像
"d : Diameter of each pixel neighborhood that is used during filtering. If it is non-positive, it is computed from sigmaSpace .",d : フィルタリングの際に利用される，各ピクセルの近傍領域の直径．正の値でない場合は， sigmaSpace から計算されます．
"sigmaColor : Filter sigma in the color space. A larger value of the parameter means that farther colors within the pixel neighborhood (see sigmaSpace ) will be mixed together, resulting in larger areas of semi-equal color.",sigmaColor : 色空間におけるフィルタリングシグマ．このパラメータの値が大きいほど，ピクセル近傍（sigmaSpace参照）の遠方の色が混合され，半均等な色の領域が広くなることを意味します．
"sigmaSpace : Filter sigma in the coordinate space. A larger value of the parameter means that farther pixels will influence each other as long as their colors are close enough (see sigmaColor ). When d>0 , it specifies the neighborhood size regardless of sigmaSpace . Otherwise, d is proportional to sigmaSpace .",sigmaSpace : 座標空間でのフィルターシグマ。このパラメータの値を大きくすると，色が近ければ遠いピクセル同士が影響しあうことになります（sigmaColor参照）．d>0 の場合，sigmaSpace とは関係なく，近傍領域のサイズを指定します．そうでなければ，d は sigmaSpace に比例します．
"NotebilateralFilter and jointBilateralFilter use L1 norm to compute difference between colors.See alsobilateralFilter, amFilter",また、bilateralFilter や jointBilateralFilter では、L1 ノルムを使って色の違いを計算しています。
Applies the bilateral texture filter to an image. It performs structure-preserving texture filter. For more details about this filter see [47].,バイラテラルテクスチャフィルタを画像に適用します。構造保持型のテクスチャフィルタです。このフィルタの詳細については，[47]をご覧ください．
src : Source image whose depth is 8-bit UINT or 32-bit FLOAT,src : 深度が8ビットUINTまたは32ビットFLOATの入力画像
fr : Radius of kernel to be used for filtering. It should be positive integer,fr :フィルタリングに利用するカーネルの半径．正の整数である必要があります．
"numIter : Number of iterations of algorithm, It should be positive integer",numIter : アルゴリズムの反復回数，正の整数でなければいけません．
"sigmaAlpha : Controls the sharpness of the weight transition from edges to smooth/texture regions, where a bigger value means sharper transition. When the value is negative, it is automatically calculated.",sigmaAlpha : エッジからスムース／テクスチャ領域への重みの移行の鋭さを制御します。値が大きいほど移行が鋭くなります。値が負の場合は、自動的に計算されます。
"sigmaAvg : Range blur parameter for texture blurring. Larger value makes result to be more blurred. When the value is negative, it is automatically calculated as described in the paper.",sigmaAvg :テクスチャをぼかすための範囲ぼかしパラメータ。値が大きいほど、よりぼかした結果になります。負の値を設定すると、論文に記載されているように自動的に計算されます。
"See alsorollingGuidanceFilter, bilateralFilter",関連項目：ローリングガイダンスフィルター、バイラテラルフィルター
Applies the rolling guidance filter to an image.,ローリングガイドフィルターを画像に適用します。
numOfIter : Number of iterations of joint edge-preserving filtering applied on the source image.,numOfIter : 元画像に適用されるジョイントエッジ保存フィルタリングのイタレーション数。
"For more details, please see [291]NoterollingGuidanceFilter uses jointBilateralFilter as the edge-preserving filter.See alsojointBilateralFilter, bilateralFilter, amFilter","詳しくは[291]NoterollingGuidanceFilterでは、エッジ保存フィルタとしてjointBilateralFilterを使っています。jointBilateralFilter, bilateralFilter, amFilterも参照してください。"
Interface for implementations of Fast Bilateral Solver. ,Fast Bilateral Solver（高速バイラテラルソルバー）を実装するためのインターフェースです。
For more details about this solver see [15] . ,このソルバーの詳細は[15]を参照してください。
Apply smoothing operation to the source image.,入力画像に，スムージング処理を施します．
src : source image for filtering with unsigned 8-bit or signed 16-bit or floating-point 32-bit depth and up to 3 channels.,src : 符号なし8ビット，符号あり16ビット，浮動小数点型32ビット，最大3チャンネルの深度でフィルタリングされる入力画像．
confidence : confidence image with unsigned 8-bit or floating-point 32-bit confidence and 1 channel.,confidence : 符号なし8ビット，または，浮動小数点型32ビットの信頼度を持つ，1チャンネルの信頼度画像．
"NoteConfidence images with CV_8U depth are expected to in [0, 255] and CV_32F in [0, 1] range.","注深さが CV_8U の信頼度画像は [0, 255]，CV_32F の信頼度画像は [0, 1] の範囲になると思われます．"
"Factory method, create instance of FastBilateralSolverFilter and execute the initialization routines.",ファクトリーメソッドで，FastBilateralSolverFilter のインスタンスを生成し，初期化ルーチンを実行します．
guide : image serving as guide for filtering. It should have 8-bit depth and either 1 or 3 channels.,guide : フィルタリングのガイドとなる画像。深度は8ビット，チャンネル数は1または3です．
"sigma_spatial : parameter, that is similar to spatial space sigma (bandwidth) in bilateralFilter.",sigma_spatial : bilateralFilter の空間シグマ（バンド幅）に相当するパラメータ．
"sigma_luma : parameter, that is similar to luma space sigma (bandwidth) in bilateralFilter.",sigma_luma : bilateralFilterのルーマ空間のシグマ（バンド幅）に相当するパラメータです。
"sigma_chroma : parameter, that is similar to chroma space sigma (bandwidth) in bilateralFilter.",sigma_chroma : bilateralFilterのクロマ空間のシグマ（バンド幅）に相当するパラメータです。
lambda : smoothness strength parameter for solver.,lambda : ソルバーの平滑度パラメータ。
"num_iter : number of iterations used for solver, 25 is usually enough.",num_iter : ソルバーで使う反復回数。
max_tol : convergence tolerance used for solver.,max_tol : ソルバーが使用する収束の許容範囲。
"For more details about the Fast Bilateral Solver parameters, see the original paper [15].",Fast Bilateral Solverのパラメータについての詳細は、オリジナルの論文[15]を参照してください。
Simple one-line Fast Bilateral Solver filter call. If you have multiple images to filter with the same guide then use FastBilateralSolverFilter interface to avoid extra computations.,シンプルな一行の高速バイラテラルソルバーのフィルターコール。同じガイドで複数の画像をフィルタリングする場合は、FastBilateralSolverFilterインターフェースを使うと、余分な計算をしなくて済みます。
src : source image for filtering with unsigned 8-bit or signed 16-bit or floating-point 32-bit depth and up to 4 channels.,src : 符号なし8ビット、符号あり16ビット、浮動小数点32ビット、最大4チャンネルの深度でフィルタリングするためのソース画像。
"For more details about the Fast Bilateral Solver parameters, see the original paper [15].NoteConfidence images with CV_8U depth are expected to in [0, 255] and CV_32F in [0, 1] range.","高速バイラテラルソルバーのパラメータに関する詳細は，原著論文[15]を参照してください．注意深さが CV_8U の信頼度画像は [0, 255]，CV_32F の信頼度画像は [0, 1]の範囲に収まることが期待されます．"
Interface for implementations of Fast Global Smoother filter. ,Fast Global Smootherフィルタを実装するためのインタフェース．
For more details about this filter see [172] and [68] . ,このフィルタの詳細については，[172]と[68]を参照してください．
"Factory method, create instance of FastGlobalSmootherFilter and execute the initialization routines.",FastGlobalSmootherFilter のインスタンスを生成し，初期化ルーチンを実行するファクトリメソッド．
lambda : parameter defining the amount of regularization,lambda : 正則化の量を定義するパラメータ。
"sigma_color : parameter, that is similar to color space sigma in bilateralFilter.",sigma_color : bilateralFilter の色空間シグマに相当するパラメータ。
"lambda_attenuation : internal parameter, defining how much lambda decreases after each iteration. Normally, it should be 0.25. Setting it to 1.0 may lead to streaking artifacts.",lambda_attenuation : 内部パラメータで、繰り返し処理を行うたびに、どのくらいlambdaが減少するかを定義します。通常は0.25に設定します。1.0に設定すると、ストリーキング現象が発生する可能性があります。
"num_iter : number of iterations used for filtering, 3 is usually enough.",num_iter : フィルタリングに使用する反復回数、通常は3回で十分です。
"For more details about Fast Global Smoother parameters, see the original paper [172]. However, please note that there are several differences. Lambda attenuation described in the paper is implemented a bit differently so do not expect the results to be identical to those from the paper; sigma_color values from the paper should be multiplied by 255.0 to achieve the same effect. Also, in case of image filtering where source and guide image are the same, authors propose to dynamically update the guide image after each iteration. To maximize the performance this feature was not implemented here.",Fast Global Smootherのパラメータの詳細については、オリジナルの論文[172]を参照してください。ただし、いくつかの違いがあることに注意してください。論文に記載されているラムダ減衰は、実装方法が少し異なるため、論文と同じ結果を期待してはいけません。論文に記載されているsigma_colorの値は、同じ効果を得るために255.0を乗じる必要があります。また，ソース画像とガイド画像が同じである画像フィルタリングの場合，著者は，各反復の後にガイド画像を動的に更新することを提案しています．性能を最大限に引き出すため，この機能はここでは実装されていない．
Simple one-line Fast Global Smoother filter call. If you have multiple images to filter with the same guide then use FastGlobalSmootherFilter interface to avoid extra computations.,シンプルな1行のFast Global Smootherフィルターコール。同じガイドで複数の画像をフィルタリングする場合は、FastGlobalSmootherFilterインターフェイスを使うと、余分な計算をしなくて済みます。
Global image smoothing via L0 gradient minimization.,L0勾配最小化によるグローバルな画像スムージング。
src : source image for filtering with unsigned 8-bit or signed 16-bit or floating-point depth.,src : 符号なし8ビット、符号あり16ビット、または浮動小数点の深度でフィルタリングするためのソース画像。
lambda : parameter defining the smooth term weight.,lambda : 平滑項の重みを定義するパラメータ．
kappa : parameter defining the increasing factor of the weight of the gradient data term.,kappa : 勾配データ項の重みの増加係数を定義するパラメータ．
"For more details about L0 Smoother, see the original paper [280].",L0 Smootherの詳細については，原著論文[280]を参照してください．
Class implementing the FLD (Fast Line Detector) algorithm described in [136] . ,136]に記載されているFLD(Fast Line Detector)アルゴリズムを実装したクラス。
"#include <iostream>#include ""opencv2/imgproc.hpp""#include ""opencv2/ximgproc.hpp""#include ""opencv2/imgcodecs.hpp""#include ""opencv2/highgui.hpp""using namespace std;using namespace cv;using namespace cv::ximgproc;int main(int argc, char** argv){    string in;    CommandLineParser parser(argc, argv, ""{@input|corridor.jpg|input image}{help h||show help message}"");    if (parser.has(""help""))    {        parser.printMessage();        return 0;    }    in = samples::findFile(parser.get<string>(""@input""));    Mat image = imread(in, IMREAD_GRAYSCALE);    if( image.empty() )    {        return -1;    }    // Create FLD detector    // Param               Default value   Description    // length_threshold    10            - Segments shorter than this will be discarded    // distance_threshold  1.41421356    - A point placed from a hypothesis line    //                                     segment farther than this will be    //                                     regarded as an outlier    // canny_th1           50            - First threshold for    //                                     hysteresis procedure in Canny()    // canny_th2           50            - Second threshold for    //                                     hysteresis procedure in Canny()    // canny_aperture_size 3            - Aperturesize for the sobel operator in Canny().    //                                     If zero, Canny() is not applied and the input    //                                     image is taken as an edge image.    // do_merge            false         - If true, incremental merging of segments    //                                     will be performed    int length_threshold = 10;    float distance_threshold = 1.41421356f;    double canny_th1 = 50.0;    double canny_th2 = 50.0;    int canny_aperture_size = 3;    bool do_merge = false;    Ptr<FastLineDetector> fld = createFastLineDetector(length_threshold,            distance_threshold, canny_th1, canny_th2, canny_aperture_size,            do_merge);    vector<Vec4f> lines;    // Because of some CPU's power strategy, it seems that the first running of    // an algorithm takes much longer. So here we run the algorithm 10 times    // to see the algorithm's processing time with sufficiently warmed-up    // CPU performance.    for (int run_count = 0; run_count < 5; run_count++) {        double freq = getTickFrequency();        lines.clear();        int64 start = getTickCount();        // Detect the lines with FLD        fld->detect(image, lines);        double duration_ms = double(getTickCount() - start) * 1000 / freq;        cout << ""Elapsed time for FLD "" << duration_ms << "" ms."" << endl;    }    // Show found lines with FLD    Mat line_image_fld(image);    fld->drawSegments(line_image_fld, lines);    imshow(""FLD result"", line_image_fld);    waitKey(1);    Ptr<EdgeDrawing> ed = createEdgeDrawing();    ed->params.EdgeDetectionOperator = EdgeDrawing::SOBEL;    ed->params.GradientThresholdValue = 38;    ed->params.AnchorThresholdValue = 8;    vector<Vec6d> ellipses;    for (int run_count = 0; run_count < 5; run_count++) {        double freq = getTickFrequency();        lines.clear();        int64 start = getTickCount();        // Detect edges        //you should call this before detectLines() and detectEllipses()        ed->detectEdges(image);        // Detect lines        ed->detectLines(lines);        double duration_ms = double(getTickCount() - start) * 1000 / freq;        cout << ""Elapsed time for EdgeDrawing detectLines "" << duration_ms << "" ms."" << endl;        start = getTickCount();        // Detect circles and ellipses        ed->detectEllipses(ellipses);        duration_ms = double(getTickCount() - start) * 1000 / freq;        cout << ""Elapsed time for EdgeDrawing detectEllipses "" << duration_ms << "" ms."" << endl;    }    Mat edge_image_ed = Mat::zeros(image.size(), CV_8UC3);    vector<vector<Point> > segments = ed->getSegments();    for (size_t i = 0; i < segments.size(); i++)    {        const Point* pts = &segments[i][0];        int n = (int)segments[i].size();        polylines(edge_image_ed, &pts, &n, 1, false, Scalar((rand() & 255), (rand() & 255), (rand() & 255)), 1);    }    imshow(""EdgeDrawing detected edges"", edge_image_ed);    Mat line_image_ed(image);    fld->drawSegments(line_image_ed, lines);    // Draw circles and ellipses    for (size_t i = 0; i < ellipses.size(); i++)    {        Point center((int)ellipses[i][0], (int)ellipses[i][1]);        Size axes((int)ellipses[i][2] + (int)ellipses[i][3], (int)ellipses[i][2] + (int)ellipses[i][4]);        double angle(ellipses[i][5]);        Scalar color = ellipses[i][2] == 0 ? Scalar(255, 255, 0) : Scalar(0, 255, 0);        ellipse(line_image_ed, center, axes, angle, 0, 360, color, 2, LINE_AA);    }    imshow(""EdgeDrawing result"", line_image_ed);    waitKey();    return 0;} ","#include <iostream>#include ""opencv2/imgproc.hpp ""#include ""opencv2/ximgproc.hpp ""#include ""opencv2/imgcodecs.hpp ""#include ""opencv2/highgui.hpp ""using namespace std;using namespace cv;using namespace cv::ximgproc;int main(int argc, char** argv){ string in; CommandLineParser parser(argc, argv, ""{@input|corridor.jpg|input image}{help h||show help message}""); if (parser.has(""help"")) { parser.printMessage(); return 0; } in = samples::findFile(parser.get<string>(""@input"")); Mat image = imread(in, IMREAD_GRAYSCALE); if( image.empty() )    { return -1; }.    // FLD 検出器の作成 // Param Default value Description // length_threshold 10 - これよりも短いセグメントは破棄されます // distance_threshold 1.41421356 - 仮説線から // これよりも遠いセグメントに置かれた点は // 外れ値とみなされる // canny_th1 50 - Canny() のヒステリシス処理のための // 第 1 の閾値 // canny_th2 50 - Canny() のヒステリシス処理のための // 第 2 の閾値 // canny_aperture_size 3 - Canny() の sobel 演算子のための開口サイズ．    // 0の場合，Canny()は適用されず，入力画像は // エッジ画像として扱われます．    // do_merge false - true の場合，セグメントのインクリメンタルなマージが // 行われます int length_threshold = 10; float distance_threshold = 1.41421356f; double canny_th1 = 50.0; double canny_th2 = 50.0; int canny_aperture_size = 3; bool do_merge = false; Ptr<FastLineDetector> fld = createFastLineDetector(length_threshold, distance_threshold, canny_th1, canny_th2, canny_aperture_size, do_merge); vector<Vec4f> lines; // CPUのパワーストラテジーによっては、アルゴリズムの最初の実行に時間がかかることがあるようです。for (int run_count = 0; run_count < 5; run_count++) { double freq = getTickFrequency(); lines.clear(); int64 start = getTickCount(); // FLDでラインを検出 fld->detect(image, lines); double duration_ms = double(getTickCount() - start) * 1000 / freq; cout << ""FLDの経過時間 "" << duration_ms << "" ms."".<< endl; }。    // FLD で見つかった線を表示する Mat line_image_fld(image); fld->drawSegments(line_image_fld, lines); imshow(""FLD result"", line_image_fld); waitKey(1); Ptr<EdgeDrawing> ed = createEdgeDrawing(); ed->params.ed->params.EdgeDetectionOperator = EdgeDrawing::SOBEL; ed->params.GradientThresholdValue = 38; ed->params.AnchorThresholdValue = 8; vector<Vec6d> ellipses; for (int run_count = 0; run_count < 5; run_count++) { double freq = getTickFrequency(); lines.clear(); int64 start = getTickCount(); // エッジを検出する // detectLines() と detectEllipses() の前に呼び出す必要がある ed->detectEdges(image); // ラインを検出する ed->detectLines(lines); double duration_ms = double(getTickCount() - start) * 1000 / freq; cout << ""EdgeDrawing の経過時間 detectLines "" << duration_ms << "" ms."".<< endl; start = getTickCount(); // 円と楕円の検出 ed->detectEllipses(ellipses); duration_ms = double(getTickCount() - start) * 1000 / freq; cout << ""EdgeDrawing detectEllipsesの経過時間 "" << duration_ms << "" ms."".<< endl; }。    Mat edge_image_ed = Mat::zeros(image.size(), CV_8UC3); vector<vector<Point> > segments = ed->getSegments(); for (size_t i = 0; i < segments.size(); i++) { const Point* pts = &segments[i][0]; int n = (int)segments[i].size(); polylines(edge_image_ed, &pts, &n, 1, false, Scalar((rand() & 255), (rand() & 255), (rand() & 255), 1); } imshow(""EdgeDrawing detected edges"", edge_image_ed); Mat line_image_ed(image); fld->drawSegments(line_image_ed, lines); // 円と楕円の描画 for (size_t i = 0; i < ellipses.size(); i++) { Point center((int)ellipses[i][0], (int)ellipses[i][1]);        Size axes((int)ellipses[i][2] + (int)ellipses[i][3], (int)ellipses[i][2] + (int)ellipses[i][4]); double angle(ellipses[i][5]); Scalar color = ellipses[i][2] == 0 ?Scalar(255, 255, 0) : Scalar(0, 255, 0); ellipse(line_image_ed, center, axes, angle, 0, 360, color, 2, LINE_AA); } imshow(""EdgeDrawing result"", line_image_ed); waitKey(); return 0;}。"
Finds lines in the input image. This is the output of the default parameters of the algorithm on the above shown image.,入力画像から線を見つけます。これは，上に示した画像にアルゴリズムのデフォルトパラメータを適用した場合の出力です．
"image : A grayscale (CV_8UC1) input image. If only a roi needs to be selected, use: fld_ptr-\>detect(image(roi), lines, ...); lines += Scalar(roi.x, roi.y, roi.x, roi.y);","image : グレースケール（CV_8UC1）の入力画像．roi だけを選択する必要がある場合は，次のようにします： fld_ptr-#>detect(image(roi), lines, ...); lines += Scalar(roi.x, roi.y, roi.x, roi.y);"
"lines : A vector of Vec4f elements specifying the beginning and ending point of a line. Where Vec4f is (x1, y1, x2, y2), point 1 is the start, point 2 - end. Returned lines are directed so that the brighter side is on their left.","lines : ラインの始点と終点を指定するVec4f要素のベクトル。Vec4fが(x1, y1, x2, y2)の場合、点1が開始点、点2が終了点となる。返された線は、明るい方が左になるように指示されます。"
image,イメージ
Draws the line segments on a given image.,与えられた画像上に線分を描画します。
"image : The image, where the lines will be drawn. Should be bigger or equal to the image, where the lines were found.",image : 線分が描画される画像です。線が発見された画像と同じかそれ以上でなければなりません。
lines : A vector of the lines that needed to be drawn.,lines : 描画する必要のある線のベクトル。
"draw_arrow : If true, arrow heads will be drawn.",draw_arrow : trueの場合、矢印の頭が描かれます。
linecolor : Line color.,linecolor : 線の色。
linethickness : Line thickness.,linethickness : 線の太さ。
Creates a smart pointer to a FastLineDetector object and initializes it.,FastLineDetector オブジェクトへのスマート ポインタを作成し、それを初期化します。
length_threshold : Segment shorter than this will be discarded,length_threshold : これより短いセグメントは破棄されます。
distance_threshold : A point placed from a hypothesis line segment farther than this will be regarded as an outlier,distance_threshold : 仮説線分からこれより遠い位置にある点は異常値とみなされます。
canny_th1 : First threshold for hysteresis procedure in Canny(),canny_th1 : Canny()のヒステリシス処理のための第一閾値
canny_th2 : Second threshold for hysteresis procedure in Canny(),canny_th2 : Canny()のヒステリシス処理のための第2の閾値
"canny_aperture_size : Aperturesize for the sobel operator in Canny(). If zero, Canny() is not applied and the input image is taken as an edge image.",canny_aperture_size : Canny()のsobel演算子の開口サイズ。0の場合，Canny()は適用されず，入力画像はエッジ画像として扱われる．
"do_merge : If true, incremental merging of segments will be performed",do_merge : trueの場合，セグメントのインクリメンタル・マージが行われる．
Examples: fld_lines.cpp.,例：fld_lines.cpp.
Create pointer to the Ridge detection filter.,リッジ検出フィルタへのポインタを作成します．
ddepth : Specifies output image depth. Defualt is CV_32FC1,ddepth : 出力画像の深度を指定します．初期値は CV_32FC1 です．
"dx : Order of derivative x, default is 1",dx : 微分xの次数，デフォルトは1．
"dy : Order of derivative y, default is 1",dy : 微分yの次数，デフォルトは1．
"ksize : Sobel kernel size , default is 3",ksize : Sobelカーネルサイズ，デフォルトは3．
"out_dtype : Converted format for output, default is CV_8UC1",out_dtype :out_dtype : 出力形式の変換，デフォルトは CV_8UC1．
"scale : Optional scale value for derivative values, default is 1",scale : 微分値に対する任意のスケール値，デフォルトは1．
"delta : Optional bias added to output, default is 0",delta :オプションで出力に追加されるバイアス，デフォルトは0
"borderType : Pixel extrapolation method, default is BORDER_DEFAULT",borderType :ピクセル外挿法，デフォルトは BORDER_DEFAULT．
"See alsoSobel, threshold, getStructuringElement, morphologyEx.( for additional refinement)","Sobel, threshold, getStructuringElement, morphologyEx.も参照してください（さらなる改良のため）．"
Apply Ridge detection filter on input image.,リッジ検出フィルタを入力画像に適用します．
_img : InputArray as supported by Sobel. img can be 1-Channel or 3-Channels.,_img : Sobelがサポートする入力配列．imgは1チャンネルまたは3チャンネルである．
out : OutputAray of structure as RidgeDetectionFilter::ddepth. Output image with ridges.,out :RidgeDetectionFilter::ddepth と同じ構造の出力配列．稜線のある画像を出力します．
Applies Ridge Detection Filter to an input image. Implements Ridge detection similar to the one in Mathematica using the eigen values from the Hessian Matrix of the input image using Sobel Derivatives. Additional refinement can be done using Skeletonization and Binarization. Adapted from [66] and [165]. ,Ridge Detection Filter を入力画像に適用します。入力画像のヘシアン行列の固有値を使った Mathematica と同様のリッジ検出を，Sobel 微分を使って実行します．スケルトン化や2値化を使って，さらに精密化することができます．66]および[165]より引用。
Creates a graph based segmentor.,グラフベースのセメンタを作成します。
"sigma : The sigma parameter, used to smooth image",sigma : 画像の平滑化に利用されるシグマパラメータ．
k : The k parameter of the algorythm,k : アルゴリスムのkパラメータ
min_size : The minimum size of segments,min_size : セグメントの最小サイズ
Graph Based Segmentation Algorithm. The class implements the algorithm described in [73] . ,グラフベースのセグメンテーションアルゴリズム．このクラスは，[73]で述べられているアルゴリズムを実装しています．
Segment an image and store output in dst.,画像を分割し，その出力を dst に格納します．
"src : The input image. Any number of channel (1 (Eg: Gray), 3 (Eg: RGB), 4 (Eg: RGB-D)) can be provided","src : 入力画像．任意のチャンネル（1 (Eg: Gray), 3 (Eg: RGB), 4 (Eg: RGB-D)）を指定することができます．"
"dst : The output segmentation. It's a CV_32SC1 Mat with the same number of cols and rows as input image, with an unique, sequential, id for each pixel.",dst : 出力されるセグメンテーション画像．これは，入力画像と同じ数の列と行を持つ CV_32SC1 マットであり，各ピクセルには一意の連続した ID が与えられます．
"Set a initial image, with a segmentation.",セグメンテーションされた初期画像をセットします．
img : The input image. Any number of channel can be provided,img : 入力画像．任意のチャンネル数を指定できます．
regions : A segmentation of the image. The parameter must be the same size of img.,regions :画像を分割したもの．imgと同じサイズのパラメータを指定する必要があります．
sizes : The sizes of different regions,sizes : 各領域のサイズ
"image_id : If not set to -1, try to cache pre-computations. If the same set og (img, regions, size) is used, the image_id need to be the same.","image_id : -1 に設定されていない場合は、事前計算のキャッシュを試みます。同じセットog（img, regions, size）を使う場合は、image_idも同じである必要があります。"
Return the score between two regions (between 0 and 1),2つのリージョン間のスコア（0〜1）を返す
r1 : The first region,r1 : 1つ目のリージョン
r2 : The second region,r2 : 2番目のリージョン
Inform the strategy that two regions will be merged.,2つのリージョンが統合されることをストラテジーに伝えます。
Create a new color-based strategy.,新しい色ベースの戦略を作る。
Create a new size-based strategy.,サイズベースのストラテジーを作成します。
Create a new fill-based strategy.,塗り潰しストラテジーを新規に作成します。
Color-based strategy for the selective search segmentation algorithm The class is implemented from the algorithm described in [252]. ,選択的探索セグメンテーションアルゴリズムのためのカラーベースストラテジー このクラスは、[252]に記載されているアルゴリズムから実装されています。
Size-based strategy for the selective search segmentation algorithm The class is implemented from the algorithm described in [252]. ,サイズベース戦略 選択的検索セグメンテーションアルゴリズムのためのサイズベース戦略 このクラスは、[252]で説明されているアルゴリズムから実装されています。
Texture-based strategy for the selective search segmentation algorithm The class is implemented from the algorithm described in [252]. ,このクラスは、[252]で説明したアルゴリズムから実装されています。
Fill-based strategy for the selective search segmentation algorithm The class is implemented from the algorithm described in [252]. ,選択的検索セグメンテーションアルゴリズムのためのフィルベースの戦略 このクラスは、[252]で説明したアルゴリズムから実装されています。
Add a new sub-strategy.,新しいサブストラテジーを追加します。
g : The strategy,g : 戦略の
weight : The weight of the strategy,weight : ストラテジーの重み
Remove all sub-strategies.,すべてのサブストラテジーを削除する。
Create a new multiple strategy and set one subtrategy.,新しい多重戦略を作成し、サブ戦略を1つ設定する。
s1 : The first strategy,s1 : 最初のストラテジー
Regroup multiple strategies for the selective search segmentation algorithm. ,選択的探索セグメンテーションアルゴリズムのために、複数のストラテジーを再編成する。
"Create a new multiple strategy and set three subtrategies, with equal weights.",新しいマルチ戦略を作成し、3つのサブ戦略を同じ重みで設定する。
s2 : The second strategy,s2 : 第2のストラテジー
s3 : The third strategy,s3 : 第3のストラテジー
"Create a new multiple strategy and set four subtrategies, with equal weights.",新規に複数のストラテジーを作成し、4つのサブストラテジーを均等なウェイトで設定します。
s4 : The forth strategy,s4 : 第4のストラテジー
Set a image used by switch* functions to initialize the class.,switch*関数がクラスを初期化する際に使用するイメージを設定します。
img : The image,img : 画像
Initialize the class with the 'Single stragegy' parameters describled in [252].,252]で説明した「Single stragegy」パラメータでクラスを初期化する。
k : The k parameter for the graph segmentation,k : グラフ分割のためのkパラメータ
sigma : The sigma parameter for the graph segmentation,sigma : グラフ分割のためのシグマ・パラメータ
Initialize the class with the 'Selective search fast' parameters describled in [252].,252]で述べた「Selective search fast」パラメータでクラスを初期化します。
base_k : The k parameter for the first graph segmentation,base_k : 最初のグラフ分割のkパラメータ
inc_k : The increment of the k parameter for all graph segmentations,inc_k : すべてのグラフ分割に対するkパラメータの増加分
Add a new image in the list of images to process.,処理する画像のリストに新しい画像を追加します。
Clear the list of images to process.,処理する画像のリストを消去します。
Add a new graph segmentation in the list of graph segementations to process.,処理するグラフセグメンテーションのリストに新しいグラフセグメンテーションを追加します。
g : The graph segmentation,g : グラフ・セグメンテーション
Clear the list of graph segmentations to process;.,処理するグラフセグメンテーションのリストを消去する；。
Add a new strategy in the list of strategy to process.,処理するストラテジのリストに新しいストラテジを追加する。
s : The strategy,s :ストラテジー
Clear the list of strategy to process;.,処理するストラテジーのリストを消去する；。
"Based on all images, graph segmentations and stragies, computes all possible rects and return them.",全ての画像、グラフ分割、stragies に基づいて、可能な全ての rects を計算し、それらを返す。
rects : The list of rects. The first ones are more relevents than the lasts ones.,rects :矩形領域のリスト．最初のものは、最後のものよりも関連性が高いです。
Selective search segmentation algorithm The class implements the algorithm described in [252]. ,選択的探索セグメンテーションアルゴリズム このクラスは，[252]で述べられているアルゴリズムを実装しています．
"Helper class for training part of [P. Dollar and C. L. Zitnick. Structured Forests for Fast Edge Detection, 2013]. ","P. Dollar and C. L. Zitit] の学習部分のヘルパークラスです．Dollar and C. L. Zitnick.Structured Forests for Fast Edge Detection, 2013] のトレーニング部分のヘルパークラスです．"
This functions extracts feature channels from src. Than StructureEdgeDetection uses this feature space to detect edges.,この関数は，srcから特徴チャンネルを抽出します．StructureEdgeDetection は，この特徴空間を利用してエッジを検出します．
src : : source image to extract features,src : : 特徴を抽出するための入力画像
features : : output n-channel floating point feature matrix.,features : nチャンネル浮動小数点型特徴量行列を出力．
gnrmRad : : __rf.options.gradientNormalizationRadius,gnrmRad : :__rf.options.gradientNormalizationRadius
gsmthRad : : __rf.options.gradientSmoothingRadius,gsmthRad : :__rf.options.gradientSmoothingRadius
shrink : : __rf.options.shrinkNumber,shrink : :__rf.options.shrinkNumber
outNum : : __rf.options.numberOfOutputChannels,outNum : :__rf.options.numberOfOutputChannels
gradNum : : __rf.options.numberOfGradientOrientations,gradNum : :__rf.options.numberOfGradientOrientations
The only constructor,唯一のコンストラクタ
model : : name of the file where the model is stored,model : : モデルが保存されているファイルの名前
"howToGetFeatures : : optional object inheriting from RFFeatureGetter. You need it only if you would like to train your own forest, pass NULL otherwise",howToGetFeatures : : RFFeatureGetterを継承したオプションのオブジェクトです。自分の森を学習したい場合にのみ必要で，そうでない場合はNULLを渡します．
Class implementing edge detection algorithm from [57] : ,57]のエッジ検出アルゴリズムを実装したクラス．
The function detects edges in src and draw them to dst.,この関数は，srcからエッジを検出し，それをdstに描画します．
"_src : source image (RGB, float, in [0;1]) to detect edges","_src : エッジを検出するための入力画像（RGB, float, in [0;1]）．"
"_dst : destination image (grayscale, float, in [0;1]) where edges are drawn","_dst : 輪郭が描画される出力画像（グレースケール，float, in [0;1] ）．"
"The algorithm underlies this function is much more robust to texture presence, than common approaches, e.g. SobelSee alsoSobel, Canny",この関数を支えるアルゴリズムは，例えば Sobel などの一般的な手法よりも，テクスチャの存在に対してより頑健です．
The function computes orientation from edge image.,この関数は，エッジ画像から向きを求めます．
_src : edge image.,_src : 輪郭画像．
_dst : orientation image.,_dst : 輪郭画像．
The function edgenms in edge image and suppress edges where edge is stronger in orthogonal direction.,この関数は，エッジ画像中のエッジを検出し，直交する方向に強いエッジを抑制します．
edge_image : edge image from detectEdges function.,edge_image : detectEdges関数によるエッジ画像。
orientation_image : orientation image from computeOrientation function.,orientation_image : computeOrientation関数によるオリエンテーション画像．
"_dst : suppressed image (grayscale, float, in [0;1])","_dst : 抑制された画像（グレースケール，float, in [0;1])"
r : radius for NMS suppression.,r : NMSを抑制するための半径．
s : radius for boundary suppression.,s : 境界線を抑制するための半径．
m : multiplier for conservative suppression.,m : 保守的抑圧のための乗数。
isParallel : enables/disables parallel computing.,isParallel : 並列計算の有効／無効を設定します．
Class implementing the LSC (Linear Spectral Clustering) superpixels algorithm described in [141]. ,141]で述べられているLSC (Linear Spectral Clustering) スーパーピクセルアルゴリズムを実装したクラスです．
"LSC (Linear Spectral Clustering) produces compact and uniform superpixels with low computational costs. Basically, a normalized cuts formulation of the superpixel segmentation is adopted based on a similarity metric that measures the color similarity and space proximity between image pixels. LSC is of linear computational complexity and high memory efficiency and is able to preserve global properties of images ",LSC (Linear Spectral Clustering) は，コンパクトで均一なスーパーピクセルを低い計算コストで生成します．基本的には、画像ピクセル間の色の類似性と空間の近接性を測定する類似性メトリックに基づいて、スーパーピクセルのセグメンテーションの正規化されたカットの定式化が採用されます。LSCは直線的な計算量と高いメモリ効率を持ち、画像の大域的な特性を保持することができます。
Calculates the actual amount of superpixels on a given segmentation computed and stored in SuperpixelLSC object.,SuperpixelLSCオブジェクトに格納された、与えられたセグメンテーション上の実際のスーパーピクセルの量を計算します。
Calculates the superpixel segmentation on a given image with the initialized parameters in the SuperpixelLSC object.,SuperpixelLSC オブジェクトに初期化されたパラメータを用いて、与えられた画像上のスーパーピクセル・セグメンテーションを計算します。
num_iterations : Number of iterations. Higher number improves the result.,num_iterations :反復処理の回数。数値が大きいほど結果が良くなります。
This function can be called again without the need of initializing the algorithm with createSuperpixelLSC(). This save the computational cost of allocating memory for all the structures of the algorithm.The function computes the superpixels segmentation of an image with the parameters initialized with the function createSuperpixelLSC(). The algorithms starts from a grid of superpixels and then refines the boundaries by proposing updates of edges boundaries.,この関数は，createSuperpixelLSC()でアルゴリズムを初期化することなく，再度呼び出すことができます．この関数は，関数 createSuperpixelLSC() で初期化されたパラメータを用いて，画像のスーパープリントセグメンテーションを計算します．このアルゴリズムは，スーパーピクセルのグリッドから始まり，エッジの境界の更新を提案することで，その境界を精密化します．
Returns the segmentation labeling of the image.,画像のセグメンテーションラベリングを返します．
"labels_out : Return: A CV_32SC1 integer array containing the labels of the superpixel segmentation. The labels are in the range [0, getNumberOfSuperpixels()].","labels_out :戻り値．スーパーピクセル・セグメンテーションのラベルを含む CV_32SC1 integer array．ラベルは， [0, getNumberOfSuperpixels()] の範囲にあります．"
"Each label represents a superpixel, and each pixel is assigned to one superpixel label.The function returns an image with the labels of the superpixel segmentation. The labels are in the range [0, getNumberOfSuperpixels()].","各ラベルはスーパーピクセルを表し，各ピクセルは1つのスーパーピクセルラベルに割り当てられます．この関数は，スーパーピクセル・セグメンテーションのラベルを含む画像を返します．ラベルは [0, getNumberOfSuperpixels()] の範囲にあります。"
Returns the mask of the superpixel segmentation stored in SuperpixelLSC object.,SuperpixelLSCオブジェクトに格納されているスーパーピクセル・セグメンテーションのマスクを返します。
"image : Return: CV_8U1 image mask where -1 indicates that the pixel is a superpixel border, and 0 otherwise.",image : 戻ります．CV_8U1 画像のマスク．ここで，-1 はそのピクセルがスーパーピクセルの境界であることを示し，それ以外は 0 です．
"thick_line : If false, the border is only one pixel wide, otherwise all pixels at the border are masked.",thick_line : falseの場合，境界は1ピクセルの幅しかなく，そうでなければ境界上のすべてのピクセルがマスクされます．
The function return the boundaries of the superpixel segmentation.,この関数は，スーパーピクセル・セグメンテーションの境界を返します．
Enforce label connectivity.,ラベルの接続性を強制します。
"min_element_size : The minimum element size in percents that should be absorbed into a bigger superpixel. Given resulted average superpixel size valid value should be in 0-100 range, 25 means that less then a quarter sized superpixel should be absorbed, this is default.",min_element_size : より大きなスーパーピクセルに吸収されるべき最小要素サイズをパーセンテージで指定します。平均的なスーパーピクセルの大きさを考えると、0-100の範囲が有効な値となり、25は1/4以下の大きさのスーパーピクセルが吸収されることを意味します。
"The function merge component that is too small, assigning the previously found adjacent label to this component. Calling this function may change the final number of superpixels.",この関数は，小さすぎるコンポーネントをマージし，以前に見つかった隣接ラベルをこのコンポーネントに割り当てます．この関数を呼び出すと，最終的なスーパピクセルの数が変わる可能性があります．
Class implementing the LSC (Linear Spectral Clustering) superpixels.,LSC (Linear Spectral Clustering) スーパーピクセルを実装するクラス．
image : Image to segment,image : 分割する画像
region_size : Chooses an average superpixel size measured in pixels,region_size : スーパーピクセルの平均的な大きさをピクセル単位で指定します．
ratio : Chooses the enforcement of superpixel compactness factor of superpixel,ratio :superpixelの施行方法を選択します． superpixelのコンパクト化係数を選択します．
"The function initializes a SuperpixelLSC object for the input image. It sets the parameters of superpixel algorithm, which are: region_size and ruler. It preallocate some buffers for future computing iterations over the given image. An example of LSC is ilustrated in the following picture. For enanched results it is recommended for color images to preprocess image with little gaussian blur with a small 3 x 3 kernel and additional conversion into CieLAB color space.image",この関数は，入力画像に対して SuperpixelLSC オブジェクトを初期化します．Superpixelアルゴリズムのパラメータである，region_sizeとrulerを設定します．また，与えられた画像に対する今後の計算反復のために，いくつかのバッファを事前に確保します．LSCの例を以下の図に示します。カラー画像の場合は、3×3の小さなカーネルでガウスぼかしをかけ、さらにCieLABの色空間に変換するなどの前処理を行うと、よりよい結果が得られるでしょう。
Class implementing the SEEDS (Superpixels Extracted via Energy-Driven Sampling) superpixels algorithm described in [257] . ,SEEDS (Superpixels Extracted via Energy-Driven Sampling) Superpixels アルゴリズムを実装したクラス [257] 。
"The algorithm uses an efficient hill-climbing algorithm to optimize the superpixels' energy function that is based on color histograms and a boundary term, which is optional. The energy function encourages superpixels to be of the same color, and if the boundary term is activated, the superpixels have smooth boundaries and are of similar shape. In practice it starts from a regular grid of superpixels and moves the pixels or blocks of pixels at the boundaries to refine the solution. The algorithm runs in real-time using a single CPU. ",このアルゴリズムでは，効率的な丘登りアルゴリズムを用いて，カラーヒストグラムとオプションである境界項に基づいたスーパーピクセルのエネルギー関数を最適化します．エネルギー関数は、スーパーピクセルが同じ色になるように促し、境界項が有効な場合、スーパーピクセルは滑らかな境界を持ち、似た形状になります。実際には、スーパーピクセルの規則的なグリッドからスタートし、境界にあるピクセルまたはピクセルのブロックを移動させて解を洗練させます。このアルゴリズムは、1つのCPUを使ってリアルタイムで実行されます。
Calculates the superpixel segmentation on a given image stored in SuperpixelSEEDS object.,SuperpixelSEEDS オブジェクトに格納されている，与えられた画像のスーパーピクセル分割を計算します．
The function computes the superpixels segmentation of an image with the parameters initialized with the function createSuperpixelSEEDS().,この関数は，関数 createSuperpixelSEEDS() で初期化されたパラメータを用いて，画像のスーパープixel segmentationを計算します．
Calculates the superpixel segmentation on a given image with the initialized parameters in the SuperpixelSEEDS object.,SuperpixelSEEDSオブジェクトに初期化されたパラメータを用いて，与えられた画像のスーパープリントセグメンテーションを計算します．
"img : Input image. Supported formats: CV_8U, CV_16U, CV_32F. Image size & number of channels must match with the initialized image size & channels with the function createSuperpixelSEEDS(). It should be in HSV or Lab color space. Lab is a bit better, but also slower.","img : 入力画像．サポートされるフォーマット．cv_8u, cv_16u, cv_32f.画像サイズとチャンネル数は，関数 createSuperpixelSEEDS() で初期化された画像サイズとチャンネル数と一致しなければなりません．画像は，HSV または Lab 色空間でなければなりません．Lab の方が少し良いですが，速度も遅くなります．"
num_iterations : Number of pixel level iterations. Higher number improves the result.,num_iterations :ピクセルレベルでの繰り返し処理の回数．数値が大きいほど，結果が良くなります．
"This function can be called again for other images without the need of initializing the algorithm with createSuperpixelSEEDS(). This save the computational cost of allocating memory for all the structures of the algorithm.The function computes the superpixels segmentation of an image with the parameters initialized with the function createSuperpixelSEEDS(). The algorithms starts from a grid of superpixels and then refines the boundaries by proposing updates of blocks of pixels that lie at the boundaries from large to smaller size, finalizing with proposing pixel updates. An illustrative example can be seen below.image",この関数は， createSuperpixelSEEDS() でアルゴリズムを初期化しなくても，他の画像に対して再度呼び出すことができます．この関数は，関数 createSuperpixelSEEDS() で初期化されたパラメータを用いて，画像のスーパープリントセグメンテーションを計算します．このアルゴリズムは，スーパーピクセルのグリッドから始まり，境界に位置するピクセルのブロックを大きいサイズから小さいサイズに更新することで境界を絞り込み，最後にピクセルの更新を提案します．その例を以下に示します。
"labels_out : Return: A CV_32UC1 integer array containing the labels of the superpixel segmentation. The labels are in the range [0, getNumberOfSuperpixels()].","labels_out :戻り値．スーパーピクセル・セグメンテーションのラベルを含む，CV_32UC1 の整数型配列．ラベルは， [0, getNumberOfSuperpixels()] の範囲にあります．"
"Each label represents a superpixel, and each pixel is assigned to one superpixel label.The function returns an image with ssthe labels of the superpixel segmentation. The labels are in the range [0, getNumberOfSuperpixels()].","各ラベルはスーパーピクセルを表し，各ピクセルは1つのスーパーピクセルラベルに割り当てられます．この関数は，スーパーピクセル・セグメンテーションのラベルを含む画像を返します．ラベルは [0, getNumberOfSuperpixels()] の範囲にあります。"
Returns the mask of the superpixel segmentation stored in SuperpixelSEEDS object.,SuperpixelSEEDSオブジェクトに格納されているスーパーピクセル・セグメンテーションのマスクを返します。
"image : Return: CV_8UC1 image mask where -1 indicates that the pixel is a superpixel border, and 0 otherwise.",image : 戻ります．CV_8UC1 画像のマスク．ここで，-1はそのピクセルがスーパーピクセルの境界であることを示し，それ以外は0です．
The function return the boundaries of the superpixel segmentation.Note,この関数は，スーパーピクセル・セグメンテーションの境界を返します．
(Python) A demo on how to generate superpixels in images from the webcam can be found at opencv_source_code/samples/python2/seeds.py,(Python) Webカメラからの画像にスーパーピクセルを生成する方法のデモは， opencv_source_code/samples/python2/seeds.py にあります．
"(cpp) A demo on how to generate superpixels in images from the webcam can be found at opencv_source_code/modules/ximgproc/samples/seeds.cpp. By adding a file image as a command line argument, the static image will be used instead of the webcam.",(cpp) ウェブカメラからの画像にスーパーピクセルを生成する方法についてのデモは、opencv_source_code/modules/ximgproc/samples/seeds.cpp にあります。コマンドライン引数にファイル画像を追加することで、ウェブカメラの代わりに静止画像が使用されます。
"It will show a window with the video from the webcam with the superpixel boundaries marked in red (see below). Use Space to switch between different output modes. At the top of the window there are 4 sliders, from which the user can change on-the-fly the number of superpixels, the number of block levels, the strength of the boundary prior term to modify the shape, and the number of iterations at pixel level. This is useful to play with the parameters and set them to the user convenience. In the console the frame-rate of the algorithm is indicated.image",ウェブカムからの映像に、スーパーピクセルの境界が赤く表示されたウィンドウが表示されます（下記参照）。出力モードを切り替えるにはSpaceを使います。ウィンドウの上部には4つのスライダがあり、スーパーピクセルの数、ブロックレベルの数、形状を修正するための境界先行項の強さ、ピクセルレベルでの反復回数をその場で変更することができます。これは、パラメータを弄り、ユーザーの都合に合わせて設定するのに便利です。コンソールには，アルゴリズムのフレームレートが表示されます．
Initializes a SuperpixelSEEDS object.,SuperpixelSEEDSオブジェクトを初期化します。
image_width : Image width.,image_width : 画像の幅を指定します。
image_height : Image height.,image_height : 画像の高さ．
image_channels : Number of channels of the image.,image_channels : 画像のチャンネル数．
num_superpixels : Desired number of superpixels. Note that the actual number may be smaller due to restrictions (depending on the image size and num_levels). Use getNumberOfSuperpixels() to get the actual number.,num_superpixels : スーパーピクセルの数を指定します．実際の数は、制限により小さくなる可能性があることに注意してください（画像サイズと num_levels に依存します）。実際の数を得るには、getNumberOfSuperpixels()を使用してください。
"num_levels : Number of block levels. The more levels, the more accurate is the segmentation, but needs more memory and CPU time.",num_levels : ブロックレベルの数。レベルが多ければ多いほどセグメンテーションの精度は高くなりますが、より多くのメモリとCPU時間を必要とします。
"prior : enable 3x3 shape smoothing term if >0. A larger value leads to smoother shapes. prior must be in the range [0, 5].","prior : 3x3 形状平滑化項を有効にします（>0）． 大きな値を設定すると，より滑らかな形状になります． prior は [0, 5] の範囲でなければなりません．"
histogram_bins : Number of histogram bins.,histogram_bins : ヒストグラムのビンの数．
"double_step : If true, iterate each block level twice for higher accuracy.",double_step : trueの場合，より高い精度を得るために，各ブロックレベルを2回反復します．
"The function initializes a SuperpixelSEEDS object for the input image. It stores the parameters of the image: image_width, image_height and image_channels. It also sets the parameters of the SEEDS superpixel algorithm, which are: num_superpixels, num_levels, use_prior, histogram_bins and double_step.The number of levels in num_levels defines the amount of block levels that the algorithm use in the optimization. The initialization is a grid, in which the superpixels are equally distributed through the width and the height of the image. The larger blocks correspond to the superpixel size, and the levels with smaller blocks are formed by dividing the larger blocks into 2 x 2 blocks of pixels, recursively until the smaller block level. An example of initialization of 4 block levels is illustrated in the following figure.image","この関数は，入力画像に対して SuperpixelSEEDS オブジェクトを初期化します．これは，画像のパラメータである image_width, image_height, image_channels を格納します．また，SEEDS スーパーピクセルアルゴリズムのパラメータである num_superpixels, num_levels, use_prior, histogram_bins, double_step も設定されます．num_levels のレベル数は，アルゴリズムが最適化に利用するブロックレベルの量を定義します．初期設定では，スーパーピクセルが画像の幅と高さに均等に配置されたグリッドになっています．大きいブロックはスーパーピクセルのサイズに対応し、小さいブロックのレベルは、大きいブロックを2×2のピクセルのブロックに分割し、小さいブロックのレベルまで再帰的に形成されます。4つのブロックレベルの初期化の例を以下の図に示します。"
Class implementing the SLIC (Simple Linear Iterative Clustering) superpixels algorithm described in [1]. ,1]で紹介したSLIC(Simple Linear Iterative Clustering)スーパーピクセルアルゴリズムを実装したクラス。
"SLIC (Simple Linear Iterative Clustering) clusters pixels using pixel channels and image plane space to efficiently generate compact, nearly uniform superpixels. The simplicity of approach makes it extremely easy to use a lone parameter specifies the number of superpixels and the efficiency of the algorithm makes it very practical. Several optimizations are available for SLIC class: SLICO stands for ""Zero parameter SLIC"" and it is an optimization of baseline SLIC described in [1]. MSLIC stands for ""Manifold SLIC"" and it is an optimization of baseline SLIC described in [150]. ","SLIC(Simple Linear Iterative Clustering)は、画素チャンネルと画像平面の空間を利用して画素をクラスタリングし、コンパクトでほぼ均一なスーパーピクセルを効率的に生成します。SLICのアプローチはシンプルであるため、スーパーピクセルの数を指定するだけで非常に簡単に使用することができ、アルゴリズムの効率性は非常に実用的です。SLICクラスにはいくつかの最適化機能があります。SLICOは、""Zero parameter SLIC ""の略で、[1]で説明したベースラインSLICの最適化です。MSLICは、""Manifold SLIC ""の略で、[150]で説明したベースラインSLICの最適化です。"
Calculates the actual amount of superpixels on a given segmentation computed and stored in SuperpixelSLIC object.,SuperpixelSLIC オブジェクトに格納されている、与えられたセグメンテーション上のスーパーピクセルの実際の量を計算します。
Calculates the superpixel segmentation on a given image with the initialized parameters in the SuperpixelSLIC object.,SuperpixelSLIC オブジェクトで初期化されたパラメータを用いて，与えられた画像上のスーパーピクセル・セグメンテーションを計算します．
This function can be called again without the need of initializing the algorithm with createSuperpixelSLIC(). This save the computational cost of allocating memory for all the structures of the algorithm.The function computes the superpixels segmentation of an image with the parameters initialized with the function createSuperpixelSLIC(). The algorithms starts from a grid of superpixels and then refines the boundaries by proposing updates of edges boundaries.,この関数は，createSuperpixelSLIC()でアルゴリズムを初期化することなく，再度呼び出すことができます。この関数は，関数 createSuperpixelSLIC() で初期化されたパラメータを用いて，画像のスーパープリントセグメンテーションを計算します．このアルゴリズムは，スーパーピクセルのグリッドから始まり，エッジの境界の更新を提案することで，境界を洗練していきます．
Returns the mask of the superpixel segmentation stored in SuperpixelSLIC object.,SuperpixelSLICオブジェクトに格納されたスーパーピクセルセグメンテーションのマスクを返します。
Initialize a SuperpixelSLIC object.,SuperpixelSLICオブジェクトの初期化を行います。
"algorithm : Chooses the algorithm variant to use: SLIC segments image using a desired region_size, and in addition SLICO will optimize using adaptive compactness factor, while MSLIC will optimize using manifold methods resulting in more content-sensitive superpixels.",algorithm : 使用するアルゴリズム・バリアントを選択する。SLICは希望するregion_sizeを用いて画像を分割し、さらにSLICOはadaptive compactness factorを用いて最適化を行い、MSLICはmanifold methodsを用いて最適化を行い、よりコンテンツに敏感なスーパーピクセルを生成します。
ruler : Chooses the enforcement of superpixel smoothness factor of superpixel,ruler : Superpixel smoothness factorの強制適用を選択します。
"The function initializes a SuperpixelSLIC object for the input image. It sets the parameters of choosed superpixel algorithm, which are: region_size and ruler. It preallocate some buffers for future computing iterations over the given image. For enanched results it is recommended for color images to preprocess image with little gaussian blur using a small 3 x 3 kernel and additional conversion into CieLAB color space. An example of SLIC versus SLICO and MSLIC is ilustrated in the following picture.image",この関数は，入力画像に対して SuperpixelSLIC オブジェクトを初期化します．また，選択された superpixel アルゴリズムのパラメータである， region_size と ruler を設定します．また，与えられた画像に対して将来的に計算を繰り返すための，いくつかのバッファを事前に確保します．カラー画像の場合は、3×3の小さなカーネルを使ってガウスぼかしをかけ、CieLABの色空間に変換することをお勧めします。SLICとSLICOおよびMSLICの比較例を以下の画像に示します。
